<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>论文笔记 - Deep Residual Learning for Image Recognition</title>
    <link href="/2024/11/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deep-Residual-Learning-for-Image-Recognition/"/>
    <url>/2024/11/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deep-Residual-Learning-for-Image-Recognition/</url>
    
    <content type="html"><![CDATA[<h2 id="1-Information"><a href="#1-Information" class="headerlink" title="1. Information"></a>1. Information</h2><p><strong>Title</strong>: Deep Residual Learning for Image Recognition<br><strong>Link</strong>: <a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">Deep Residual Learning for Image Recognition</a><br><strong>Source</strong>: IEEE Conference on Computer Vision and Pattern Recognition, CVPR<br><strong>Date</strong>: 2015</p><h2 id="2-Summary"><a href="#2-Summary" class="headerlink" title="2. Summary"></a>2. Summary</h2><p>本论文提出了一种新型的深度学习架构——残差网络（ResNet），它通过引入<strong>残差连接</strong>来解决传统深度神经网络在训练过程中遇到的梯度消失和退化问题。作者通过实验证明，残差网络在多个图像识别任务中，特别是在ImageNet图像分类任务上，超越了现有的深度网络架构，达到了更好的效果。该方法可以加速深层网络的训练，并显著提高模型的性能。</p><h2 id="3-Background"><a href="#3-Background" class="headerlink" title="3. Background"></a>3. Background</h2><p>随着深度学习技术的发展，深度神经网络的应用逐渐取得了许多成功。然而，当网络层数增多时，模型的训练难度也随之增加，通常会遇到梯度消失、过拟合、训练退化等问题。传统的做法是通过增加层数来提升模型的表现，但实际效果往往没有预期那么好，这也成为深度学习研究中的一个瓶颈。因此，如何构建更深且更易训练的网络结构成为研究的热点。</p><h2 id="4-Research-Objective"><a href="#4-Research-Objective" class="headerlink" title="4. Research Objective"></a>4. Research Objective</h2><p>本研究的主要目标是通过引入<strong>残差学习</strong>来<strong>改进非常深的网络</strong>训练，使其能够更好地进行图像识别任务。</p><h2 id="5-Method"><a href="#5-Method" class="headerlink" title="5. Method"></a>5. Method</h2><ul><li><strong>残差学习</strong>：ResNet不是直接学习输入和输出之间的映射，而是学习输入与输出之间的残差，即 $\mathcal{F}(x)&#x3D;H(x)-x$，其中$H(x)$是期望的映射。</li></ul><img src="/论文笔记-Deep-Residual-Learning-for-Image-Recognition/1.png" style="zoom:67%;" /><ul><li><p><strong>网络模块</strong>：该架构由<strong>残差块（Residual Block）</strong>组成，每个残差块中包含一个快捷连接（identity mapping），该连接绕过一个或多个层。</p></li><li><p><strong>深层网络</strong>：通过残差学习，ResNet能够构建任意深度的网络（如152层），同时避免性能退化。</p></li><li><p><strong>优化</strong>：快捷连接确保了在反向传播中梯度能够顺利流动，从而更容易训练非常深的网络。</p></li></ul><h2 id="6-Evaluation"><a href="#6-Evaluation" class="headerlink" title="6. Evaluation"></a>6. Evaluation</h2><ul><li>作者在<strong>ImageNet 2012分类挑战</strong>中评估了ResNet架构，并取得了最先进的结果。ResNet-152模型的top-5错误率为3.0%，远超之前的模型。</li><li>作者还在COCO检测和分割任务中验证了该方法，表现也十分优秀。</li><li>他们将ResNet与传统的深度CNN模型以及其他网络进行了比较，证明了更深的ResNet在准确率上始终优于浅层网络。</li></ul><h2 id="7-Conclusion"><a href="#7-Conclusion" class="headerlink" title="7. Conclusion"></a>7. Conclusion</h2><ul><li><strong>残差网络（ResNets）</strong>使得构建非常深的网络成为可能，并且能够避免训练中的性能退化问题，解决了深层神经网络的梯度消失和优化问题。</li><li><strong>残差学习框架</strong>既简单又有效，可以广泛应用于各种任务，除了图像分类，还包括目标检测和图像分割等。</li><li>残差网络的成功表明，更深的网络结构并非一定会导致性能下降，只要能够有效地传递梯度。</li></ul>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>训练加速</tag>
      
      <tag>算法优化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Batch Normalization: Accelerating Deep Network Training by  Reducing Internal Covariate Shift</title>
    <link href="/2024/11/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/"/>
    <url>/2024/11/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/</url>
    
    <content type="html"><![CDATA[<h2 id="1-Information"><a href="#1-Information" class="headerlink" title="1. Information"></a>1. Information</h2><p><strong>Title</strong>: Batch Normalization Accelerating Deep Network Training by  Reducing Internal Covariate Shift<br><strong>Link</strong>: <a href="https://asvk.cs.msu.ru/~sveta/%D1%80%D0%B5%D1%84%D0%B5%D1%80%D0%B0%D1%82/batch_normalization.pdf">Batch Normalization Paper</a><br><strong>Source</strong>: International Conference on Machine Learning, ICML<br><strong>Date</strong>: 2015</p><h2 id="2-Summary"><a href="#2-Summary" class="headerlink" title="2. Summary"></a>2. Summary</h2><p>本文提出了批量归一化（Batch Normalization，BN）技术，通过减少深度神经网络中的内部协变量偏移（internal covariate shift），加速网络训练。内部协变量偏移是指在训练过程中，层输入的分布发生变化。BN的核心创新是对每一层的输入进行归一化，使其均值为零，方差为1，然后进行一个学习的线性变换。BN加速了收敛速度，允许使用更高的学习率，并且缓解了梯度消失和梯度爆炸的问题。BN还具有一定的正则化效果，减少了对dropout的需求。</p><h2 id="3-Background"><a href="#3-Background" class="headerlink" title="3. Background"></a>3. Background</h2><p>深度神经网络在训练时常常面临内部协变量偏移问题，即随着参数的更新，层输入的分布发生变化。这种不稳定性会导致优化过程效率低下，需要精心设计的初始化和学习率调优。之前的解决方法，如预训练和权重初始化，间接解决了这一问题，而BN通过直接归一化层输入来解决根本问题。</p><h2 id="4-Research-Objective"><a href="#4-Research-Objective" class="headerlink" title="4. Research Objective"></a>4. Research Objective</h2><ul><li>提出批量归一化方法来减少内部协变量偏移。</li><li>证明BN能够加速训练并提升模型性能。</li><li>评估BN与常用优化方法（如SGD）兼容性。</li><li>探讨BN的正则化效果及对超参数调节的影响。</li></ul><h2 id="5-Method"><a href="#5-Method" class="headerlink" title="5. Method"></a>5. Method</h2><ul><li><p><strong>归一化</strong>：<br>对每个mini-batch中的激活值$x$，进行归一化：<br>$$<br>\hat{x}&#x3D;\frac{x-\mu_B}{\sqrt{\sigma_B^2+\epsilon}}<br>$$<br>其中，$\mu_B$和$\sigma_B^2$分别是mini-batch的均值和方差，$\epsilon$用于防止除零错误。</p></li><li><p><strong>仿射变换</strong>：<br>归一化之后，使用可学习的参数$\gamma$（缩放）和$\beta$（平移）对激活值进行缩放和偏移：</p></li></ul><p>$$<br>y&#x3D;\gamma \hat{x}+\beta<br>$$</p><ul><li><p><strong>训练阶段</strong>：<br>在训练过程中，使用mini-batch的统计量进行归一化。同时，利用移动平均对均值$\mu_B$和方差$\sigma_B^2$进行估算，以便在推理时使用。</p></li><li><p><strong>推理阶段</strong>：<br>在推理阶段，使用固定的均值和方差（训练时的全局统计量）进行归一化，确保输出是确定的。</p></li></ul><h2 id="6-Evaluation"><a href="#6-Evaluation" class="headerlink" title="6. Evaluation"></a>6. Evaluation</h2><ul><li><p><strong>数据集</strong>：在CIFAR-10、ImageNet等基准数据集上进行测试。</p></li><li><p><strong>结果</strong>：</p><ul><li><p>BN显著加速了收敛（例如在ImageNet上，训练时间减少了多达14倍）。</p></li><li><p>与没有BN的模型相比，BN提高了模型的准确率。</p></li></ul></li><li><p><strong>消融实验</strong>：</p><ul><li><p>显示了$\gamma$和$\beta$的重要性，以及mini-batch归一化的有效性。</p></li><li><p>BN具有一定的正则化效果，减少了对dropout的需求。</p></li></ul></li></ul><h2 id="7-Conclusion"><a href="#7-Conclusion" class="headerlink" title="7. Conclusion"></a>7. Conclusion</h2><p>批量归一化提出了一种简单有效的方法来稳定并加速深度神经网络的训练。它通过减少内部协变量偏移，能够加快收敛速度并提高泛化能力。BN的普适性和有效性使其成为现代深度学习架构中的标准组件。研究强调了解决网络内部分布变化问题对提升训练效率的重要性。</p><h2 id="8-Notes"><a href="#8-Notes" class="headerlink" title="8. Notes"></a>8. Notes</h2><ul><li><p>为什么要进行归一化？</p><blockquote><ol><li><p><strong>避免梯度消失或梯度爆炸：</strong></p><ul><li><strong>梯度消失：</strong><br>对于偏大的通道值，激活函数（如 Sigmoid 或 Tanh）的输出可能趋近其饱和区间（例如，Sigmoid 趋近于 0 或 1）。在饱和区域，导数接近于 0，导致梯度几乎消失，权重无法有效更新。</li><li><strong>梯度爆炸：</strong><br>对于偏小的通道值，激活函数的导数可能非常大，导致梯度在反向传播过程中不断累积并放大，最终引起梯度爆炸。</li></ul><p>这些现象会使优化过程变得极其不稳定，甚至使模型无法收敛。</p></li><li><p><strong>平衡通道值范围：</strong></p><ul><li>如果不同通道的值范围差异显著：<ul><li><strong>梯度更新受大值主导：</strong> 较大的值会主导梯度更新方向，网络可能优先调整这些通道的权重。</li><li><strong>忽略小值信息：</strong> 较小值的通道可能被忽略，导致网络无法充分利用所有特征信息。</li></ul></li></ul><p>这种不平衡会降低模型的学习效率，延长训练时间，并难以达到最佳性能。</p></li><li><p><strong>简化损失函数的优化过程：</strong></p><ul><li>通道间值差异较大时，损失函数的形状可能会变得复杂（例如，陡峭的谷底或平坦的高原）。</li><li>优化器可能需要更小的学习率逐渐调整权重，从而减慢模型的收敛速度。</li></ul></li></ol></blockquote></li></ul>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>训练加速</tag>
      
      <tag>算法优化</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
