<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>论文笔记 - Going Deeper with Convolutions</title>
    <link href="/2024/11/21/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Going-Deeper-with-Convolutions/"/>
    <url>/2024/11/21/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Going-Deeper-with-Convolutions/</url>
    
    <content type="html"><![CDATA[<h2 id="1-Information"><a href="#1-Information" class="headerlink" title="1. Information"></a>1. Information</h2><p><strong>Title</strong>: Going Deeper with Convolutions<br><strong>Link</strong>: <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf">Inception V1 Paper</a><br><strong>Source</strong>: IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<br><strong>Date</strong>: 2015</p><h2 id="2-Summary"><a href="#2-Summary" class="headerlink" title="2. Summary"></a>2. Summary</h2><p>本文提出了一种名为 <strong>Inception</strong> 的深度卷积神经网络架构，在提高模型深度和宽度的同时，保持计算开销较低。基于此架构设计的 <strong>GoogLeNet</strong> 在图像分类和目标检测任务中取得了显著的性能提升。其核心思想是通过多个并行计算路径近似局部稀疏结构，兼顾了计算效率和模型精度。</p><h2 id="3-Background"><a href="#3-Background" class="headerlink" title="3. Background"></a>3. Background</h2><ul><li><p>深度学习的发展依赖于更强大的硬件、更大的数据集以及更高效的网络架构。然而，在移动设备或嵌入式环境中，功耗和内存限制要求算法需更高效。</p></li><li><p>增大网络规模虽能提升性能，但带来了两个问题：</p><ol><li><p>容易过拟合，需要昂贵的高质量标注数据。</p></li><li><p>参数利用率低，造成计算资源浪费。</p></li></ol></li><li><p>稀疏网络可减少计算量，但现代硬件在稀疏计算上效率不高。</p></li></ul><h2 id="4-Research-Objective"><a href="#4-Research-Objective" class="headerlink" title="4. Research Objective"></a>4. Research Objective</h2><p>设计一种高效的网络架构，在降低计算复杂度和参数量的同时，保留深度模型的表达能力。通过使用密集的并行模块近似稀疏性，解决传统稀疏结构难以高效并行的问题。</p><h2 id="5-Method"><a href="#5-Method" class="headerlink" title="5. Method"></a>5. Method</h2><ul><li><p><strong>核心思想</strong>：</p><ol><li>使用 <strong>1×1、3×3 和 5×5 卷积</strong> 提取多尺度特征，同时结合池化操作以捕获全局信息。</li><li>在大卷积核之前加入 <strong>1×1 卷积</strong>，用于降维和提升非线性表达能力。</li><li>通过模块化设计，平衡计算成本和特征提取能力。</li></ol></li><li><p><strong>网络结构</strong>：</p><ul><li>初版 Inception 模块中并行使用不同卷积核和池化操作，会导致通道数增加过快。</li></ul></li></ul><p align="center">  <img src="1.png" style="zoom:50%;" /></p><ul><li>改进版通过在每条路径前增加 <strong>1×1 卷积降维</strong>，有效控制通道数，降低参数量。</li></ul><p align="center"><img src="2.png" style="zoom:50%;" /></p><ul><li>GoogLeNet 总体架构：<ul><li>采用多层 Inception 模块堆叠，深度增加但计算效率较高。</li><li>引入辅助分类器（仅训练时使用）缓解梯度消失问题。</li></ul></li></ul><p align="center"><img src="3.png" style="zoom:50%;" /></p><h2 id="6-Evaluation"><a href="#6-Evaluation" class="headerlink" title="6. Evaluation"></a>6. Evaluation</h2><h3 id="①-图像分类任务"><a href="#①-图像分类任务" class="headerlink" title="① 图像分类任务"></a>① 图像分类任务</h3><ul><li>数据集：ImageNet</li><li>GoogLeNet 在分类任务中取得了 6.67% 的 top-5 错误率，相比 AlexNet 和 VGG 显著提升。</li></ul><p align="center"><img src="4.png" style="zoom:50%;" /></p><h3 id="②-目标检测任务"><a href="#②-目标检测任务" class="headerlink" title="② 目标检测任务"></a>② 目标检测任务</h3><ul><li>数据集：PASCAL VOC 和 COCO</li><li>在目标检测任务中，结合 Inception 的 R-CNN 模型在精度和效率上表现出色。</li></ul><p align="center"><img src="5.png" style="zoom:50%;" /></p><h2 id="7-Conclusion"><a href="#7-Conclusion" class="headerlink" title="7. Conclusion"></a>7. Conclusion</h2><ul><li><strong>稀疏性近似</strong>：通过并行使用多尺度卷积和池化操作，Inception 模块模拟局部稀疏结构，既降低了计算复杂度，又避免了稀疏计算的硬件瓶颈。</li><li><strong>模块化设计</strong>：使用 1×1 卷积降维，控制通道数增长，有效减少参数量和内存占用。</li><li><strong>高效性能</strong>：GoogLeNet 在分类和检测任务上均实现了卓越的性能，是一种计算资源友好的深度学习模型。</li></ul><h2 id="8-Notes"><a href="#8-Notes" class="headerlink" title="8. Notes"></a>8. Notes</h2><ol><li><p><strong>1×1 卷积的作用</strong>：</p><ul><li>降维与升维</li><li>降低参数量</li><li>跨通道信息融合</li><li>提高非线性表达能力</li></ul></li><li><p><strong>辅助分类器的设计注意事项</strong>：</p><ul><li><p>如果设计不当，可能干扰主分类器优化。</p></li><li><p>解决方法包括降低辅助分类器损失权重或简化其结构。</p></li></ul></li></ol>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>算法优化</tag>
      
      <tag>CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Deep Residual Learning for Image Recognition</title>
    <link href="/2024/11/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deep-Residual-Learning-for-Image-Recognition/"/>
    <url>/2024/11/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deep-Residual-Learning-for-Image-Recognition/</url>
    
    <content type="html"><![CDATA[<h2 id="1-Information"><a href="#1-Information" class="headerlink" title="1. Information"></a>1. Information</h2><p><strong>Title</strong>: Deep Residual Learning for Image Recognition<br><strong>Link</strong>: <a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">Deep Residual Learning for Image Recognition</a><br><strong>Source</strong>: IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<br><strong>Date</strong>: 2015</p><h2 id="2-Summary"><a href="#2-Summary" class="headerlink" title="2. Summary"></a>2. Summary</h2><p>本论文提出了一种新型的深度学习架构——残差网络（ResNet），它通过引入<strong>残差连接</strong>来解决传统深度神经网络在训练过程中遇到的梯度消失和退化问题。作者通过实验证明，残差网络在多个图像识别任务中，特别是在ImageNet图像分类任务上，超越了现有的深度网络架构，达到了更好的效果。该方法可以加速深层网络的训练，并显著提高模型的性能。</p><h2 id="3-Background"><a href="#3-Background" class="headerlink" title="3. Background"></a>3. Background</h2><p>随着深度学习技术的发展，深度神经网络的应用逐渐取得了许多成功。然而，当网络层数增多时，模型的训练难度也随之增加，通常会遇到梯度消失、过拟合、训练退化等问题。传统的做法是通过增加层数来提升模型的表现，但实际效果往往没有预期那么好，这也成为深度学习研究中的一个瓶颈。因此，如何构建更深且更易训练的网络结构成为研究的热点。</p><h2 id="4-Research-Objective"><a href="#4-Research-Objective" class="headerlink" title="4. Research Objective"></a>4. Research Objective</h2><p>本研究的主要目标是通过引入<strong>残差学习</strong>来改进非常深的网络训练，使其能够更好地进行图像识别任务。</p><h2 id="5-Method"><a href="#5-Method" class="headerlink" title="5. Method"></a>5. Method</h2><ul><li><strong>残差学习</strong>：ResNet不是直接学习输入和输出之间的映射，而是学习输入与输出之间的残差，即 $\mathcal{F}(x)&#x3D;H(x)-x$，其中$H(x)$是期望的映射。</li></ul><p align="center">  <img src="1.png" style="zoom:67%;" /></p><ul><li><p><strong>网络模块</strong>：该架构由<strong>残差块（Residual Block）</strong>组成，每个残差块中包含一个快捷连接，该连接绕过一个或多个层。</p></li><li><p><strong>深层网络</strong>：通过残差学习，ResNet能够构建任意深度的网络（如152层），同时避免性能退化。</p></li><li><p><strong>优化</strong>：快捷连接确保了在反向传播中梯度能够顺利流动，从而更容易训练非常深的网络。</p></li></ul><h2 id="6-Evaluation"><a href="#6-Evaluation" class="headerlink" title="6. Evaluation"></a>6. Evaluation</h2><ul><li>作者在<strong>ImageNet 2012分类挑战</strong>中评估了ResNet架构，并取得了最先进的结果。ResNet-152模型的top-5错误率为3.0%，远超之前的模型。</li><li>作者还在COCO检测和分割任务中验证了该方法，表现也十分优秀。</li><li>他们将ResNet与传统的深度CNN模型以及其他网络进行了比较，证明了更深的ResNet在准确率上始终优于浅层网络。</li></ul><h2 id="7-Conclusion"><a href="#7-Conclusion" class="headerlink" title="7. Conclusion"></a>7. Conclusion</h2><ul><li><strong>残差网络（ResNets）</strong>使得构建非常深的网络成为可能，并且能够避免训练中的性能退化问题，解决了深层神经网络的梯度消失和优化问题。</li><li><strong>残差学习框架</strong>既简单又有效，可以广泛应用于各种任务，除了图像分类，还包括目标检测和图像分割等。</li><li>残差网络的成功表明，更深的网络结构并非一定会导致性能下降，只要能够有效地传递梯度。</li></ul>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>训练加速</tag>
      
      <tag>算法优化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Batch Normalization: Accelerating Deep Network Training by  Reducing Internal Covariate Shift</title>
    <link href="/2024/11/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/"/>
    <url>/2024/11/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/</url>
    
    <content type="html"><![CDATA[<h2 id="1-Information"><a href="#1-Information" class="headerlink" title="1. Information"></a>1. Information</h2><p><strong>Title</strong>: Batch Normalization Accelerating Deep Network Training by  Reducing Internal Covariate Shift<br><strong>Link</strong>: <a href="https://asvk.cs.msu.ru/~sveta/%D1%80%D0%B5%D1%84%D0%B5%D1%80%D0%B0%D1%82/batch_normalization.pdf">Batch Normalization Paper</a><br><strong>Source</strong>: International Conference on Machine Learning (ICML)<br><strong>Date</strong>: 2015</p><h2 id="2-Summary"><a href="#2-Summary" class="headerlink" title="2. Summary"></a>2. Summary</h2><p>本文提出了批量归一化（Batch Normalization，BN）技术，通过减少深度神经网络中的内部协变量偏移（internal covariate shift），加速网络训练。内部协变量偏移是指在训练过程中，层输入的分布发生变化。BN的核心创新是对每一层的输入进行归一化，使其均值为零，方差为1，然后进行一个学习的线性变换。BN加速了收敛速度，允许使用更高的学习率，并且缓解了梯度消失和梯度爆炸的问题。BN还具有一定的正则化效果，减少了对dropout的需求。</p><h2 id="3-Background"><a href="#3-Background" class="headerlink" title="3. Background"></a>3. Background</h2><p>深度神经网络在训练时常常面临内部协变量偏移问题，即随着参数的更新，层输入的分布发生变化。这种不稳定性会导致优化过程效率低下，需要精心设计的初始化和学习率调优。之前的解决方法，如预训练和权重初始化，间接解决了这一问题，而BN通过直接归一化层输入来解决根本问题。</p><h2 id="4-Research-Objective"><a href="#4-Research-Objective" class="headerlink" title="4. Research Objective"></a>4. Research Objective</h2><ul><li>提出批量归一化方法来减少内部协变量偏移。</li><li>证明BN能够加速训练并提升模型性能。</li><li>评估BN与常用优化方法（如SGD）兼容性。</li><li>探讨BN的正则化效果及对超参数调节的影响。</li></ul><h2 id="5-Method"><a href="#5-Method" class="headerlink" title="5. Method"></a>5. Method</h2><ul><li><p><strong>归一化</strong>：<br>对每个mini-batch中的激活值$x$，进行归一化：<br>$$<br>\hat{x}&#x3D;\frac{x-\mu_B}{\sqrt{\sigma_B^2+\epsilon}}<br>$$<br>其中，$\mu_B$和$\sigma_B^2$分别是mini-batch的均值和方差，$\epsilon$用于防止除零错误。</p></li><li><p><strong>仿射变换</strong>：<br>归一化之后，使用可学习的参数$\gamma$（缩放）和$\beta$（平移）对激活值进行缩放和偏移：</p></li></ul><p>$$<br>y&#x3D;\gamma \hat{x}+\beta<br>$$</p><ul><li><p><strong>训练阶段</strong>：<br>在训练过程中，使用mini-batch的统计量进行归一化。同时，利用移动平均对均值$\mu_B$和方差$\sigma_B^2$进行估算，以便在推理时使用。</p></li><li><p><strong>推理阶段</strong>：<br>在推理阶段，使用固定的均值和方差（训练时的全局统计量）进行归一化，确保输出是确定的。</p></li></ul><h2 id="6-Evaluation"><a href="#6-Evaluation" class="headerlink" title="6. Evaluation"></a>6. Evaluation</h2><ul><li><p><strong>数据集</strong>：在CIFAR-10、ImageNet等基准数据集上进行测试。</p></li><li><p><strong>结果</strong>：</p><ul><li><p>BN显著加速了收敛（例如在ImageNet上，训练时间减少了多达14倍）。</p></li><li><p>与没有BN的模型相比，BN提高了模型的准确率。</p></li></ul></li><li><p><strong>消融实验</strong>：</p><ul><li><p>显示了$\gamma$和$\beta$的重要性，以及mini-batch归一化的有效性。</p></li><li><p>BN具有一定的正则化效果，减少了对dropout的需求。</p></li></ul></li></ul><h2 id="7-Conclusion"><a href="#7-Conclusion" class="headerlink" title="7. Conclusion"></a>7. Conclusion</h2><p>批量归一化提出了一种简单有效的方法来稳定并加速深度神经网络的训练。它通过减少内部协变量偏移，能够加快收敛速度并提高泛化能力。BN的普适性和有效性使其成为现代深度学习架构中的标准组件。研究强调了解决网络内部分布变化问题对提升训练效率的重要性。</p><h2 id="8-Notes"><a href="#8-Notes" class="headerlink" title="8. Notes"></a>8. Notes</h2><ul><li><p>为什么要进行归一化？</p><blockquote><ol><li><p><strong>避免梯度消失或梯度爆炸：</strong></p><ul><li><strong>梯度消失：</strong><br>对于偏大的通道值，激活函数（如 Sigmoid 或 Tanh）的输出可能趋近其饱和区间（例如，Sigmoid 趋近于 0 或 1）。在饱和区域，导数接近于 0，导致梯度几乎消失，权重无法有效更新。</li><li><strong>梯度爆炸：</strong><br>对于偏小的通道值，激活函数的导数可能非常大，导致梯度在反向传播过程中不断累积并放大，最终引起梯度爆炸。</li></ul><p>这些现象会使优化过程变得极其不稳定，甚至使模型无法收敛。</p></li><li><p><strong>平衡通道值范围：</strong></p><ul><li>如果不同通道的值范围差异显著：<ul><li><strong>梯度更新受大值主导：</strong> 较大的值会主导梯度更新方向，网络可能优先调整这些通道的权重。</li><li><strong>忽略小值信息：</strong> 较小值的通道可能被忽略，导致网络无法充分利用所有特征信息。</li></ul></li></ul><p>这种不平衡会降低模型的学习效率，延长训练时间，并难以达到最佳性能。</p></li><li><p><strong>简化损失函数的优化过程：</strong></p><ul><li>通道间值差异较大时，损失函数的形状可能会变得复杂（例如，陡峭的谷底或平坦的高原）。</li><li>优化器可能需要更小的学习率逐渐调整权重，从而减慢模型的收敛速度。</li></ul></li></ol></blockquote></li></ul>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>训练加速</tag>
      
      <tag>算法优化</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
