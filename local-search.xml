<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>论文笔记 - ComprehendEdit: A Comprehensive Dataset and Evaluation Framework for Multimodal Knowledge Editing</title>
    <link href="/2025/03/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-ComprehendEdit-A-Comprehensive-Dataset-and-Evaluation-Framework-for-Multimodal-Knowledge-Editing/"/>
    <url>/2025/03/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-ComprehendEdit-A-Comprehensive-Dataset-and-Evaluation-Framework-for-Multimodal-Knowledge-Editing/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: ComprehendEdit: A Comprehensive Dataset andEvaluation Framework for Multimodal Knowledge Editing<br /><strong>Link</strong>: <ahref="https://arxiv.org/pdf/2412.12821">ComprehendEdit Paper</a><br /><strong>Source</strong>: arxiv<br /><strong>Date</strong>: 2024.12.17</p><h2 id="summary">2. Summary</h2><ul><li>提出了<strong>ComprehendEdit</strong>，一个用于多模态知识编辑的基准数据集和评测框架。</li><li>设计了新的评估指标 <strong>知识泛化指数 (KGI, KnowledgeGeneralization Index)</strong> 和 <strong>知识保持指数 (KPI, KnowledgePreservationIndex)</strong>，用于衡量编辑后模型对相似任务的泛化能力以及对原有正确知识的保持能力。</li><li>引入 <strong>层次化上下文编辑 (HICE, Hierarchical In-ContextEditing)</strong>，作为基线方法，旨在在不影响外部样本的情况下改进模型的编辑能力。</li><li>进行了广泛的实验，包括对比现有方法（如<strong>IKE、SERAC、MEND</strong>），并展示了 HICE方法在多个关键指标上的优势。</li></ul><h2 id="background">3. Background</h2><p>随着大型多模态语言模型（MLLMs）的发展，其在自然语言处理和视觉理解方面取得了显著进展，但这些模型常常包含过时或不准确的信息。现有的多模态知识编辑评估方法存在局限性，例如评估范围狭窄、依赖AI合成样本导致评估偏差，以及仅关注与编辑任务无关的样本。这些问题限制了对多模态知识编辑技术的全面评估和改进。</p><h2 id="research-objective">4. Research Objective</h2><ul><li>构建一个全面的基准数据集 <strong>ComprehendEdit</strong>，包含 8种任务（如<strong>目标识别、目标计数、空间关系、文本识别</strong>）。</li><li>设计新的评估指标 <strong>KGI 和KPI</strong>，衡量模型在知识编辑后对相似任务的泛化能力以及是否破坏了原有正确知识。</li><li>提出 <strong>HICE方法</strong>，在不修改模型参数的情况下，通过上下文学习提高编辑性能。</li></ul><h2 id="method">5. Method</h2><h3 id="数据集构建"><strong>5.1 数据集构建</strong></h3><ul><li><strong>ComprehendEdit</strong> 由 <strong>8 种任务</strong>组成，数据来源包括<strong>GQA、TallyQA、VSR、TextVQA、MathVista</strong>等，确保任务多样性。</li><li>采用 <strong>BLIP-2 OPT 2.7B</strong> 和 <strong>MiniGPT-47B</strong> 进行预测，并过滤掉两个模型都无法正确预测的样本。</li><li>训练集与测试集的比例约为 <strong>3:1</strong>。</li></ul><h3 id="评估指标">5.2 评估指标</h3><h4 id="传统指标cheng-et-al.-2023">传统指标（Cheng et al. 2023）</h4><ul><li><strong>可靠性（Reliability）</strong>：编辑后目标样本的正确率。</li><li><strong>泛化性（Generality）</strong>：改写问题（T-G）与生成图像（M-G）的正确率。</li><li><strong>局部性（Locality）</strong>：跨领域样本的输出一致性。</li></ul><h4 id="新指标kgi与kpi">新指标：KGI与KPI</h4><ul><li><p><strong>知识泛化指数（KGI）</strong>：衡量编辑后模型对同领域<strong>原错误样本</strong>的改进效果。<span class="math display">\[\mathcal{M}_{K G I}=\mathbb{E}_{s \in \mathcal{D}_e, s^{\prime} \in\mathcal{D}_{K G I}(s)} \mathbb{I}\left(f\left(i^{\prime}, x^{\prime} ;\theta_e\right)=y^{\prime}\right)\]</span></p></li><li><p><strong>知识保持指数（KPI）</strong>：衡量编辑后模型对同领域<strong>原正确样本</strong>的保持能力。<span class="math display">\[\mathcal{M}_{K P I}=\mathbb{E}_{s \in \mathcal{D}_e, s^{\prime} \in\mathcal{D}_{K P I}(s)} \mathbb{I}\left(f\left(i^{\prime}, x^{\prime} ;\theta_e\right)=y^{\prime}\right)\]</span></p></li><li><p><strong>高效采样策略</strong>：基于图像/文本相似性，从 KGI/KPI子集中选取最相似与最不相似的样本（k=4），降低计算开销。</p></li></ul><h3 id="层次化上下文编辑hice"><strong>5.3层次化上下文编辑（HICE）</strong></h3><ol type="1"><li><p><strong>计算分类器</strong> <spanclass="math inline">\(W^*\)</span>：</p><ul><li>提取文本特征 <span class="math inline">\(F \in \mathbb{R}^{4 N\times d}\)</span> 并投影到高维空间：</li></ul><p><span class="math display">\[F_p=F W_r \in \mathbb{R}^{4 N \times M}\]</span></p><ul><li>通过带正则项的最小二乘优化计算 <spanclass="math inline">\(W^*\)</span>：</li></ul><p><span class="math display">\[W^*=\arg \min _W\left\|Y-F_p W\right\|^2+\lambda\|W\|^2\]</span></p><ul><li>解析解：</li></ul><p><span class="math display">\[W^*=\left(F_p^{\top} F_p+\lambda I\right)^{-1} F_p^{\top} Y\]</span></p><p>其中 <span class="math inline">\(\lambda\)</span>为正则化系数。</p></li><li><p><strong>构建记忆存储</strong>：</p><ul><li>维护两个记忆：<ul><li><strong>文本记忆</strong> <spanclass="math inline">\(M_1\)</span>：存储训练样本，通过 <strong>k-means聚类</strong> 选择代表样本。</li><li><strong>外部难样本记忆</strong> <spanclass="math inline">\(M_2\)</span>：存储难以分类的外部样本，以提高分类器<span class="math inline">\(W^*\)</span> 的准确率。</li></ul></li></ul></li><li><p><strong>推理阶段</strong>：</p><ul><li><p><strong>判断样本是否需要编辑</strong>：如果问题 <spanclass="math inline">\(x\)</span> 与 <spanclass="math inline">\(M_2\)</span> 中样本的最大相似度低于阈值 <spanclass="math inline">\(T\)</span>，且被分类为<strong>同域数据</strong>，则进行编辑。</p></li><li><p><strong>构造新的输入问题</strong>： <span class="math display">\[x_{\text {new }}=\left[s_1 ; s_2 ; \ldots ; s_{k_0} ; s_o ; x\right]\]</span> 其中 <spanclass="math inline">\(\left\{s_i\right\}_{i=1}^{k_0}\)</span> 是从 <spanclass="math inline">\(M_1\)</span> 中检索的相似示例，<spanclass="math inline">\(s_o\)</span> 是当前问题的转换版本。</p></li></ul></li></ol><h2 id="evaluation">6. Evaluation</h2><h3 id="实验设置">6.1 实验设置</h3><ul><li><strong>对比方法</strong>：FT-L（微调语言模型）、FT-V（微调视觉模型）、IKE（上下文学习）、SERAC（反事实模型）、MEND（元学习）。</li><li><strong>评估指标</strong>：Rel、T-G、T-L、M-L、KGI、KPI。</li></ul><h3 id="主要结果">6.2 主要结果</h3><ul><li><strong>ComprehendEdit基准</strong>：HICE 在 KGI（13.9）和KPI（46.34）上显著优于其他方法，验证其对同领域样本的平衡能力。</li><li><strong>跨数据集对比</strong>：在 E-VQA 上，HICE 的M-L（81.58%）优于 MEND（52.56%），表明其分类器有效区分跨领域样本。</li></ul><h3 id="消融实验">6.3 消融实验</h3><ul><li><strong>关键模块</strong>：记忆库 <spanclass="math inline">\(M_1\)</span> 对提升泛化性（T-G+76.34%）至关重要，分类器W∗<em>W</em>∗与记忆库M2<em>M</em>2显著改善跨领域局部性（M-L+44.94%）。</li><li><strong>参数敏感性</strong>：投影维度 <em>M</em>=10,000时效果最佳；阈值 <em>T</em>=0.85 平衡了编辑效果与局部性。</li></ul><h2 id="conclusion">7. Conclusion</h2><ul><li><strong>ComprehendEdit</strong> 提供了<strong>更全面的多模态知识编辑评测</strong>，克服了现有数据集的缺陷。</li><li>提出的 <strong>HICE 方法</strong> 在<strong>知识泛化（KGI）、知识保持（KPI）、文本局部性（T-L）、多模态局部性（M-L）</strong>上取得平衡。</li></ul>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>知识编辑</tag>
      
      <tag>多模态</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - MMKE-Bench: A Multimodal Editing Benchmark for Diverse Visual Knowledge</title>
    <link href="/2025/03/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-MMKE-Bench-A-Multimodal-Editing-Benchmark-for-Diverse-Visual-Knowledge/"/>
    <url>/2025/03/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-MMKE-Bench-A-Multimodal-Editing-Benchmark-for-Diverse-Visual-Knowledge/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: MMKE-Bench: A Multimodal Editing Benchmarkfor Diverse Visual Knowledge<br /><strong>Link</strong>: <ahref="https://arxiv.org/pdf/2502.19870">MMKE-Bench Paper</a><br /><strong>Source</strong>: International Conference on LearningRepresentations (ICLR)<br /><strong>Date</strong>: 2025</p><h2 id="summary">2. Summary</h2><p>本文提出了<strong>MMKE-Bench</strong>，一个面向真实场景的多模态知识编辑基准测试，旨在评估大型多模态模型（LMMs）对复杂视觉知识的编辑能力。与现有基于三元组的实体级编辑基准不同，MMKE-Bench采用自由形式的自然语言描述与图像结合的知识表示方式，并设计了三种编辑任务：</p><ol type="1"><li><strong>视觉实体编辑</strong>（更新实体相关视觉知识，如替换图像并修改关键属性）；</li><li><strong>视觉语义编辑</strong>（修改复杂行为或关系，如裁判手势的规则）；</li><li><strong>用户特定编辑</strong>（注入个性化知识，如用户与物品的关系）。</li></ol><p>实验表明，现有方法在视觉语义和用户特定编辑任务中表现较差，且<strong>无单一方法能在所有评估指标上表现优异</strong>。MMKE-Bench包含 2,940 条知识条目和 8,363 张图像，覆盖 33个类别，显著提升了多模态知识编辑的挑战性。</p><h2 id="background">3. Background</h2><p>大规模语言模型（LLMs）和大规模多模态模型（LMMs）已在多个任务上取得成功。然而：</p><ul><li>其知识会随着时间推移而<strong>过时或错误</strong>，需要<strong>知识编辑</strong>技术来进行更新，而非重新训练整个模型。</li><li>现有的知识编辑基准（如VLKEB、MMEdit、MIKE）大多基于<strong>三元组表示</strong>（subject,relation,object），主要关注<strong>实体级知识</strong>，无法充分模拟现实中<strong>多模态知识的复杂性</strong>（如动作、姿态、物体关系等）。</li><li>在 VLKEB 基准上，简单微调 LLaVA 模型即可达到<strong>99.59%可靠性</strong>、<strong>99.43% 文本泛化性</strong>、<strong>95.48%图像泛化性</strong>，表明现有基准已被模型“学透”，缺乏挑战性。</li></ul><p>MMKE-Bench旨在解决这些问题，引入<strong>更具挑战性的多模态知识编辑任务</strong>。</p><h2 id="research-objective">4. Research Objective</h2><ol type="1"><li><strong>构建更全面的多模态知识编辑基准</strong>，不仅涵盖<strong>视觉实体编辑</strong>，还包括<strong>视觉语义编辑</strong>和<strong>用户特定编辑</strong>。</li><li><strong>提供更真实的知识表达</strong>，使用自由格式的自然语言（而非三元组）来表示知识，提高可读性和灵活性。</li><li><strong>定义四个核心评估标准</strong>（可靠性、局部性、泛化性、可迁移性），系统评估知识编辑方法的表现。</li><li><strong>分析当前 LMMs的知识编辑能力</strong>，揭示现有方法的局限性，并推动新方法的发展。</li></ol><h2 id="method">5. Method</h2><h3 id="知识编辑任务定义"><strong>5.1 知识编辑任务定义</strong></h3><p>MMKE-Bench 设计了三种知识编辑任务：</p><ol type="1"><li><strong>视觉实体编辑</strong>：针对实体中心的修改，描述涵盖实体的多个方面。通过替换同类型的实体图像，并将关键信息修改为反事实内容，以纠正模型对实体的错误识别或过时信息。</li><li><strong>视觉语义编辑</strong>：专注于复杂的视觉语义中心修改，包括肢体动作、物体行为和关系等。通过替换同类型的语义动作图像，并修改规则或含义为反事实内容，以纠正模型对语义的错误理解和识别。</li><li><strong>用户特定编辑</strong>：关注将个性化用户信息注入LMMs，描述用户与对象之间的关系及其体验。由于这是为模型添加全新的个性化知识，因此不需要进行反事实编辑，直接使用原始知识作为编辑知识。</li></ol><p>知识表示采用： <span class="math display">\[k=(i, d)\]</span> 其中：</p><ul><li><span class="math inline">\(i\)</span> 为图像，</li><li><span class="math inline">\(d\)</span>为自由格式的文本描述（包括主要对象、视觉内容或用户个性化信息）。</li></ul><p>编辑后知识表示：</p><ul><li>视觉实体/语义编辑：<span class="math inline">\(k_e=(i_e,d_e)\)</span></li><li>用户特定编辑：<span class="math inline">\(k_e = (i, d)\)</span>（不涉及图像或文本修改，只是插入新知识）</li></ul><h3 id="数据集构建">5.2 数据集构建</h3><p>基准测试的构建分为四个步骤：</p><ul><li><strong>原始知识收集</strong>：列出候选的细粒度实体、视觉语义或用户特定项目，并收集相应的图像和描述。对于视觉实体编辑，从MMpedia 和 OVEN数据集中获取候选实体，并进行筛选和图像收集；对于视觉语义编辑，定义了 14个广泛类别的语义知识，并从相关数据集或通过爬取和人工验证收集图像和描述；对于用户特定编辑，考虑了9 个类别的个性化信息，并从不同来源收集图像和描述。</li><li><strong>编辑知识生成</strong>：对视觉实体和视觉语义知识进行编辑，而用户特定知识保持不变。在视觉模态中，采用图像替换方法，将实体或语义动作的图像随机替换为同类型的另一张图像；在文本模态中，将实体的关键信息和语义动作的规则或含义修改为反事实内容，并更新动作描述以与新的视觉内容一致。</li><li><strong>评估问题生成</strong>：遵循可靠性、局部性、泛化能力和可移植性四个关键评估原则，自动生成问题和答案，并进行人工验证和修订。具体来说：<ul><li><strong>可靠性问题生成</strong>：评估编辑后的知识是否正确生成，考虑文本可靠性和图像可靠性，分别衡量LMM 在文本和视觉模态中的编辑能力。通过提示 LLM生成与编辑后的反事实内容相关的问题，并要求问题必须涉及编辑内容的一个方面。例如，对于足球裁判的越位手势，可以生成关于越位手势处罚位置的问题。</li><li><strong>局部性问题生成</strong>：评估编辑模型中未涉及的知识保持不变的程度，通过比较编辑前后模型的输出来衡量。对于文本和图像局部性，分别从VLKEB 基准测试中获取问题和答案，其中文本问题来自 NQ 数据集，图像问题由VLKEB 特别设计。</li><li><strong>泛化能力问题生成</strong>：评估模型对邻近样本的响应效果。由于知识表示为自由形式，仅关注图像泛化能力。通过随机选择同一实体、视觉行为或个性化项目的另一张图像，并重复使用图像可靠性中的相同问题和答案来生成泛化问题。</li><li><strong>可移植性问题生成</strong>：评估编辑后的知识是否能够成功应用于相关的内容。对于视觉实体编辑，利用Wikipedia中的补充信息生成关于编辑内容的问题，并结合另一个关于编辑内容的问题来形成最终的可移植性问题。对于视觉语义和用户特定编辑，将主行为或项目的图像与同类型的另一张图像组合成新图像，并提出关于两张图像之间差异的问题，如头发颜色或物体形状，然后将该问题与涉及编辑内容的问题结合起来生成最终的可移植性问题。</li></ul></li><li><strong>人工检查与基准测试统计</strong>：在基准测试构建过程中，多次手动收集、审查和筛选样本，确保收集的图像质量，并在反事实编辑和问题生成后，人工审查问题，修订不适当的问题，纠正错误的答案。</li></ul><h3 id="评估标准">5.3 评估标准</h3><ul><li><p><strong>可靠性（Reliability）</strong>：验证编辑后的知识是否被正确应用；<span class="math display">\[\mathbb{E}_{\left(i_e, q_r, a_r\right) \sim Q_{r e l}}\mathbb{I}\left[M_\theta^{\prime}\left(i_e, q_r\right)=a_r\right]\]</span></p></li><li><p><strong>局部性（Locality）</strong>：确保无关知识未被修改； <spanclass="math display">\[\mathbb{E}_{\left(i_l, q_l\right) \sim Q_{l o c}}\mathbb{I}\left[M_\theta\left(i_l,q_l\right)=M_\theta^{\prime}\left(i_l, q_l\right)\right]\]</span></p></li><li><p><strong>泛化性（Generalization）</strong>：测试模型对同类图像的泛化能力；<span class="math display">\[\mathbb{E}_{\left(i_e^g, q_g, a_g\right) \sim Q_{g e n}}\mathbb{I}\left[M_\theta^{\prime}\left(i_e^g, q_g\right)=a_g\right]\]</span></p></li><li><p><strong>可移植性（Portability）</strong>：评估知识在新上下文中的应用能力；<span class="math display">\[\mathbb{E}_{\left(i_e^p, q_p, a_p\right) \sim Q_{p o r t}}\mathbb{I}\left[M_\theta^{\prime}\left(i_e^p, q_p\right)=a_p\right]\]</span></p></li></ul><h2 id="evaluation">6. Evaluation</h2><h3 id="实验设置">6.1 实验设置</h3><ul><li><strong>模型</strong>：BLIP-2、MiniGPT-4、LLaVA-1.5；</li><li><strong>编辑方法</strong>：Fine-tuning（FT）、IKE、SERAC、MEND、KE；</li><li><strong>任务类型</strong>：单次编辑（SingleEditing）和连续编辑（Sequential Editing）。</li></ul><h3 id="主要发现">6.2 主要发现</h3><ol type="1"><li><strong>IKE在可靠性和泛化性上表现最佳</strong>（如LLaVA-1.5的文本可靠性达75.65%）；</li><li><strong>SERAC和MEND在局部性上最优</strong>（图像局部性&gt;99%）；</li><li><strong>视觉语义和用户特定编辑更具挑战性</strong>（可靠性比视觉实体低10-20%）；</li><li><strong>现有方法在可移植性上普遍较差</strong>（KE表现最佳，但平均仅25.4%）；</li><li><strong>LLaVA-1.5整体性能最优</strong>，得益于更大的模型规模和指令调优设计。</li></ol><h2 id="conclusion">7. Conclusion</h2><ul><li>MMKE-Bench 提供<strong>更真实的知识编辑任务</strong>，推动 LMMs研究。</li><li><strong>没有单一方法能在所有指标上表现最佳</strong>，需要进一步优化。</li><li><strong>现有方法在视觉和个性化编辑方面仍有较大挑战</strong>。</li></ul><h2 id="notes">8. Notes</h2><h3 id="使用反事实编辑潜在的问题">8.1 使用反事实编辑潜在的问题</h3><blockquote><h4 id="反事实内容的合理性"><strong>1. 反事实内容的合理性</strong></h4><p><strong>问题：</strong></p><ul><li>如果<strong>生成的反事实不够自然或合理</strong>，可能会导致模型在编辑后仍然无法正确理解内容。</li><li>例如，将“<strong>伊布是瑞典人</strong>”改为“<strong>伊布是日本人</strong>”可能明显不符合常识，而如果改为“<strong>伊布是挪威人</strong>”可能更自然。</li></ul><p><strong>影响：</strong></p><ul><li>如果反事实内容过于随机或不符合语境，可能会误导模型，使其学习到不合理的知识结构。</li><li>可能影响评测结果，使得知识编辑任务变得过于简单或难以评估。</li></ul><hr /><h4 id="反事实可能引发模型的误解"><strong>2.反事实可能引发模型的误解</strong></h4><p><strong>问题：</strong></p><ul><li>反事实内容可能会<strong>影响模型的已有知识</strong>，导致模型在知识编辑后对<strong>原始事实和反事实的区分不清</strong>。</li><li>例如，如果反事实内容是“<strong>Eiffel Tower 位于London</strong>”，模型可能会混淆 <strong>London</strong> 和<strong>Paris</strong> 之间的关系。</li></ul><p><strong>影响：</strong></p><ul><li>如果知识编辑方法不能很好地局部更新知识，则可能影响模型对未修改内容的理解，例如：<ul><li>修改“伊布是瑞典人”为“伊布是挪威人”，可能导致模型对其他瑞典球员（如拉尔森）的认知也受到干扰。</li><li>编辑“裁判举手表示越位”为“裁判举手表示点球”，可能导致模型错误回答未被编辑的问题。</li></ul></li></ul><hr /><h4 id="反事实知识的冲突"><strong>3. 反事实知识的冲突</strong></h4><p><strong>问题：</strong></p><ul><li>反事实知识可能与<strong>模型已有知识冲突</strong>，导致编辑失败或产生不一致的输出。</li><li>例如，若 LMM 之前已经学习到“Eiffel Tower 在Paris”，但编辑任务要求改为“Eiffel Tower 在 London”，模型可能会：<ul><li>拒绝修改，仍然输出 Paris（知识编辑失败）。</li><li>在不同回答中输出 Paris 和 London（知识冲突）。</li><li>仅在某些场景下记住 London，但在其他场景下仍然回答Paris（部分编辑成功）。</li></ul></li></ul><p><strong>影响：</strong></p><ul><li>多轮编辑可能导致矛盾：<ul><li>例如，模型先被编辑为“伊布是挪威人”，后又被编辑为“伊布是巴西人”，可能导致模型输出不稳定。</li></ul></li><li><strong>影响模型一致性</strong>，使得相同问题在不同对话轮次或不同场景下给出不同答案。</li></ul><hr /><h4 id="泛化性问题"><strong>4. 泛化性问题</strong></h4><p><strong>问题：</strong></p><ul><li>反事实知识编辑通常只修改<strong>单个知识点</strong>，但现实世界的知识往往是<strong>互相关联的</strong>，模型可能无法正确泛化。</li><li>例如：<ul><li>编辑“伊布是瑞典人”为“伊布是挪威人”，但模型仍然回答“伊布出生于瑞典”。</li><li>编辑“越位判罚后在犯规点开球”为“越位判罚后在球门区开球”，但模型在其他足球规则相关问题中仍然使用旧规则。</li></ul></li></ul><p><strong>影响：</strong></p><ul><li>反事实编辑后，模型可能对相关知识的推理能力下降，只能在特定问题中表现正确，而在新问题上仍然依赖旧知识。</li><li>可能导致<strong>知识编辑的效果有限</strong>，无法真正模拟现实场景中的知识变化。</li></ul><hr /><h4 id="可迁移性问题"><strong>5. 可迁移性问题</strong></h4><p><strong>问题：</strong></p><ul><li>反事实编辑可能仅适用于<strong>特定场景</strong>，但无法<strong>迁移到类似场景</strong>。</li><li>例如：<ul><li>修改“伊布是瑞典人”为“伊布是挪威人”，但如果问“瑞典足球史上最著名的球员是谁？”，模型可能仍然回答伊布，而不是拉尔森。</li></ul></li></ul><p><strong>影响：</strong></p><ul><li>影响模型对新输入的适应能力，导致知识编辑的作用<strong>局限于特定案例</strong>，无法真正推广到整个知识库。</li></ul></blockquote>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>知识编辑</tag>
      
      <tag>多模态</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning</title>
    <link href="/2025/03/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-InstructBLIP-Towards-General-purpose-Vision-Language-Models-with-Instruction-Tuning/"/>
    <url>/2025/03/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-InstructBLIP-Towards-General-purpose-Vision-Language-Models-with-Instruction-Tuning/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: InstructBLIP: Towards General-purposeVision-Language Models with Instruction Tuning<br /><strong>Link</strong>: <ahref="https://arxiv.org/pdf/2305.06500">InstructBLIP Paper</a><br /><strong>Source</strong>: Advances in Neural Information ProcessingSystems (NeurIPS)<br /><strong>Date</strong>: 2023.05.11</p><h2 id="summary">2. Summary</h2><ul><li>InstructBLIP 是基于 BLIP-2预训练模型的视觉-语言指令微调（instructiontuning）框架，旨在提升模型的通用性。</li><li>通过对 26个公开数据集进行指令格式转换，并采用指令感知（instruction-aware）的QueryTransformer（Q-Former）模块，使模型能根据不同指令提取特定视觉信息。</li><li>训练时采用 13 个数据集（held-in），零样本测试时在 13个未见数据集（held-out）上评估。</li><li>结果显示 InstructBLIP 在零样本任务上显著超越 BLIP-2 和Flamingo。</li><li>在具体任务（如 ScienceQA）上，InstructBLIP 经过微调后取得了 90.7%的准确率，优于已有模型。</li></ul><h2 id="background">3. Background</h2><p>在自然语言处理（NLP）领域，指令调优已被证明是一种有效的方法，能够使大型语言模型（LLM）根据自然语言指令执行多种任务。然而，将这种方法扩展到视觉-语言任务中面临诸多挑战，因为视觉输入的多样性和任务的复杂性使得模型难以泛化到未见过的任务。以往的研究主要分为两类：一是多任务学习（multitasklearning），将多种任务统一为相同的输入输出格式，但这种方法在未见过的数据集和任务上泛化能力较差；二是通过图像描述数据训练视觉组件，但这些数据过于有限，无法支持需要复杂视觉理解的任务。</p><h2 id="research-objective">4. Research Objective</h2><ul><li>探索视觉-语言指令调优的系统化方法。</li><li>构建能通过自然语言指令适应多种任务的通用模型。</li><li>验证模型在零样本泛化和下游任务微调中的性能优势。</li></ul><h2 id="method">5. Method</h2><p align="center"><img src="1.png" style="zoom:67%;" /></p><h3 id="模型架构">5.1 模型架构</h3><p>基于BLIP-2的三模块设计：</p><ol type="1"><li><strong>冻结图像编码器</strong>（如ViT-g/14）提取视觉特征。</li><li><strong>指令感知Q-Former</strong>：<ul><li>输入：图像编码特征 + 文本指令（通过共享自注意力层交互）。</li><li>输出：动态提取与指令相关的视觉特征（K个查询嵌入）。</li></ul></li><li><strong>冻结大语言模型（LLM）</strong>（如FlanT5或Vicuna）：接收Q-Former的视觉特征和指令，生成文本响应。</li></ol><h3 id="数据集构建">5.2 数据集构建</h3><ul><li><strong>数据来源</strong>：收集 26个公开可用的视觉-语言数据集，并转换为指令微调格式。</li><li><strong>数据集类别</strong>：涵盖 11类任务，包括图像描述、视觉问答、视觉推理等。</li><li><strong>指令模板</strong>：<ul><li>为每个数据集手工设计 10-15个不同的指令格式，确保模型学习通用指令跟随能力。</li><li>对于<strong>图像描述任务</strong>：模板可能包括“<Image> A shortimage caption:”或“<Image> Describe the content of the image.”。</li><li>对于<strong>视觉问答任务</strong>：模板可能包括“<Image> {Question}Short answer:”或“<Image> Question: {Question} Answer:”。</li></ul></li><li><strong>数据分割</strong>：<ul><li><strong>训练集（held-in）</strong>：13 个数据集</li><li><strong>测试集（held-out）</strong>：13 个数据集，包括 4类任务完全未见</li></ul></li></ul><h2 id="evaluation">6. Evaluation</h2><ul><li><strong>零样本任务评估</strong>：<ul><li><strong>InstructBLIP 显著优于 BLIP-2 和 Flamingo</strong></li><li>在 ScienceQA 数据集（含图像）上，FlanT5-XXL 版本准确率达到<strong>90.7%</strong></li><li><strong>Vicuna 版本更适合开放式生成任务</strong>，而 <strong>FlanT5版本更擅长多选任务</strong></li></ul></li><li><strong>消融实验（Ablation Study）</strong>：<ul><li>移除 <strong>指令感知 Q-Former</strong>，性能下降 2.5%-7.6%</li><li>移除 <strong>数据平衡策略</strong>，整体性能下降 1%-5%</li></ul></li><li><strong>微调实验（Fine-tuning Study）</strong>：<ul><li>经过微调后，InstructBLIP 在多个任务上达到最优性能。</li></ul></li></ul><h2 id="conclusion">7. Conclusion</h2><ul><li>InstructBLIP 通过 <strong>指令微调</strong> 实现了<strong>视觉-语言任务的广泛泛化</strong>。</li><li><strong>指令感知 Q-Former</strong>使模型能适应不同任务，提取特定视觉信息。</li><li><strong>数据平衡采样策略</strong> 解决了数据集规模不均的问题。</li><li>在 <strong>零样本评估</strong> 和 <strong>下游任务微调</strong>方面，InstructBLIP 均达到最优。</li></ul>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Pre-training</tag>
      
      <tag>Fine-tuning</tag>
      
      <tag>MLLMs</tag>
      
      <tag>指令微调</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - MiniGPT-4 Enhancing Vision-Language Understanding with Advanced Large Language Models</title>
    <link href="/2025/03/06/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-MiniGPT-4-Enhancing-Vision-Language-Understanding-with-Advanced-Large-Language-Models/"/>
    <url>/2025/03/06/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-MiniGPT-4-Enhancing-Vision-Language-Understanding-with-Advanced-Large-Language-Models/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: MiniGPT-4: Enhancing Vision-LanguageUnderstanding with Advanced Large Language Models<br /><strong>Link</strong>: <ahref="https://arxiv.org/pdf/2304.10592">MiniGPT-4 Paper</a><br /><strong>Source</strong>: arXiv<br /><strong>Date</strong>: 2023.04.20</p><h2 id="summary">2. Summary</h2><p>MiniGPT-4是一个视觉-语言模型，它通过对齐一个冻结的视觉编码器（ViT-G/14 +Q-Former）与一个先进的大型语言模型（Vicuna），仅使用一个投影层（LinearProjection），成功地复制了 GPT-4的一些高级多模态能力，如详细的图像描述生成、基于手绘草图的网页创建、诗歌与故事生成等。此外，论文发现仅基于短图像标题训练的模型会生成不自然的语言，因此作者引入了一个详细图像描述数据集进行微调，以提升模型的生成质量。</p><h2 id="background">3. Background</h2><ul><li><strong>多模态模型挑战</strong>：GPT-4展示了强大的视觉语言能力，但其技术细节未公开。</li><li><strong>关键假设</strong>：GPT-4的能力源于先进语言模型（LLM）与视觉特征的深度结合。</li><li><strong>现有模型局限</strong>：如 BLIP-2 和 Kosmos-1因语言模型较弱，无法支持复杂多模态任务。</li><li><strong>目标</strong>：探索如何通过高效对齐视觉与语言模型，低成本复现GPT-4 的能力。</li></ul><h2 id="research-objective">4. Research Objective</h2><ul><li>验证假设：<strong>对齐视觉特征与先进 LLM是实现高级多模态能力的关键</strong>。</li><li>设计轻量架构，仅训练单层投影实现对齐。</li><li>解决短文本训练导致的生成不连贯问题（如重复、碎片化）。</li></ul><h2 id="method">5. Method</h2><h3 id="模型架构">模型架构</h3><ol type="1"><li><strong>视觉编码器</strong>：BLIP-2 的 ViT-G/14 +Q-Former（冻结参数）。</li><li><strong>语言模型</strong>：Vicuna（基于 LLaMA，性能接近 ChatGPT 的90%）。</li><li><strong>投影层</strong>：单线性层（输入维度：Q-Former输出的视觉特征维度；输出维度：Vicuna 的嵌入维度）。</li></ol><h3 id="两阶段训练">两阶段训练</h3><h4 id="第一阶段预训练">第一阶段：预训练</h4><ul><li><strong>目标</strong>：对齐视觉与语言特征。</li><li><strong>损失函数</strong>：基于文本生成的交叉熵损失，仅优化投影层参数。</li><li><strong>数据集</strong>：5M 图像-文本对（LAION、ConceptualCaptions、SBU）。</li><li><strong>训练细节</strong>：20k 步，batch size=256，4×A100（10小时）。</li></ul><h4 id="第二阶段微调">第二阶段：微调</h4><ul><li><p><strong>问题</strong>：预训练后生成文本存在重复、碎片化。</p></li><li><p><strong>解决方案</strong>：构建高质量数据集（3,500对详细描述），通过对话模板微调。</p></li><li><p><strong>模板示例</strong>：</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs clean">###Human: &lt;Img&gt;&lt;ImageFeature&gt;&lt;/Img&gt; Describe this image <span class="hljs-keyword">in</span> detail.  <br>###Assistant: [生成详细描述]  <br></code></pre></td></tr></table></figure></li><li><p><strong>微调细节</strong>：400 步，batch size=12，单卡 A100（7分钟）。</p></li></ul><h2 id="conclusion">6. Conclusion</h2><ul><li>通过一个线性投影层对齐视觉特征与先进语言模型，可以实现 GPT-4级别的多模态能力。</li><li>仅使用短标题训练不足以提升模型对话能力，第二阶段微调显著改善了语言流畅性。</li><li>未来可以通过增加数据规模、优化训练策略进一步提升 MiniGPT-4的性能。</li></ul>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Pre-training</tag>
      
      <tag>Fine-tuning</tag>
      
      <tag>MLLMs</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</title>
    <link href="/2025/03/06/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-BLIP-2-Bootstrapping-Language-Image-Pre-training-with-Frozen-Image-Encoders-and-Large-Language-Models/"/>
    <url>/2025/03/06/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-BLIP-2-Bootstrapping-Language-Image-Pre-training-with-Frozen-Image-Encoders-and-Large-Language-Models/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: BLIP-2: Bootstrapping Language-ImagePre-training with Frozen Image Encoders and Large Language Models<br /><strong>Link</strong>: <a href="https://arxiv.org/pdf/2301.12597">BLIP-2Paper</a><br /><strong>Source</strong>: International Joint Conference on ArtificialIntelligence (IJCAI)<br /><strong>Date</strong>: 2023.01.30</p><h2 id="summary">2. Summary</h2><p>BLIP-2 提出了一种新的视觉-语言预训练 (Vision-Language Pre-training,VLP) 方法，通过冻结的图像编码器和大规模语言模型 (LLM)进行高效的跨模态学习。核心创新点包括：</p><ol type="1"><li><strong>Querying Transformer (Q-Former)</strong>:设计了一种轻量级的查询 Transformer，作为图像编码器与 LLM之间的信息桥梁。</li><li>两阶段训练策略:<ul><li>第一阶段：从冻结的图像编码器学习视觉-语言表示。</li><li>第二阶段：从冻结的 LLM 学习视觉到语言的生成能力。</li></ul></li><li><strong>高效计算</strong>: 相比 Flamingo80B，BLIP-2 仅使用 54倍更少的可训练参数，在零样本 VQA 任务上仍然能取得 8.7% 的性能提升。</li><li><strong>Emerging Capabilities</strong>: BLIP-2具备零样本图像到文本生成的能力，能响应自然语言指令，如视觉知识推理、视觉对话等。</li></ol><h2 id="background">3. Background</h2><p>传统的 VLP方法依赖于端到端训练大规模模型，这导致计算成本高昂，并且难以充分利用现有的单模态预训练模型（如CLIP 和 GPT）。BLIP-2的核心目标是通过冻结这些预训练模型，降低训练成本，同时提高性能。</p><p>主要挑战：</p><ul><li><strong>模态对齐问题</strong>: 由于 LLM 在预训练时未见过图像，冻结LLM 使得视觉-语言对齐变得困难。</li><li><strong>计算开销</strong>: 现有方法（如Flamingo）需要在大规模数据集上进行计算密集的端到端训练。</li></ul><h2 id="research-objective">4. Research Objective</h2><p>BLIP-2 旨在通过<strong>冻结的图像编码器和 LLM</strong>，引导轻量级Q-Former 学习模态对齐，实现高效的 VLP 训练。其具体目标包括：</p><ol type="1"><li><strong>提高视觉-语言对齐能力</strong>: 通过 Q-Former从冻结的图像编码器提取信息，并桥接到 LLM。</li><li><strong>提升计算效率</strong>:通过模块化方法减少可训练参数，同时保持高性能。</li><li><strong>提升零样本能力</strong>:使模型在没有额外微调的情况下，执行图像描述、VQA 和视觉对话任务。</li></ol><h2 id="method">5. Method</h2><p>BLIP-2 采用了一种新的两阶段预训练框架，并引入了 Q-Former作为视觉-语言桥梁。</p><h3 id="q-former-结构">5.1 Q-Former 结构</h3><p align="center"><img src="1.png" style="zoom:80%;" /></p><p>Q-Former 是一个轻量级 Transformer，由两个子模块组成：</p><ol type="1"><li><strong>图像 Transformer</strong>:通过自注意力和交叉注意力层，从冻结的图像编码器中提取视觉特征。</li><li><strong>文本 Transformer</strong>:既可以作为编码器，也可以作为解码器，用于连接 LLM。</li></ol><p><strong>查询向量 (Query Embeddings)</strong> 模型使用 <spanclass="math inline">\(N\)</span> 个可学习查询向量 (Queries)，它们：</p><ul><li>通过自注意力机制交互。</li><li>通过交叉注意力机制与冻结的图像特征交互。</li><li>最终生成固定大小的视觉表示，用于 LLM 输入。</li></ul><p><strong>公式表示</strong>：</p><ol type="1"><li><p>图像编码器输出 <span class="math inline">\(X =\text{FrozenImageEncoder}(I)\)</span></p></li><li><p>查询向量 <span class="math inline">\(Q = [q_1, q_2, ...,q_N]\)</span></p></li><li><p>经过 Transformer 层： <span class="math display">\[Z=\mathrm{Q}\operatorname{Former}(Q, X)\]</span> 其中 <span class="math inline">\(Z\)</span>是输出的查询表示，用于下游任务。</p></li></ol><p>BLIP-2 通过冻结的图像编码器学习视觉-语言对齐，使用三种损失：</p><ol type="1"><li><strong>图像-文本对比学习（Image-Text Contrastive Learning,ITC）</strong>：通过最大化图像和文本表示之间的互信息，学习对齐图像和文本表示。具体来说，通过对比正样本对的图像-文本相似度与负样本对的相似度来实现。</li><li><strong>图像引导的文本生成（Image-grounded Text Generation,ITG）</strong>：训练Q-Former根据输入图像生成文本。由于 Q-Former的架构不允许图像编码器直接与文本标记交互，因此需要通过查询向量提取信息，并通过自注意力层传递给文本标记。</li><li><strong>图像-文本匹配（Image-Text Matching,ITM）</strong>：这是一个二分类任务，模型需要预测图像-文本对是否匹配。通过双向自注意力掩码，查询向量和文本可以相互交互，从而捕获多模态信息。</li></ol><h3 id="视觉到语言生成学习">5.2 视觉到语言生成学习</h3><p>在这一阶段，Q-Former 连接到一个冻结的LLM，学习视觉信息到文本的映射。</p><p align="center"><img src="2.png" style="zoom:80%;" /></p><p><strong>目标</strong>：</p><ul><li>通过前馈层将 Q-Former 输出映射到 LLM 的嵌入空间：</li></ul><p><span class="math display">\[Z^{\prime}=W Z+b\]</span></p><ul><li>将其作为软视觉提示附加到输入文本嵌入中。这一阶段的目标是训练Q-Former 生成 LLM 能够理解的视觉表示，从而减少 LLM在学习视觉-语言对齐时的负担。</li></ul><h2 id="evaluation">6. Evaluation</h2><p>BLIP-2 在多个任务上进行了评估：</p><ol type="1"><li><strong>零样本 VQA</strong>: 在 VQAv2 数据集上，优于 Flamingo80B8.7%。</li><li><strong>图像字幕生成</strong>: 在 NoCaps 和 COCO 上，CIDEr 分数超过121。</li><li><strong>图像-文本检索</strong>: Flickr30K 数据集上，R@1 达到97.6%。</li></ol><h2 id="conclusion">7. Conclusion</h2><ul><li>BLIP-2 通过冻结预训练的图像编码器和 LLM，利用轻量级的 Q-Former桥接视觉和语言模态之间的差距，提出了一种高效且通用的视觉-语言预训练方法。该方法在多种视觉-语言任务上达到了最先进的性能，同时显著减少了可训练参数。此外，BLIP-2还展示了零样本图像到文本生成的能力，能够遵循自然语言指令。这一研究为构建多模态对话AI 代理迈出了重要一步。</li></ul>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Pre-training</tag>
      
      <tag>Fine-tuning</tag>
      
      <tag>MLLMs</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - BLIP Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</title>
    <link href="/2025/03/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-BLIP-Bootstrapping-Language-Image-Pre-training-for-Unified-Vision-Language-Understanding-and-Generation/"/>
    <url>/2025/03/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-BLIP-Bootstrapping-Language-Image-Pre-training-for-Unified-Vision-Language-Understanding-and-Generation/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: BLIP: Bootstrapping Language-ImagePre-training for Unified Vision-Language Understanding andGeneration<br /><strong>Link</strong>: <a href="https://arxiv.org/pdf/2201.12086">BLIPPaper</a><br /><strong>Source</strong>: International Conference on Machine Learning(ICML)<br /><strong>Date</strong>: 2022.01.28</p><h2 id="summary">2. Summary</h2><p>BLIP提出了一种统一的视觉-语言预训练框架，解决了现有方法的两大局限性：</p><ol type="1"><li><strong>模型局限性</strong>：现有模型要么仅擅长理解任务（如检索），要么仅擅长生成任务（如图像描述），而BLIP通过<strong>多模态混合编码器-解码器（MED）</strong>实现了多任务统一建模。</li><li><strong>数据局限性</strong>：传统方法依赖具有噪声网络数据，BLIP提出<strong>Captioning andFiltering（CapFilt）</strong>方法，通过生成合成标题并过滤噪声数据，显著提升了数据质量。实验表明，BLIP 在图像-文本检索（+2.7% Recall@1）、图像描述生成（+2.8%CIDEr）和视觉问答（+1.6% VQA 分数）等任务中达到SOTA，并在零样本视频-语言任务中展现了强泛化能力。</li></ol><h2 id="background">3. Background</h2><ul><li><p>近年来，VLP 取得了显著进展，但现有方法存在两个主要局限性：</p><ul><li><strong>模型层面</strong>: 仅使用编码器（如 CLIP,ALBEF）无法直接适应文本生成任务，而编码-解码模型（如SimVLM）在图像-文本检索任务中表现不佳。</li><li><strong>数据层面</strong>: 现有方法主要依赖 Web爬取的图像-文本对数据，但噪声较大，影响模型性能。</li></ul><p>BLIP 通过 MED 和 CapFilt 解决了上述问题，提升了 VLP的适应性和数据利用效率。</p></li></ul><h2 id="research-objective">4. Research Objective</h2><ul><li>设计一个灵活的视觉-语言模型，支持<strong>理解任务（如检索）</strong>和<strong>生成任务（如图像描述）</strong>。</li><li>通过数据增强方法（CapFilt）提升噪声数据的利用率。</li><li>在多种下游任务中实现性能突破，并验证零样本迁移能力。</li></ul><h2 id="method">5. Method</h2><h3 id="多模态编码-解码混合模型med">5.1多模态编码-解码混合模型（MED）</h3><p>BLIP 采用了一种新的 MED结构，使模型能够同时用于理解（检索、匹配）和生成（描述、回答）任务。MED具有三种模式：</p><ol type="1"><li><strong>单模态编码器 (Unimodal Encoder)</strong>:独立对图像和文本进行编码，并使用 <strong>图像-文本对比损失 (Image-TextContrastive Loss, ITC)</strong> 进行优化。</li><li><strong>图像引导文本编码器 (Image-grounded Text Encoder)</strong>:通过 <strong>图像-文本匹配损失 (Image-Text Matching Loss,ITM)</strong>，学习细粒度的视觉-语言表示。</li><li><strong>图像引导文本解码器 (Image-grounded Text Decoder)</strong>:使用 <strong>语言建模损失 (Language Modeling Loss,LM)</strong>，生成图像描述或回答问题。</li></ol><p align="center"><img src="1.png" style="zoom:60%;" /></p><h4 id="图像-文本对比损失itc"><strong>(1)图像-文本对比损失（ITC）</strong></h4><p>目标是最大化匹配的图像-文本对 <span class="math inline">\((I,T)\)</span> 之间的相似度，同时最小化不匹配对 <spanclass="math inline">\((I, T&#39;)\)</span> 的相似度。 <spanclass="math display">\[L_{I T C}=-\log \frac{\exp \left(\operatorname{sim}\left(E_I(I),E_T(T)\right) / \tau\right)}{\sum_{T^{\prime} \in \mathcal{B}} \exp\left(\operatorname{sim}\left(E_I(I), E_T\left(T^{\prime}\right)\right)/ \tau\right)}\]</span> 其中：</p><ul><li><span class="math inline">\(E_I(I)\)</span> 和 <spanclass="math inline">\(E_T(T)\)</span>分别是图像编码器和文本编码器的输出。</li><li><span class="math inline">\(\text{sim}(\cdot, \cdot)\)</span>表示相似度计算（通常为余弦相似度）。</li><li><span class="math inline">\(\tau\)</span> 是温度参数。</li><li><span class="math inline">\(\mathcal{B}\)</span> 是训练 batch。</li></ul><h4 id="图像-文本匹配损失itm"><strong>(2)图像-文本匹配损失（ITM）</strong></h4><p>目标是预测给定图像-文本对是否匹配，采用二元交叉熵损失： <spanclass="math display">\[L_{I T M}=-y \log p-(1-y) \log (1-p)\]</span> 其中 <span class="math inline">\(y \in {0,1}\)</span>，<spanclass="math inline">\(p\)</span> 是 ITM 头部输出的匹配概率。</p><h4 id="语言建模损失lm"><strong>(3) 语言建模损失（LM）</strong></h4><p>用于文本生成任务，采用自回归损失： <span class="math display">\[L_{L M}=-\sum_{t=1}^N \log P\left(w_t \mid w_{&lt;t}, I\right)\]</span> 其中：</p><ul><li><span class="math inline">\(w_t\)</span> 是时间步 <spanclass="math inline">\(t\)</span> 处的单词。</li><li><span class="math inline">\(P(w_t | w_{&lt;t}, I)\)</span>是解码器生成的概率。</li></ul><h3 id="数据增强captioning-and-filteringcapfilt">5.2数据增强：Captioning and Filtering（CapFilt）</h3><p align="center"><img src="2.png" style="zoom:60%;" /></p><ul><li>CapFilt（Captioning and Filtering）的流程可以概括为以下步骤：<ol type="1"><li><strong>初始化模型</strong>：基于预训练的多模态混合编码器-解码器（MED），分别微调两个模块：<ul><li><strong>Captioner（生成器）</strong>：使用 MED的解码器部分，在高质量数据集（如COCO）上微调，使其能够根据输入图像生成文本描述。</li><li><strong>Filter（过滤器）</strong>：使用 MED的编码器部分，在同一高质量数据集上微调，使其能判断图像-文本对是否匹配（例如，通过图像-文本匹配损失ITM）。</li></ul></li><li><strong>生成合成标题</strong>： 对网络爬取的图像（<spanclass="math inline">\(I_w\)</span>），使用 Captioner 生成合成标题（<spanclass="math inline">\(T_s\)</span>）。生成时采用<strong>核采样（nucleussampling）</strong>（而非确定性方法如束搜索），以增加标题的多样性。例如，对一张海滩图片，Captioner可能生成“阳光下的沙滩与椰树”或“海边度假的宁静场景”等多样化描述。</li><li><strong>过滤噪声数据</strong>： 将原始网络文本（<spanclass="math inline">\(T_w\)</span>）和合成文本（<spanclass="math inline">\(T_s\)</span>​）输入Filter，判断它们与对应图像的匹配程度。Filter会为每对图像-文本计算匹配概率，若概率低于阈值（或通过二分类），则判定为噪声并剔除。例如，原始网络文本“我的周末旅行”可能因与图像内容无关而被过滤，而合成文本“蓝天下的沙滩与遮阳伞”则被保留。</li><li><strong>构建增强数据集</strong>：将过滤后的高质量文本（包括保留的原始文本 <spanclass="math inline">\(T_w^{\prime}\)</span> 和合成文本 <spanclass="math inline">\(T_s^{\prime}\)</span>）与人工标注数据（如 COCO的<spanclass="math inline">\(T_h\)</span>）合并，形成最终训练集。例如，原始 14M噪声数据经过 CapFilt 处理后，可能保留 10M 高质量数据，并与 COCO 的 50万标注数据结合，用于训练更鲁棒的模型。</li><li><strong>重新预训练模型</strong>：使用增强后的数据集对MED模型进行预训练，联合优化对比学习（ITC）、匹配（ITM）和生成（LM）目标。</li></ol></li></ul><h2 id="evaluation">6. Evaluation</h2><ul><li><strong>图像-文本检索</strong>：BLIP 在COCO 数据集上达到了 80.6%的文本检索召回率 <span class="citation"data-cites="1">@1</span>（TR@1）和 63.1% 的图像检索召回率 <spanclass="citation" data-cites="1">@1</span>（IR@1），在 Flickr30K数据集上达到了 96.0% 的 TR@1 和 85.0% 的 IR@1，均优于现有方法。</li><li><strong>图像字幕生成</strong>：在 COCO 数据集上，BLIP 的CIDEr分数达到了 129.7，SPICE 分数达到了 14.4；在NoCaps 数据集上，CIDEr分数达到了 105.1，SPICE 分数达到了 14.4，显著优于其他方法。</li><li><strong>视觉问答（VQA）</strong>：BLIP 在 VQA2.0 数据集上达到了78.24% 的准确率，优于 ALBEF 等方法。</li><li><strong>零样本视频语言任务</strong>：在 MSRVTT 数据集上，BLIP在零样本文本到视频检索任务中达到了 43.3%的召回率@1，显著优于其他方法。</li></ul><h2 id="conclusion">7. Conclusion</h2><p>BLIP通过提出新的模型架构和数据引导方法，在多种视觉-语言任务上取得了显著的性能提升，并展现了强大的泛化能力。该研究证明了通过优化数据质量和模型架构来提升视觉-语言预训练模型性能的有效性。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Pre-training</tag>
      
      <tag>Fine-tuning</tag>
      
      <tag>MLLMs</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Masked Autoencoders Are Scalable Vision Learners</title>
    <link href="/2025/03/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Masked-Autoencoders-Are-Scalable-Vision-Learners/"/>
    <url>/2025/03/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Masked-Autoencoders-Are-Scalable-Vision-Learners/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: Masked Autoencoders Are Scalable VisionLearners<br /><strong>Link</strong>: <a href="https://arxiv.org/pdf/2111.06377">MAEPaper</a><br /><strong>Source</strong>: Conference on Computer Vision and PatternRecognition (CVPR)<br /><strong>Date</strong>: 2021.11.11</p><h2 id="summary">2. Summary</h2><p>该论文提出了一种简单但有效的自监督学习方法——<strong>MaskedAutoencoders (MAE)</strong>，用于视觉表示学习。其核心思想是：</p><ol type="1"><li><strong>随机屏蔽输入图像的一部分，并仅基于可见部分进行重建</strong>。</li><li>采用非对称的编码器-解码器架构：<ul><li>编码器（encoder）仅处理未屏蔽的可见部分，不使用mask token。</li><li>解码器（decoder）使用轻量级设计，从编码表示和masktokens恢复原始图像。</li></ul></li><li><strong>高屏蔽率（75%）提高训练效率和任务难度</strong>，促进模型学习有意义的视觉特征。</li></ol><p>该方法实现了：</p><ul><li><strong>高效的训练（加速3倍以上）</strong>，同时提高了模型的泛化能力。</li><li><strong>在ImageNet-1K上，ViT-Huge 87.8%最高精度</strong>，超越其他仅使用ImageNet-1K数据的方法。</li><li><strong>在多个下游任务（目标检测、语义分割）中优于监督预训练</strong>，表现出良好的扩展性。</li></ul><h2 id="background">3. Background</h2><p>深度学习模型对数据的需求不断增长，NLP领域已广泛采用<strong>自监督学习（SSL）</strong>，例如：</p><ul><li><strong>GPT系列</strong>（基于自回归建模）</li><li><strong>BERT</strong>（基于Masked Language Modeling）</li></ul><p>然而，计算机视觉中的自监督学习进展相对滞后，原因包括：</p><ol type="1"><li><strong>架构差异</strong>：早期视觉模型主要是CNN，不适合直接引入masktokens或位置编码。而ViT（Vision Transformer）的出现消除了这一障碍。</li><li>信息密度差异：<ul><li>NLP中的文本高度语义化，预测缺失单词可促进模型理解句子结构。</li><li>视觉数据存在空间冗余，许多缺失部分可通过邻近区域推断，降低了任务难度。</li></ul></li><li>解码器的角色不同：<ul><li>NLP解码器生成语义丰富的词。</li><li>视觉解码器重建像素，导致学习到的特征更偏向低级信息。</li></ul></li></ol><h2 id="research-objective">4. Research Objective</h2><ul><li>探索Masked Autoencoders在计算机视觉中的可行性和扩展性。</li><li>设计高效的自监督学习方法，使其适用于大规模视觉任务。</li></ul><h2 id="method">5. Method</h2><p align="center"><img src="1.png" style="zoom:60%;" /></p><h3 id="输入图像的-masking-策略">5.1 <strong>输入图像的 Masking策略</strong></h3><ul><li>将图像划分为固定大小（如16×16）的 patches（与 ViT 一致）。</li><li>随机屏蔽高比例的patches（默认75%），保证剩余部分的信息密度足够高，以促进学习。</li></ul><h3 id="非对称编码器-解码器架构">5.2<strong>非对称编码器-解码器架构</strong></h3><ul><li>编码器（Encoder）：<ul><li>仅处理未被 mask 的 patches（不同于 BERT，它不会插入 masktokens）。</li><li>采用标准 ViT，但计算量仅为完整 ViT 的 25%。</li></ul></li><li>解码器（Decoder）：<ul><li>仅在预训练阶段使用，采用轻量级设计。</li><li>输入包括编码器输出的 visible patches + mask tokens。</li><li>采用 Transformer 架构，最终输出重建的像素值。</li></ul></li></ul><h3 id="重建目标reconstruction-target">5.3<strong>重建目标（Reconstruction Target）</strong></h3><ul><li><p>直接在像素空间（pixel space）计算均方误差（MSE）损失： <spanclass="math display">\[\mathcal{L}=\frac{1}{|\mathcal{M}|} \sum_{i \in\mathcal{M}}\left\|x_i-\hat{x}_i\right\|^2\]</span> 其中：</p><ul><li><span class="math inline">\(\mathcal{M}\)</span> 为被 mask 的 patch集合。</li><li><span class="math inline">\(x_i\)</span> 为原始图像 patch。</li><li><span class="math inline">\(\hat{x}_i\)</span> 为模型重建的patch。</li></ul></li></ul><p>该设计类似于传统去噪自编码器（Denoising Autoencoder,DAE），但更高效。</p><h2 id="conclusion">6. Conclusion</h2><p>MAE作为一种简单的自监督学习方法，在计算机视觉领域展现了与自然语言处理中BERT 类似的可扩展性。通过高比例掩码策略和非对称编码器-解码器设计，MAE能够高效地学习视觉表示，并在多种下游任务中超越监督预训练方法。此外，MAE的像素级重建方法比基于标记的方法更简单、更高效，且不依赖于额外的预训练步骤。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Generative Adversarial Nets</title>
    <link href="/2025/02/26/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Generative-Adversarial-Nets/"/>
    <url>/2025/02/26/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Generative-Adversarial-Nets/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: Generative Adversarial Nets<br /><strong>Link</strong>: <ahref="https://papers.nips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf">GANPaper</a><br /><strong>Source</strong>: Annual Conference on Neural InformationProcessing Systems (NeurIPS)<br /><strong>Date</strong>: 2014.06.10</p><h2 id="summary">2. Summary</h2><p>本文提出了一种新的生成模型训练框架——生成对抗网络（GAN），通过对抗过程同时训练两个模型：</p><ol type="1"><li><strong>生成器（G）</strong>：学习数据分布，生成逼真样本以欺骗判别器；</li><li><strong>判别器（D）</strong>：区分真实数据与生成数据。核心贡献包括：</li></ol><ul><li>提出极小极大博弈目标函数，无需马尔可夫链或近似推断；</li><li>理论证明全局最优解为生成分布等于真实数据分布（<spanclass="math inline">\(p_g=p_{\text {data}}\)</span>），此时判别器输出概率恒为1/2；</li><li>实验验证框架有效性，生成样本质量与现有模型相当，且计算效率更高。</li></ul><h2 id="background">3. Background</h2><p>深度学习的主要成功集中在判别模型上，这些模型将高维的感知输入映射到类标签。然而，深度生成模型的影响较小，主要是因为在最大似然估计等策略中存在很多计算难题。为了解决这些问题，本文提出了生成对抗网络（GAN），它通过对抗过程训练生成模型，避免了传统生成模型的复杂性。</p><h2 id="research-objective">4. Research Objective</h2><ul><li><strong>核心目标</strong>：设计一种高效生成模型框架，避免复杂概率计算，直接通过对抗博弈逼近真实数据分布。</li><li><strong>关键要求</strong>：<ul><li>生成器与判别器均为可微函数（如多层感知机）；</li><li>训练过程仅依赖反向传播，无需近似推断。</li></ul></li></ul><h2 id="method">5. Method</h2><h3 id="对抗框架">5.1 对抗框架</h3><ul><li><strong>生成器（G）</strong>：输入噪声 <span class="math inline">\(z\sim p_z(z)\)</span>，输出生成样本 <spanclass="math inline">\(G(z)\)</span>；</li><li><strong>判别器（D）</strong>：输入样本 <spanclass="math inline">\(x\)</span>，输出 <span class="math inline">\(D(x)\in[0,1]\)</span> 表示其为真实数据的概率。</li></ul><h3 id="目标函数">5.2 目标函数</h3><p><strong>极小极大博弈</strong>： <span class="math display">\[\min _G \max _D V(D, G)=\mathbb{E}_{x \sim p_{\text {data }}}[\logD(x)]+\mathbb{E}_{z \sim p_z}[\log (1-D(G(z)))]\]</span></p><ul><li><strong>判别器优化</strong>：固定 G，最大化 <spanclass="math inline">\(V(D, G)\)</span>，即区分真实与生成样本；</li><li><strong>生成器优化</strong>：固定 D，最小化 <spanclass="math inline">\(V(D, G)\)</span>，即生成样本欺骗D。</li></ul><h3 id="训练策略">5.3 训练策略</h3><ul><li><strong>交替训练</strong>：每轮先更新判别器 k 次，再更新生成器 1次，防止过拟合；</li><li><strong>梯度修正</strong>：早期训练中，将生成器目标从 <spanclass="math inline">\(\min \log (1-D(G(z)))\)</span> 改为 <spanclass="math inline">\(\max \logD(G(z))\)</span>，缓解梯度消失问题。</li></ul><p>在训练初期，生成器 <em>G</em>的性能通常较差，生成的样本与真实样本差异较大。此时，判别器 <em>D</em>可以很容易地区分生成样本和真实样本，因此 <spanclass="math inline">\(D(G(z))\)</span> 的值会非常接近 0（表示 <em>D</em>认为生成样本是假的）。此时，生成器 <em>G</em> 的目标函数为： <spanclass="math display">\[\log (1-D(G(z)))\]</span> 由于 <span class="math inline">\(D(G(z)) \approx0\)</span>，因此 <span class="math inline">\(1-D(G(z)) \approx1\)</span>，进而： <span class="math display">\[\log (1-D(G(z))) \approx \log (1)=0\]</span> 这意味着生成器 <em>G</em> 的目标函数在训练初期接近于0，导致梯度非常小，甚至接近于零。这种情况下，生成器很难通过梯度下降进行有效的更新，从而出现梯度消失问题。</p><p>为了解决这一问题，论文提出将生成器 <em>G</em> 的目标改为最大化 <spanclass="math inline">\(\logD(G(z))\)</span>。这种目标函数在数学上等价于原始目标函数，但在训练初期可以提供更强的梯度。具体来说：</p><p>当 <span class="math inline">\(D(G(z)) \approx 0\)</span> 时，<spanclass="math inline">\(\log D(G(z))\)</span> 的梯度为： <spanclass="math display">\[\nabla_{\theta_g} \log D(G(z))=\frac{1}{D(G(z))} \cdot \nabla_{\theta_g}D(G(z))\]</span> 此时，分母 <em>D</em>(<em>G</em>(<em>z</em>))非常小，导致整体梯度非常大，从而为生成器 <em>G</em>提供了更强的更新信号。</p><h3 id="理论分析">5.4 理论分析</h3><p>在非参数极限下，假设 <em>G</em> 和 <em>D</em>具有足够的容量，可以证明：</p><ol type="1"><li><p><strong>最优判别器</strong>：对于固定的 <em>G</em>，最优判别器<span class="math inline">\(D_G^*(x)\)</span> 为： <spanclass="math display">\[D_G^*(x)=\frac{p_{\text {data }}(x)}{p_{\text {data }}(x)+p_g(x)}\]</span> 其中 <span class="math inline">\(p_g(x)\)</span>是生成模型的分布。</p></li><li><p><strong>全局最优性</strong>：当 <spanclass="math inline">\(p_g=p_{\text {data }}\)</span>时，博弈的全局最小值 <span class="math inline">\(C(G)=-\log4\)</span>被达到，此时 <spanclass="math inline">\(D_G^*(x)=0.5\)</span>。</p></li></ol><h2 id="evaluation">6. Evaluation</h2><p>为了评估生成对抗网络的性能，作者在多个数据集上进行了实验，包括MNIST、Toronto Face Database (TFD) 和 CIFAR-10。通过使用高斯 Parzen窗口方法对生成样本进行似然估计，结果表明生成对抗网络在生成样本的质量上表现出色，且在生成的样本上具有竞争力。</p><h2 id="conclusion">7. Conclusion</h2><p>生成对抗网络（GANs）通过对抗训练的方式，成功地避免了传统生成模型中复杂的概率计算和推理过程，仅通过反向传播即可训练生成模型。实验结果表明，GANs在生成高质量样本方面具有显著潜力。然而，GANs也存在一些缺点，例如没有显式的概率分布表示，且在训练过程中需要保持生成器和判别器的同步。未来的研究方向包括扩展到条件生成模型、学习近似推理、半监督学习以及提高训练效率等。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CV</tag>
      
      <tag>AIGC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - MIKE: A New Benchmark for Fine-grained Multimodal Entity Knowledge Editing Editing</title>
    <link href="/2025/02/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-MIKE-A-New-Benchmark-for-Fine-grained-Multimodal-Entity-Knowledge-Editing/"/>
    <url>/2025/02/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-MIKE-A-New-Benchmark-for-Fine-grained-Multimodal-Entity-Knowledge-Editing/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: MIKE: A New Benchmark for Fine-grainedMultimodal Entity Knowledge Editing<br /><strong>Link</strong>: <a href="https://arxiv.org/pdf/2402.14835">PaperLink</a><br /><strong>Source</strong>: ACL 2024 Findings<br /><strong>Date</strong>: 2024.02.28</p><h2 id="summary">2. Summary</h2><ol type="1"><li><strong>提出 MIKE基准测试</strong>：该基准用于细粒度多模态实体知识编辑（Fine-grainedMultimodal Entity Knowledge Editing, FG-MKE）。</li><li><strong>设计三类任务</strong>：<ul><li>Vanilla Name Answering (VNA)：MLLMs 需回答图片中实体的名称。</li><li>Entity-Level Caption (ELC)：MLLMs需生成带有实体名称的描述性字幕。</li><li>Complex-Scenario Recognition (CSR)：在复杂场景中识别目标实体。</li></ul></li><li><strong>引入多步知识编辑（Multi-Step Editing）</strong>：测试 MLLMs在多轮编辑中的学习效率。</li><li><strong>评估当前方法</strong>：通过广泛实验，发现现有 MKE 方法在 FG任务上存在显著挑战。</li></ol><h2 id="section"></h2><h2 id="background">3. Background</h2><p>多模态知识编辑 (MKE) 对于维护和提升多模态大语言模型 (MLLMs)的准确性至关重要。然而，现有的基准测试主要集中在粗粒度知识上，对于细粒度(FG)多模态实体知识的编辑研究相对较少。FG 实体识别对于 MLLMs在现实世界场景中的实际部署和有效性至关重要。例如，在政治图像描述中，理想的输出应该是“总统乔·拜登抵达白宫”，而粗粒度方法可能仅生成“一位白发老人抵达建筑物”这样的描述，缺乏关键细节。因此，研究FG 实体知识编辑对于提升 MLLMs 的性能具有重要意义。</p><h2 id="research-objective">4. Research Objective</h2><p>本文的主要目标是探索如何有效地将 FG 多模态实体知识编辑到 MLLMs中。具体目标包括：</p><ol type="1"><li>提供一个专门针对 FG 多模态实体知识编辑的综合基准测试和数据集。</li><li>设计多个任务来评估 MLLMs 在 FG 实体识别和描述方面的能力。</li><li>引入多步编辑形式，以评估编辑效率并探索 MLLMs在不同编辑图像数量下的适应性和学习效率。</li></ol><h2 id="method">5. Method</h2><h3 id="mike-数据集构建">5.1 MIKE 数据集构建</h3><ul><li><p>选择 OVEN 数据集中 1500 个 FG 实体，每个实体至少 5张图片。</p></li><li><p>采用 4 个筛选标准：</p><ul><li><strong>可观察性（Observable）</strong>：实体必须具备视觉特征。</li><li><strong>具体性（Specific）</strong>：排除抽象或广义概念。</li><li><strong>唯一性（Unambiguous）</strong>：避免歧义实体（如 Apple代表公司还是水果）。</li><li><strong>单一性（Unitary）</strong>：每张图片只包含一个目标实体。</li></ul></li><li><p>最终得到 1103 个 FG 实体。</p></li><li><p><strong>数据过滤</strong>：通过预训练的 MLLMs检查实体是否已被模型识别，以确保目标实体未被预编码。</p></li><li><p><strong>数据统计</strong>：使用 t-SNE 对 FG实体图像的嵌入进行可视化，发现同一超类别的嵌入形成了紧凑的簇，表明 FG实体在每个超类别中具有相似的表示，这为 MKE 带来了挑战。</p></li></ul><h3 id="任务设计">5.2 任务设计</h3><ol type="1"><li><strong>Vanilla Name Answering (VNA)</strong>：要求 MLLMs在编辑后识别图像中的目标实体并给出其简短名称。</li><li><strong>Entity-Level Caption (ELC)</strong>：要求 MLLMs生成包含目标实体名称的详细图像描述。</li><li><strong>Complex-Scenario Recognition(CSR)</strong>：在包含多个实体的复杂场景中识别目标 FG 实体。</li></ol><h3 id="多步编辑multi-step-editing">5.3 多步编辑（Multi-StepEditing）</h3><ul><li><strong>方法</strong>：使用 2-4张图像逐步编辑同一实体，提升模型对实体特征的泛化能力。</li><li><strong>动机</strong>：单张图像覆盖特征有限，多步编辑可增强视觉-文本对齐。</li></ul><h2 id="evaluation">6. Evaluation</h2><h3 id="实验设置">6.1 实验设置</h3><ul><li><strong>模型</strong>：BLIP-2 (2.7B/6.7B)、MiniGPT-4 (7.3B)。</li><li><strong>基线方法</strong>：MEND、SERAC、IKE。</li><li><strong>指标</strong>：采用实体精确匹配准确率 (entity exact matchaccuracy)，同时考虑可靠性 (Reliability)、泛化性 (Generality) 和局部性(Locality)。</li></ul><h4 id="实验结果">6.2 实验结果</h4><ul><li><strong>VNA 任务</strong>：IKE 在 VNA 任务中表现最佳，但在 ELC的图像泛化和文本泛化方面表现较差。</li><li><strong>ELC 任务</strong>：所有编辑方法在 ELC任务中的图像泛化能力最弱，表明该任务对 MLLMs 的挑战最大。</li><li><strong>CSR任务</strong>：多步编辑显著提高了模型的可靠性、图像泛化能力和文本泛化能力。</li><li><strong>模型大小影响</strong>：模型大小对性能的影响不明显，表明知识编辑不需要将大量知识编码到MLLMs 中。</li></ul><h2 id="conclusion">7. Conclusion</h2><h3 id="研究发现">7.1 研究发现：</h3><ol type="1"><li>MIKE 评估了 FG 多模态知识编辑的挑战。</li><li>现有 MKE 方法在 ELC 任务上的表现较差，表明 MLLMs在细粒度实体描述方面存在不足。</li><li>多步知识编辑可以显著提高 MLLMs 的识别能力。</li><li>模型大小对编辑效果影响不大，表明 MKE主要依赖编辑方法，而非模型规模。</li></ol><h3 id="未来工作">7.2 未来工作：</h3><ol type="1"><li>扩展 FG 实体数据集。</li><li>评估更多知识编辑方法。</li><li>提出新的知识编辑方法，提高编辑效果。</li></ol>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>知识编辑</tag>
      
      <tag>多模态</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Plug-and-Play Adaptation for Continuously-updated QA</title>
    <link href="/2025/01/26/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Plug-and-Play-Adaptation-for-Continuously-updated-QA/"/>
    <url>/2025/01/26/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Plug-and-Play-Adaptation-for-Continuously-updated-QA/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: Plug-and-Play Adaptation forContinuously-updated QA<br /><strong>Link</strong>: <a href="https://arxiv.org/pdf/2204.12785">PPAPaper</a><br /><strong>Source</strong>: Annual Meeting of the Association forComputational Linguistics (ACL)<br /><strong>Date</strong>: 2022.04.27</p><h2 id="summary">2. Summary</h2><p>本文提出了一种新的任务——连续更新问答 (Continuously-updated QA,CuQA)，旨在评估语言模型 (LMs) 作为隐式知识库 (KBs)时，如何在保留现有知识的同时，有效地添加和更新知识。作者通过引入插件模块，有效地处理了大规模知识更新的问题，并在zsRE QA 和 NQ数据集上进行了实验，结果表明该方法在更新/遗忘比率上比传统的微调方法高出4 倍。</p><h2 id="background">3. Background</h2><p>语言模型 (LMs) 因其出色的事实知识记忆能力，被广泛用作隐式知识库(KBs)。然而，为了保持与不断变化的世界同步，LMs需要定期更新知识。现有的评估 LMs 作为 KBs 的任务，如 LAMA 和CBQA，未能充分考虑大规模知识更新的情况。因此，作者提出了 CuQA任务，以测试 LMs 在连续注入知识更新时的能力。</p><h2 id="research-objective">4. Research Objective</h2><p>本文旨在开发一种方法，使得问答模型能够：</p><ol type="1"><li>在无需完全重训的情况下快速适配新知识；</li><li>在适配新知识时，保留对旧知识的记忆；</li><li>提高对持续知识更新场景的问答性能。</li></ol><h2 id="method">5. Method</h2><h3 id="问题定义">5.1 问题定义</h3><p>CuQA 任务分为两个阶段：</p><ol type="1"><li>训练初始模型 <spanclass="math inline">\(\theta_{\text{old}}\)</span>存储<strong>原始知识</strong> <spanclass="math inline">\(K_s\)</span>。</li><li>在模型中注入<strong>目标知识</strong> <spanclass="math inline">\(K_t\)</span>，获得更新后的模型 <spanclass="math inline">\(\theta_{\text{new}}\)</span>。目标是使 <spanclass="math inline">\(\theta_{\text{new}}\)</span> 在学习 <spanclass="math inline">\(K_t\)</span> 时，尽量减少对 <spanclass="math inline">\(K_s\)</span> 的遗忘。</li></ol><p>指标定义：</p><ul><li><strong>目标知识准确率</strong>: 衡量 <spanclass="math inline">\(\theta_{\text{new}}\)</span> 对 <spanclass="math inline">\(K_t\)</span> 的记忆能力。</li><li><strong>原始知识准确率</strong>: 衡量 <spanclass="math inline">\(\theta_{\text{new}}\)</span> 遗忘 <spanclass="math inline">\(K_s\)</span> 的程度。</li><li><strong>F/U 比率</strong>:遗忘/更新的比率，反映学习一个新知识时遗忘旧知识的数量。</li></ul><h3 id="插拔式适配方法">5.2 插拔式适配方法</h3><p align="center"><img src="1.png" style="zoom:30%;" /></p><p><strong>核心思想</strong>:使用可选模块存储新知识，避免修改原模型参数，结合低秩矩阵实现高效更新。</p><ol type="1"><li><p><strong>参数扩展公式</strong>: 插入新模块 <spanclass="math inline">\(g(x)\)</span>，存储目标知识。整体输出为： <spanclass="math display">\[h=f(x)+\sigma(q) \cdot g(x)\]</span></p><ul><li><span class="math inline">\(f(x)\)</span>:原模型的输出，固定参数。</li><li><span class="math inline">\(g(x)\)</span>:插入的扩展模块，用于存储目标知识。</li><li><span class="math inline">\(\sigma(q)\)</span>: 选择器，决定是否激活<span class="math inline">\(g(x)\)</span>。</li></ul></li><li><p><strong>选择器定义</strong>: 通过最近邻匹配计算输入 <spanclass="math inline">\(q\)</span> 与目标知识 <spanclass="math inline">\(K_t\)</span> 的相似性： <spanclass="math display">\[s_q=\max _{m_i \in M} \operatorname{sim}\left(m_i, q\right)\]</span></p><p><span class="math display">\[\sigma(q)= \begin{cases}1 &amp; \text { 如果 } s_q \geq \delta \\ 0&amp; \text { 如果 } s_q&lt;\delta\end{cases}\]</span></p></li></ol><p>其中 <span class="math inline">\(s_q\)</span> 是查询嵌入与记忆嵌入<span class="math inline">\(M\)</span> 中最近邻之间的余弦相似度。</p><ol start="3" type="1"><li><strong>多组知识更新的一般情况</strong>：模型可以被表达为：</li></ol><p><span class="math display">\[h=f(x)+\sum_{k=1}^M \sigma_k(q) \cdot g_k(x)\]</span></p><ul><li><span class="math inline">\(f(x)\)</span>:基础模型的固定函数，用于表示初始的知识存储（未更新）。</li><li><span class="math inline">\(g_k(x)\)</span>: 第 <spanclass="math inline">\(k\)</span> 个知识更新的适配函数，用于添加目标知识<span class="math inline">\(K_{tk}\)</span>。</li><li><span class="math inline">\(\sigma_k(q)\)</span>:开关函数，用于根据查询 <span class="math inline">\(q\)</span>动态激活相关知识适配函数 <spanclass="math inline">\(g_k(x)\)</span>。</li></ul><p>训练阶段:</p><p>在训练第 <span class="math inline">\(j\)</span> 个目标知识 <spanclass="math inline">\(K_{tj}\)</span> 时，开关函数 <spanclass="math inline">\(\sigma_k(q)\)</span> 会被激活，其中 <spanclass="math inline">\(1 \leq k \leq j\)</span>。也就是说，模型只会激活当前和之前的知识更新模块。</p><p>推理阶段:</p><p>在推理时，利用一个<strong>最近邻（NN）选择器</strong>从存储模块中选出与查询最相似的知识组：<span class="math display">\[m^*=\arg \max (\operatorname{sim}(m, q)), \quad m \in M_1: M\]</span></p><ul><li><span class="math inline">\(m^*\)</span>:最近邻选择出的最相关知识单元。</li><li><span class="math inline">\(\text{sim}(m, q)\)</span>: 表示知识单元<span class="math inline">\(m\)</span> 与查询 <spanclass="math inline">\(q\)</span> 的相似度。</li></ul><p>如果 <span class="math inline">\(m^* \inM_j\)</span>，则说明查询需要访问第 <spanclass="math inline">\(j\)</span> 次知识更新。此时，开关函数 <spanclass="math inline">\(\sigma_k(q)\)</span> 的激活规则如下： <spanclass="math display">\[\sigma_k(q)= \begin{cases}1, &amp; \text { 如果 } s_q \geq \delta \text{ 且 } 1 \leq k \leq j, \\ 0, &amp; \text { 如果 } s_q&lt;\delta.\end{cases}\]</span></p><ul><li><span class="math inline">\(s_q\)</span>: 表示查询 <spanclass="math inline">\(q\)</span> 的激活信号强度。</li><li><span class="math inline">\(\delta\)</span>: 预设的激活阈值。</li><li><span class="math inline">\(k \leq j\)</span>:保证只激活当前及之前的适配模块。</li></ul><p>在上述机制下，最终的知识表示是所有被激活模块的累积函数： <spanclass="math display">\[\sum_{k=1}^j g_k(x)\]</span> <strong>累积的意义</strong>:累积不同模块的知识表示可以捕捉多层次的信息**，即既包含较早知识的基础信息，也包含最新知识的细化信息。</p><p><strong>示例</strong>:</p><ul><li><p>如果我们在训练中依次学习了多个知识模块 <spanclass="math inline">\(K_{t 1}, K_{t 2}, K_{t3}\)</span>，它们之间可能是关联的。比如：</p><ul><li><p><span class="math inline">\(K_{t1}\)</span>:基础常识（语言结构）。</p></li><li><p><span class="math inline">\(K_{t2}\)</span>:特定领域知识（医学）。</p></li><li><p><span class="math inline">\(K_{t3}\)</span>:新的医学技术（补充知识）。</p><p>使用累积函数可以保证对每层知识的组合理解，而不仅仅是最新一组。</p></li></ul></li></ul><h2 id="evaluation">6. Evaluation</h2><ol type="1"><li><strong>数据集</strong>:<ul><li><strong>zsRE</strong>: 包含关系特定的 QA 对，分为原始知识 <spanclass="math inline">\(K_s\)</span>、目标知识 <spanclass="math inline">\(K_t\)</span> 和未见问题集 <spanclass="math inline">\(P_s\)</span>、<spanclass="math inline">\(P_t\)</span>。</li><li><strong>NQ + SituatedQA</strong>: 使用 NQ 作为 <spanclass="math inline">\(K_s\)</span>，将时间相关的 QA 配对作为 <spanclass="math inline">\(K_t\)</span>。</li></ul></li><li><strong>实验结果</strong>:<ul><li><strong>准确率</strong>: 在 <spanclass="math inline">\(K_s\)</span>、<spanclass="math inline">\(K_t\)</span> 上均表现出色，显著优于基线模型。</li><li><strong>F/U 比率</strong>: 插拔式方法的遗忘率最低，仅为 Fine-tuning的 1/4。</li></ul></li><li><strong>消融实验</strong>:<ul><li>研究不同模块（如前馈层、注意力层）对性能的影响，发现前馈层对知识记忆更重要。</li></ul></li></ol><h2 id="conclusion">7. Conclusion</h2><p>本文提出了一个名为 CuQA 的新任务和一种使用插件模块高效更新 LMs的方法。该方法在保留现有知识的同时添加新知识，展示了在 zsRE 和 NQ数据集上的优越性能。提出的方法具有可扩展性，能够处理多次大规模知识更新。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>NLP</tag>
      
      <tag>知识编辑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Editable Neural Networks</title>
    <link href="/2025/01/26/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Editable-Neural-Networks/"/>
    <url>/2025/01/26/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Editable-Neural-Networks/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: Editable Neural Networks<br /><strong>Link</strong>: <ahref="http://arxiv.org/pdf/2004.00345">Editable Training Paper</a><br /><strong>Source</strong>: International Conference on LearningRepresentations (ICLR)<br /><strong>Date</strong>: 2020.04.01</p><h2 id="summary">2. Summary</h2><p>这篇论文提出了一种新方法，称为 <strong>EditableTraining</strong>，用于快速高效地修正神经网络在特定样本上的错误，而不会影响其他样本的行为。核心贡献包括：</p><ul><li>提出并定义了神经网络编辑的问题。</li><li>开发了一种基于元学习 (meta-learning)的通用训练技术，使得模型可以快速编辑。</li><li>在图像分类和机器翻译等任务上进行了大规模实验，证明了方法的有效性。</li></ul><h2 id="background">3. Background</h2><p>深度神经网络在图像分类、机器翻译等任务中表现出色，但模型错误可能导致严重后果。因此，快速修正模型错误至关重要。然而，深度神经网络的预测通常依赖于所有模型参数，更新模型以改变单个输入的预测可能会降低其他输入的性能。目前，修正模型错误的方法包括重新训练模型或使用手动缓存，但这些方法要么计算成本高昂，要么对输入变化不够鲁棒。</p><h2 id="research-objective">4. Research Objective</h2><p>本研究旨在设计一种训练方法，使得神经网络模型可以在检测到错误后快速修正特定输入的预测，而不显著影响其他输入的性能。</p><h2 id="method">5. Method</h2><h3 id="神经网络编辑的定义">5.1 神经网络编辑的定义</h3><p>神经网络表示为 <span class="math inline">\(f(x,\theta)\)</span>，其中 <span class="math inline">\(x\)</span>是输入，<span class="math inline">\(\theta\)</span>是参数集。为了修正错误，定义了一个编辑函数： <spanclass="math display">\[\theta&#39; = \text{Edit}(\theta, l_e)\]</span> 其中 <span class="math inline">\(l_e\)</span>是约束条件，用于强制模型对某些输入产生预期输出。</p><p>在分类任务中，定义 <span class="math inline">\(l_e\)</span> 为：<span class="math display">\[l_e\left(\theta^{\prime}\right)=\max _{y_i} \log p\left(y_i \mid x,\theta^{\prime}\right)-\log p\left(y_{\text {ref }} \mid x,\theta^{\prime}\right)\]</span> 其中 <span class="math inline">\(y_{\text{ref}}\)</span>是目标标签，约束条件 <span class="math inline">\(l_e(\theta&#39;) \leq0\)</span> 表示模型在目标样本上正确分类。</p><h3 id="梯度下降编辑器">5.2 梯度下降编辑器</h3><p>利用梯度下降实现参数更新： <span class="math display">\[\operatorname{Edit}_\alpha^k\left(\theta, l_e, k\right)=\begin{cases}\theta, &amp; \text { if } l_e(\theta) \leq 0 \text { or }k=0, \\ \operatorname{Edit}_\alpha^{k-1}\left(\theta-\alpha\nabla_\theta l_e(\theta), l_e\right), &amp; \text { else }\end{cases}\]</span>此编辑器可以通过引入如动量（Momentum）、自适应学习率（RMSProp、Adam）等优化技术提高效率和稳定性。</p><h3 id="editable-training-的目标函数">5.3 Editable Training的目标函数</h3><p>为了使模型适应编辑器，定义总目标函数： <span class="math display">\[\operatorname{Obj}\left(\theta, l_e\right)=L_{\text {base}}(\theta)+c_{\text {edit }} \cdot L_{\text {edit }}(\theta)+c_{\text{loc }} \cdot L_{\text {loc }}(\theta),\]</span> 其中：</p><ul><li><spanclass="math inline">\(L_{\text{base}}(\theta)\)</span>：主任务损失（如交叉熵）。</li><li><span class="math inline">\(L_{\text{edit}}(\theta) = \max(0,l_e(\text{Edit}_\alpha^k(\theta,l_e)))\)</span>：鼓励编辑效率和可靠性。</li><li><span class="math inline">\(L_{\text{loc}}(\theta) = \mathbb{E}_{x\sim p(x)} D_{\text{KL}}(p(y|x, \theta) \| p(y|x,\text{Edit}_\alpha^k(\theta,l_e)))\)</span>：通过最小化原始模型和编辑后模型的 KL散度，确保局部性。</li></ul><p>超参数 <span class="math inline">\(c_{\text{edit}}\)</span> 和 <spanclass="math inline">\(c_{\text{loc}}\)</span>用于平衡编辑性能与主任务表现。</p><h2 id="evaluation">6. Evaluation</h2><h3 id="cifar-10-实验">6.1 CIFAR-10 实验</h3><ul><li>使用 ResNet-18 模型，比较了不同编辑器（如GD、RMSProp）的性能，指标包括<strong>drawdown</strong>（编辑前后误差差异）、成功率、和平均编辑步数。</li><li>Editable Training 显著减少了 drawdown（例如，从 <spanclass="math inline">\(1.77\%\)</span> 降到 <spanclass="math inline">\(0.65\%\)</span>）。</li></ul><h3 id="imagenet-实验">6.2 ImageNet 实验</h3><ul><li>在预训练 ResNet-18 和 DenseNet-169模型上进行微调，使用额外的可训练层进一步提高编辑性能。</li><li>与 Elastic Weight Consolidation 和 Deep k-Nearest Neighbors等基线方法相比，Editable Training 在修正效果和效率上均占优。</li></ul><h2 id="conclusion">7. Conclusion</h2><p>本文提出的 Editable Training方法有效地解决了神经网络的错误修复问题，通过元学习技术增强了模型的编辑能力，同时保持整体性能，具有广泛的实际应用前景。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>NLP</tag>
      
      <tag>知识编辑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Memory-assisted prompt editing to improve GPT-3 after deployment</title>
    <link href="/2025/01/26/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Memory-assisted-prompt-editing-to-improve-GPT-3-after-deployment/"/>
    <url>/2025/01/26/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Memory-assisted-prompt-editing-to-improve-GPT-3-after-deployment/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: Memory-assisted prompt editing to improveGPT-3 after deployment<br /><strong>Link</strong>: <ahref="http://arxiv.org/pdf/2201.06009">MemPrompt Paper</a><br /><strong>Source</strong>: Empirical Methods in Natural LanguageProcessing (EMNLP)<br /><strong>Date</strong>: 2022.01.16</p><h2 id="summary">2. Summary</h2><p>MemPrompt 是一种用于增强 GPT-3部署后性能的新方法，旨在通过用户反馈的记忆模块改进模型输出而无需重新训练。核心思想是利用用户提供的交互式反馈，通过动态调整提示内容，解决模型误解用户意图的问题。在四种任务（词汇关系、单词重组、高级伦理推理两种变体）中，MemPrompt显著提高了 GPT-3 的任务准确率。</p><h2 id="background">3. Background</h2><p>现有的大规模语言模型（如GPT-3）虽然具有强大的生成能力，但仍会因缺乏常识而导致误解用户意图。例如，GPT-3可能将 "What word is similar to good?"理解为寻找同音词，而用户实际想要寻找同义词。传统解决方案如重新训练模型成本高昂，因此需要一种低成本的后部署优化方法。</p><p>MemPrompt利用递归记忆理论和用户反馈，动态调整提示以避免重复错误，从而实现模型行为的低成本改进。</p><h2 id="research-objective">4. Research Objective</h2><p>目标是通过交互式用户反馈提高 GPT-3 在不同任务中的表现，具体包括：</p><ol type="1"><li>利用记忆模块存储用户反馈，并将其应用于新问题。</li><li>动态更新提示以改进模型对用户意图的理解，而不需要重新训练模型。</li></ol><h2 id="method">5. Method</h2><p>MemPrompt 方法包括以下关键组件：</p><ol type="1"><li><strong>内存(M)</strong>：一个不断增长的键值对表，其中键是误解的问题，值是用户提供的澄清反馈。</li><li><strong>查找函数(M(x))</strong>：一个函数，将新查询与内存中的所有键进行匹配（使用余弦相似度），以找到类似的问题。</li><li><strong>组合器 (C(x,M(x)))</strong>：一个门控函数，如果相关，则将检索到的反馈与新查询连接起来。</li><li><strong>少样本提示</strong>：该方法使用 GPT-3的少样本提示，其中提示由几个输入输出示例组成，后跟新查询。提示通过从内存中检索到的相关反馈进行丰富。</li></ol><p>系统的工作原理如下：</p><ul><li>给定一个新查询 <spanclass="math inline">\(x\)</span>，模型生成一个输出 <spanclass="math inline">\(y\)</span> 和一个句子 <spanclass="math inline">\(u\)</span>，表达其对任务的理解。</li><li>用户可以对 <span class="math inline">\(u\)</span> 提供反馈 <spanclass="math inline">\(fb\)</span>，该反馈存储在内存中。</li><li>对于后续查询，系统从内存中检索相关反馈，并将其附加到提示中，帮助模型避免类似的误解。</li></ul><h2 id="conclusion">6. Conclusion</h2><p>MemPrompt提供了一种无需重新训练的低成本优化方法，通过记忆模块和用户反馈显著提高了GPT-3的性能。这种方法在实际部署中具有广阔应用前景，同时为交互式模型调试和动态提示优化提供了新思路。</p><h2 id="notes">7. Notes</h2><h3 id="举例说明-menprompt-的工作流程">7.1 举例说明 MenPrompt的工作流程。</h3><blockquote><h4 id="模型初始错误与用户反馈"><strong>1.模型初始错误与用户反馈</strong></h4><ol type="1"><li><p>用户输入问题：</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs applescript">What <span class="hljs-built_in">word</span> <span class="hljs-keyword">is</span> similar <span class="hljs-keyword">to</span> <span class="hljs-string">&quot;good&quot;</span>?<br></code></pre></td></tr></table></figure><ul><li><strong>输入</strong> <spanclass="math inline">\(x_1\)</span>：<code>What word is similar to "good"?</code></li></ul></li><li><p>GPT-3 的初始错误回答：</p><ul><li>输出 <spanclass="math inline">\(y_1\)</span>：<code>The homophone of "good" is: wood.</code></li><li>任务理解 <spanclass="math inline">\(u_1\)</span>：<code>The question is asking for a homophone.</code></li></ul></li><li><p>用户提供反馈：</p><ul><li><spanclass="math inline">\(fb_1\)</span>：<code>"Similar to" means "with similar meaning."</code></li></ul></li></ol><p><strong>处理</strong>：此时，用户发现 GPT-3错误地理解了任务（认为需要找“同音词”），并给出明确的反馈 <spanclass="math inline">\(fb_1\)</span>，该反馈表明问题实际是找“同义词”</p><h4 id="记忆模块更新"><strong>2. 记忆模块更新</strong></h4><ol type="1"><li><p>存储用户反馈：</p><ul><li>记忆模块 <span class="math inline">\(M\)</span> 中记录以下键值对：<span class="math display">\[M = \{ x_1 : fb_1 \}\]</span> 即，问题 <spanclass="math inline">\(x_1\)</span>（用户输入的问题）作为键，反馈 <spanclass="math inline">\(fb_1\)</span> 作为值存储到记忆中。</li></ul></li></ol><h4 id="第二次用户输入反馈检索"><strong>3.第二次用户输入（反馈检索）</strong></h4><ol type="1"><li><p>新问题输入：用户问：</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs applescript">What <span class="hljs-built_in">word</span> <span class="hljs-keyword">is</span> similar <span class="hljs-keyword">to</span> <span class="hljs-string">&quot;surprised&quot;</span>?<br></code></pre></td></tr></table></figure><ul><li><strong>输入</strong> <spanclass="math inline">\(x_2\)</span>：<code>What word is similar to "surprised"?</code></li></ul></li><li><p>检索相关反馈：</p><ul><li>检索模块 <span class="math inline">\(M(x_2)\)</span> 在记忆中查找与<span class="math inline">\(x_2\)</span> 相似的问题。</li><li>检索到的反馈：<code>"Similar to" means "with similar meaning."</code></li></ul></li></ol><h4 id="动态提示更新"><strong>4. 动态提示更新</strong></h4><ol type="1"><li><p>动态拼接提示：将检索到的反馈 <spanclass="math inline">\(fb_1\)</span> 附加到新问题中，构造新的提示： <spanclass="math display">\[p_{new} = x_2 + fb_1\]</span> 即，新的提示变为：</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">What word <span class="hljs-keyword">is</span> <span class="hljs-keyword">similar</span> <span class="hljs-keyword">to</span> &quot;surprised&quot;? <br><span class="hljs-keyword">Similar</span> <span class="hljs-keyword">to</span> means <span class="hljs-keyword">with</span> <span class="hljs-keyword">similar</span> meaning.<br></code></pre></td></tr></table></figure></li><li><p>GPT-3 输出改进：</p><ul><li>使用更新后的提示，GPT-3正确回答：<code>The synonym of "surprised" is: amazed.</code></li></ul></li></ol></blockquote>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>NLP</tag>
      
      <tag>知识编辑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions</title>
    <link href="/2025/01/26/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-MQuAKE-Assessing-Knowledge-Editing-in-Language-Models-via-Multi-Hop-Questions/"/>
    <url>/2025/01/26/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-MQuAKE-Assessing-Knowledge-Editing-in-Language-Models-via-Multi-Hop-Questions/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: MQuAKE: Assessing Knowledge Editing inLanguage Models via Multi-Hop Questions<br /><strong>Link</strong>: <a href="https://arxiv.org/pdf/2305.14795">MeLLoPaper</a><br /><strong>Source</strong>: Empirical Methods in Natural LanguageProcessing (EMNLP)<br /><strong>Date</strong>: 2023.05.24</p><h2 id="summary">2. Summary</h2><p>本文介绍了 MQuAKE (Multi-hop Question Answering for KnowledgeEditing)，这是一个用于评估大型语言模型 (LLMs)知识编辑效果的基准测试，通过多跳问题来评估。作者指出，当前的知识编辑方法虽然能够成功回忆编辑过的事实，但在需要基于编辑事实进行推理的多跳问题上表现糟糕。为此，他们提出了MeLLo (Memory-based Editing for Large LanguageModels)，这是一种新颖的方法，通过将编辑过的事实存储在外部记忆中，并迭代地提示语言模型生成一致的答案。MeLLo在处理多跳问题上显著优于现有方法，且无需额外训练。</p><h2 id="background">3. Background</h2><p>随着大型语言模型 (LLMs)在各种应用中的广泛部署，其知识迅速过时的问题日益凸显。由于重新训练这些模型的成本过高，因此出现了通过更新模型权重来注入新事实的技术。然而，现有的评估范式主要关注模型是否能够回忆起新注入的事实，而忽略了模型在编辑事实后是否能够处理答案应随之改变的问题。这一评估缺口至关重要，因为它反映了模型整合新知识并进行推理的能力。</p><h2 id="research-objective">4. Research Objective</h2><p>本研究的主要目标是开发一个全面的基准测试 MQuAKE，以评估 LLMs的知识编辑方法的有效性。具体来说，该基准旨在评估编辑后的模型是否能够正确回答多跳问题，这些问题的答案应基于编辑事实的逻辑后果而改变。此外，作者还旨在提出一种新的方法MeLLo，通过利用外部记忆存储编辑过的事实，并确保与编辑知识的一致性，从而有效处理多跳问题。</p><h2 id="method">5. Method</h2><h3 id="问题定义">5.1 <strong>问题定义</strong></h3><ol type="1"><li><p><strong>知识表示</strong></p><ul><li>事实表示为三元组 <span class="math inline">\((s, r,o)\)</span>，其中 <span class="math inline">\(s\)</span> 为主体，<spanclass="math inline">\(r\)</span> 为关系，<spanclass="math inline">\(o\)</span> 为客体。</li><li>模板 <span class="math inline">\(t_r(s)\)</span>用于将三元组转化为自然语言问题。</li></ul><p><strong>公式</strong>： <span class="math display">\[f^*(t_r(s)) = o^*\]</span> 其中 <span class="math inline">\(f^*\)</span>表示编辑后的模型。</p></li><li><p><strong>多跳问题构造</strong></p></li></ol><p align="center"><img src="1.png" style="zoom:30%;" /></p><p>多跳问题链： <span class="math display">\[   C=\left\langle\left(s_1, r_1, o_1\right),\left(s_2, r_2, o_2\right),\ldots,\left(s_n, r_n, o_n\right)\right\rangle   \]</span> 满足 <span class="math inline">\(o_i = s_{i+1}\)</span></p><ol start="3" type="1"><li><p><strong>知识编辑公式</strong></p><ul><li>单个编辑： <span class="math inline">\(e = (s, r, o \too^*)\)</span></li><li>多个编辑： <span class="math inline">\(K(f, E) = f^*\)</span> 其中<span class="math inline">\(K\)</span> 为知识编辑函数。</li></ul></li></ol><h3 id="mello-方法">5.2 <strong>Mello 方法</strong></h3><ul><li>提出了新的知识编辑方法 MeLLo，核心思想如下：<ol type="1"><li><strong>存储已编辑事实</strong>：将编辑事实存储在外部记忆中。</li><li><strong>逐步推理</strong>：将多跳问题分解为子问题，模型逐步回答并检查答案与已编辑事实的一致性。</li><li><strong>自我校验</strong>：模型检查推理步骤中是否违反已编辑知识，必要时用检索到的已编辑事实覆盖冲突答案，从而确保推理结果与已编辑知识一致。</li></ol></li><li>MeLLo 不需要额外训练，可以应用于大型黑箱 LMs，如 GPT-3.5。</li></ul><h2 id="conclusion">6. Conclusion</h2><p>本文提出了 MQuAKE，这是一个用于评估 LLMs知识编辑方法的基准测试，通过多跳问题来评估。结果表明，现有方法在处理多跳问题时效果不佳，突显了开发更忠实的知识编辑技术的必要性。提出的MeLLo方法通过利用外部记忆存储编辑过的事实，并通过迭代提示确保一致性，展示了卓越的性能。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>NLP</tag>
      
      <tag>知识编辑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Transformer-Patcher: One Mistake worth One Neuron</title>
    <link href="/2025/01/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Transformer-Patcher-One-Mistake-worth-One-Neuron/"/>
    <url>/2025/01/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Transformer-Patcher-One-Mistake-worth-One-Neuron/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: Transformer-Patcher: One Mistake worth OneNeuron<br /><strong>Link</strong>: <ahref="http://arxiv.org/pdf/2301.09785">T-Patcher Paper</a><br /><strong>Source</strong>: International Conference on LearningRepresentations<br /><strong>Date</strong>: 2023.01.24</p><h2 id="summary">2. Summary</h2><p>本文提出了一个名为 Transformer-Patcher 的方法，用于对基于 Transformer的预训练语言模型 (PLMs) 进行连续的错误修正。该方法通过在 Transformer模型的最后一层前馈网络 (FFN)中添加少量可训练的神经元（称为“patches”），实现了对模型行为的高效修正。实验表明，Transformer-Patcher能够在保持模型整体性能的同时，连续修正多达数千个错误，并且具有良好的泛化能力。该方法在顺序模型编辑(Sequential Model Editing, SME) 任务中达到了最先进的性能。</p><h2 id="background">3. Background</h2><p>基于 Transformer 的预训练语言模型 (PLMs) 在自然语言处理 (NLP)任务中取得了显著的成果，但仍然会偶尔产生错误输出。在工业环境中部署这些模型时，快速且稳健地修正这些错误对于提升用户体验至关重要。以往的研究主要关注单次模型编辑(Model Editing,ME)，即修正单个错误。然而，实际应用中模型会不断出现新的错误，且同一错误可能多次出现。因此，本文将ME 任务扩展到顺序模型编辑 (SME) 任务，旨在开发更实用的编辑方法。</p><h2 id="research-objective">4. Research Objective</h2><p>本文的主要目标是提出一种能够连续修正模型错误的方法，以满足实际应用中不断出现错误的场景需求。具体而言，该方法需要满足以下三个属性：</p><ol type="1"><li>可靠性(Reliability)：修正后的模型能够针对特定输入给出期望的输出。</li><li>泛化性(Generality)：修正后的模型能够泛化到等价的输入（例如语义相似的句子）。</li><li>局部性(Locality)：修正仅影响特定错误，不影响模型在其他无关输入上的表现。</li></ol><h2 id="method">5. Method</h2><h3 id="transformer-patcher-架构">5.1 Transformer-Patcher 架构</h3><p>Transformer-Patcher 的核心思想是在 Transformer 模型的最后一层 FFN中添加少量可训练的神经元 (patches)。这些patches仅在遇到特定错误输入时被激活，从而修正模型的输出。具体来说，对于分类任务，添加一个patch 即可修正模型；对于自回归生成任务，根据错误生成的 token数量添加相应数量的 patches。</p><h3 id="patch-的定义与训练">5.2 Patch 的定义与训练</h3><p align="center"><img src="1.png" style="zoom:50%;" /></p><ul><li><p><strong>Patch 的定义</strong>： Patch 可以被视为 FFN层中的一个额外的键值对 (key-value pair)。给定一个查询向量 <spanclass="math inline">\(q\)</span>，标准 FFN 的输出为： <spanclass="math display">\[a=\operatorname{Act}\left(q \cdot K+b_k\right)\]</span></p><p><span class="math display">\[\operatorname{FFN}(q)=a \cdot V+b_v\]</span></p><p>其中，Act 是非线性激活函数（如 ReLU 或 GeLU），<spanclass="math inline">\(a\)</span> 是激活值向量，<spanclass="math inline">\(b_k\)</span> 和 <spanclass="math inline">\(b_v\)</span> 是偏置向量。添加一个 patch后，新的输出为： <span class="math display">\[\left[\begin{array}{ll}a, a_p\end{array}\right]=\operatorname{Act}\left(q \cdot\left[\begin{array}{ll}K, \left.\left.k_p\right]+\left[b_k, b_p\right]\right)\end{array}\right.\right.\]</span> 其中，<span class="math inline">\(k_p\)</span> 是 patch的键，<span class="math inline">\(v_p\)</span> 是 patch 的值，<spanclass="math inline">\(b_p\)</span> 是 patch 的偏置，<spanclass="math inline">\(a_p\)</span> 是 patch的激活值。通过上述公式，patch 的输出可以表示为： <spanclass="math display">\[\operatorname{FFN}_p(q)=\operatorname{FFN}(q)+a_p \cdot v_p\]</span></p></li><li><p><strong>训练 Patch</strong>： 为了训练patch，作者提出了三种损失函数：</p><ol type="1"><li><p><strong>激活损失（Activation Loss）</strong>： 为了确保 patch在遇到错误输入时被激活，定义激活损失为： <span class="math display">\[l_a=\exp \left(-q_e \cdot k_p-b_p\right)\]</span> 其中，<span class="math inline">\(q_e\)</span>是错误输入的查询向量。该损失函数的目标是最大化 patch 的激活值。</p></li><li><p><strong>编辑损失（Edit Loss）</strong>：编辑损失使用任务的原始损失函数，确保修正后的模型输出与目标输出一致：<span class="math display">\[l_e=L\left(y_e, p_e\right)\]</span> 其中，<span class="math inline">\(L(\cdot)\)</span>是任务的损失函数，<span class="math inline">\(y_e\)</span>是目标输出，<span class="math inline">\(p_e\)</span>是修正后模型的输出。</p></li><li><p><strong>记忆损失（Memory Loss）</strong>：</p><p>为了确保 patch 仅对特定错误输入响应，而不影响不相关的输入，需要满足：<span class="math display">\[\forall i \in \mathbb{I}_{x_e}, \boldsymbol{q}_i \cdot\boldsymbol{k}_p+b_p \leq \beta \rightarrow \max_i\left(\boldsymbol{q}_i \cdot \boldsymbol{k}_p+b_p\right) \leq \beta\]</span> 由此定义记忆损失为： <span class="math display">\[l_m=l_{m 1}+l_{m 2}\]</span> 其中，<span class="math inline">\(l_{m 1}\)</span> 用于确保patch 对记忆中的查询向量不激活。记忆损失 <spanclass="math inline">\(l_{m 1}\)</span> 通过限制 patch对之前见过的样本（记忆）的激活值，确保 patch不会对这些无关的输入产生响应，从而保证模型在修正特定错误时不会影响其他已学习的内容。：<span class="math display">\[l_{m 1}=\mathrm{S}\left(M \cdot k_p+b_p-\beta ; k\right)\]</span> <span class="math inline">\(l_{m 2}\)</span> 用于进一步强化patch 对错误输入的专一性。它通过确保 patch对错误输入的激活值大于对记忆中查询向量的激活值，进一步拉开激活值的差距，使得patch 能够更准确地识别和响应特定的错误输入： <spanclass="math display">\[l_{m 2}=\mathrm{S}\left(\left(M-q_e\right) \cdot k_p+b_p-\gamma ;k\right)\]</span> 其中，<span class="math inline">\(M\)</span>是从之前见过的样本中随机保留的查询向量矩阵，<spanclass="math inline">\(\beta\)</span> 和 <spanclass="math inline">\(\gamma\)</span>是超参数，用于控制激活值的阈值，<span class="math inline">\(S\)</span>函数接收一个向量 <strong>v</strong> 和一个整数 <spanclass="math inline">\(k\)</span>，输出一个标量,具体定义为： <spanclass="math display">\[S(\mathbf{v} ; k)=\operatorname{Avg}[\operatorname{TopK}(\exp(\mathbf{v}) ; k)]\]</span></p></li></ol></li><li><p>最终的 patch 训练损失为： <span class="math display">\[l_p=l_e+\alpha l_a+\mu l_m\]</span> 其中，<span class="math inline">\(\alpha\)</span> 和 <spanclass="math inline">\(\mu\)</span>是超参数，用于平衡不同损失函数的权重。</p></li></ul><h2 id="conclusion">6. Conclusion</h2><p>本文提出了顺序模型编辑 (SME) 任务，并设计了 Transformer-Patcher方法来高效地修正 Transformer 模型中的错误。该方法通过在模型的最后一层FFN 中添加少量可训练的神经元(patches)，实现了对模型行为的快速修正。实验结果表明，Transformer-Patcher能够在连续修正多达数千个错误的同时，保持模型的整体性能，并且具有良好的泛化能力。该方法为实际应用中快速修正模型错误提供了一种有效的解决方案。</p><h2 id="notes">7. Notes</h2><h3 id="每次模型编辑增加一个神经元还是多个">7.1每次模型编辑增加一个神经元还是多个？</h3><blockquote><p><strong>Transformer-Patcher</strong>的设计允许在每次编辑中<strong>添加多个神经元补丁</strong>，具体数量取决于任务类型和模型错误的复杂性。</p><hr /><h3 id="分类任务-classification-tasks">1. <strong>分类任务(Classification Tasks)</strong></h3><p>对于分类任务，一般情况下<strong>每次编辑只添加一个神经元补丁</strong>：</p><ul><li>每次编辑用于修正单个错误（一个错误输入 <spanclass="math inline">\(x_e\)</span>）。</li><li>补丁由一个键值对 <span class="math inline">\((k_p, v_p)\)</span>和偏置 <span class="math inline">\(b_p\)</span> 组成，用于在目标输入<span class="math inline">\(q_e\)</span> 上激活并修正模型行为。</li></ul><p>这种方式足够解决分类任务中的单个误分类问题。</p><hr /><h3 id="生成任务-autoregressive-generation-tasks">2. <strong>生成任务(Autoregressive Generation Tasks)</strong></h3><p>在生成任务中，模型可能会在<strong>多个位置</strong>生成错误的输出，因此需要<strong>为每个错误生成一个独立的补丁</strong>：</p><ul><li>每个错误对应一个查询 <spanclass="math inline">\(q_e\)</span>，因此需要为每个错误添加一个补丁。</li><li>如果一个输入序列中有 <span class="math inline">\(n\)</span>个错误，则需要添加 <span class="math inline">\(n\)</span> 个补丁。</li></ul></blockquote><h3 id="每次模型编辑是否都要进行训练">7.2每次模型编辑是否都要进行训练？</h3><blockquote><p>根据 <strong>Transformer-Patcher</strong>的机制，每进行一次模型编辑（修正一个错误），都需要对新增的补丁进行训练。然而，这种训练只需优化补丁参数（如<span class="math inline">\(k_p\)</span>、<spanclass="math inline">\(v_p\)</span>、<spanclass="math inline">\(b_p\)</span>），而不影响原始模型参数，因此训练过程非常高效。</p><p>每次编辑的核心步骤：</p><ul><li><strong>新增补丁</strong>：为每次编辑新增一个神经元补丁，该补丁只与特定错误相关联。</li><li><strong>训练补丁</strong>：新增补丁的参数需要训练，以确保该补丁能修正目标错误，同时不影响其他输入的预测能力。</li></ul><p>是否需要重新训练整个模型？</p><p>不需要重新训练整个模型，只需训练新增补丁的参数。这种方式避免了大规模模型重训练，显著降低了计算成本。实验中显示，<strong>每次训练补丁的耗时</strong>：</p><ul><li>FEVER 数据集：每次编辑耗时约 <strong>7.1 秒</strong>。</li><li>zsRE 数据集：每次编辑耗时约 <strong>18.9 秒</strong>。</li></ul></blockquote><h3 id="编辑时训练数据如何构建">7.3 编辑时训练数据如何构建？</h3><blockquote><p>训练补丁需要以下几类数据，以满足论文提出的三个关键特性（可靠性、通用性和局部性）：</p><h4 id="编辑样本-edit-example"><strong>(1) 编辑样本 (EditExample)</strong></h4><p>用于修正错误的核心样本，表示为 <span class="math inline">\((x_e,y_e)\)</span>，其中：</p><ul><li><spanclass="math inline">\(x_e\)</span>：输入数据（模型预测错误的输入）。</li><li><spanclass="math inline">\(y_e\)</span>：目标输出（正确的标签或目标预测）。</li></ul><p><strong>作用</strong>：确保补丁能够修正该错误（满足可靠性）。训练损失： <span class="math display">\[l_e=L\left(y_e, p_e\right)\]</span> 其中，<span class="math inline">\(L\)</span>是任务的原始损失函数（如交叉熵损失）。</p><h4 id="等价样本-equivalent-examples"><strong>(2) 等价样本 (EquivalentExamples)</strong></h4><p><span class="math inline">\(x_e\)</span>的等价变体（如输入重述、同义表述），这些样本的目标输出与 <spanclass="math inline">\(y_e\)</span> 一致。</p><ul><li><strong>生成方法</strong>：可以通过数据增强方法生成等价样本，例如反向翻译(back-translation)。</li></ul><p><strong>作用</strong>：提高补丁的通用性，确保补丁不仅能修正 <spanclass="math inline">\(x_e\)</span>，也能修正等价输入。</p><h4 id="无关样本-irrelevant-examples"><strong>(3) 无关样本 (IrrelevantExamples)</strong></h4><p>这些样本与目标错误无关，目标是保证补丁不会对这些样本产生不必要的影响。</p><ul><li><strong>来源</strong>：随机从模型的训练集 <spanclass="math inline">\(D_{\text {train }}^{\prime}\)</span> 或测试集<span class="math inline">\(D_{\text {test }}\)</span> 中选取。</li><li>为了减少计算开销，论文中保留了一部分历史样本的特征表示（记忆向量）。</li></ul><p><strong>作用</strong>：避免补丁对无关样本激活（满足局部性）。训练损失： <span class="math display">\[l_{m 1}=S\left(M \cdot k_p+b_p-\beta ; k\right)\]</span></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>NLP</tag>
      
      <tag>知识编辑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Calibrating Factual Knowledge in Pretrained Language Models</title>
    <link href="/2025/01/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Calibrating-Factual-Knowledge-in-Pretrained-Language-Models/"/>
    <url>/2025/01/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Calibrating-Factual-Knowledge-in-Pretrained-Language-Models/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: Calibrating Factual Knowledge in PretrainedLanguage Models<br /><strong>Link</strong>: <ahref="https://arxiv.org/pdf/2210.03329">CaliNET Paper</a><br /><strong>Source</strong>: Empirical Methods in Natural LanguageProcessing (EMNLP)<br /><strong>Date</strong>: 2022.10.07</p><h2 id="summary">2. Summary</h2><p>本文提出了一种名为 CALINET的方法，用于在无需从头开始重新训练的情况下对预训练语言模型 (PLMs)中的事实性知识进行校准。作者首先通过对比知识评估 (Contrastive KnowledgeAssessment, CKA) 方法检测 PLMs 中的错误知识，然后引入CALINET，这是一种轻量级方法，通过添加新参数来纠正错误事实。该方法通过知识探测和问答任务进行评估，结果显示其在事实正确性和泛化能力方面有显著提升。此外，研究还通过分析模型的记忆槽提供了对校准机制的见解。</p><h2 id="background">3. Background</h2><p>预训练语言模型（如 BERT、T5 等）被广泛应用于 NLP任务，其内置的事实性知识库特性使其能支持问答、事实验证等任务。然而，模型中存储的事实知识可能包含错误，直接影响下游任务的性能。相比于从头训练或外部知识注入，本文探索了一种高效的轻量级方法校准PLMs 内部的错误知识。</p><h2 id="research-objective">4. Research Objective</h2><p>本文的主要研究目标包括：</p><ol type="1"><li>使用对比评估方法检测 PLMs 中存储的错误知识。</li><li>提出一种轻量级方法(CALINET)，通过添加新参数来纠正错误事实，而不修改原始模型。</li><li>通过知识密集型任务评估校准后的 PLMs 的有效性和泛化能力。</li></ol><h2 id="method">5. Method</h2><h3 id="对比知识评估-contrastive-knowledge-assessment-cka">5.1对比知识评估 (Contrastive Knowledge Assessment, CKA)</h3><p>为了检测 PLMs 中的错误知识，作者提出了对比知识评估 (CKA)方法，通过对比模型对正确事实和错误事实的预测来评估。给定一个事实三元组⟨s,r, o⟩，其中 s 是主体，r 是关系，o 是宾语，CKA 分数计算如下： <spanclass="math display">\[\mathrm{CKA}_M(s, r, o)=\frac{P_M(o \mid s,r)}{E_{r^{\prime}}\left[P_M\left(o \mid s,r^{\prime}\right)\right]+\alpha}\]</span> 其中，<span class="math inline">\(P_M(o \mid s, r)\)</span>是模型对正确事实的预测概率，<spanclass="math inline">\(E_{r^{\prime}}\left[P_M\left(o \mid s,r^{\prime}\right)\right]\)</span> 是错误事实（使用错误关系 <spanclass="math inline">\(r^{\prime}\)</span>）的期望概率，<spanclass="math inline">\(\alpha\)</span> 是平滑因子。如果CKA 分数低于1，则表明模型对错误事实的预测概率更高，暗示模型存在错误知识。</p><blockquote><p>注：</p><p>CKA 的核心目标是评估 PLMs中存储的事实知识，并识别其中错误的部分。在实际运行中，CKA针对的是从特定知识数据集中采样的实例，而不是直接对所有可能的事实进行评估。</p><p><strong>选择实例的依据</strong>：</p><ol type="1"><li>数据来源：从知识三元组（如<code>&lt;subject, relation, object&gt;</code>）组成的数据集（如<strong>T-REx</strong>）中选取目标实例。</li><li>样本规模：实验中通常会选取固定数量（例如 100 或 1000个三元组）进行评估，而非整个知识库。</li><li>优先级：某些重要关系或高频关系可能被优先选择，以测试模型在关键知识点上的表现。</li></ol><p><strong>负样本的构建</strong>：</p><ol type="1"><li><strong>关系替换</strong>（Relation Replacement）：<ul><li>替换三元组中的关系 <spanclass="math inline">\(r\)</span>，生成负样本提示。</li><li>示例：<ul><li>正样本提示：<code>[Obama was born in Hawaii.]</code></li><li>负样本提示：<code>[Obama was died in Hawaii.]</code> 或<code>[Obama worked in Hawaii.]</code></li></ul></li><li>替换后的关系应与原关系语义矛盾，但依然语法合理。</li></ul></li><li><strong>负样本模板设计</strong>：<ul><li>为每种关系设计专门的负样本模板，以保证负样本与正样本的对比性。</li><li>示例：对于<code>&lt;subject, subclass of, object&gt;</code>，正样本模板为<code>[X] is a subclass of [Y]</code>，负样本模板可能为<code>[X] is a parent class of [Y]</code>。</li></ul></li><li><strong>数量控制</strong>：<ul><li>每个正样本会生成多个负样本（通常为 3个），以保证负样本的多样性和稳定性。</li></ul></li><li><strong>负样本设计的特点</strong>：<ul><li>负样本的语义需与正样本矛盾，而不是随机生成。</li></ul></li></ol></blockquote><h3 id="校准机制-calinet">5.2 校准机制 (CaliNET)</h3><p><strong>关键组件</strong>: 校准的核心在于扩展原模型中的前馈网络(FFN)，添加校准记忆槽（calibration memory slots）。</p><p align="center"><img src="1.png" style="zoom:30%;" /></p><p>原始 FFN 的输出为： <span class="math display">\[FFN(H) = GELU(HK^T)V\]</span> 添加的校准项为： <span class="math display">\[\Delta F F N(H)=G E L U\left(H \tilde{K}^T\right) \tilde{V}\]</span> 校准后的输出为： <span class="math display">\[F F N^{\prime}(H)=F F N(H)+\Delta F F N(H)\]</span></p><ul><li><span class="math inline">\(\tilde{K}, \tilde{V}\)</span>是校准记忆槽的参数矩阵，维度远小于原 FFN 参数矩阵。</li></ul><p><strong>校准过程</strong>:</p><ol type="1"><li>构建包含多个表面形式的校准数据集。</li><li>冻结原模型参数，仅优化校准记忆槽的参数。</li></ol><h3 id="校准数据构建">5.3 校准数据构建</h3><ul><li><strong>基础数据</strong>： 使用 <strong>PARAREL数据集</strong>，该数据集包含 38种关系的多种自然语言模板，是构建多样化表面形式的理想基础数据集。</li><li><strong>目标事实</strong>： 从知识图谱（如 <strong>T-REx数据集</strong>）中抽取目标三元组<code>&lt;subject, relation, object&gt;</code>，每个三元组代表一个事实。</li></ul><h4 id="多样化表面形式的生成">多样化表面形式的生成**</h4><ul><li>多模板生成：<ul><li>针对每种关系 <spanclass="math inline">\(r\)</span>，选用多种模板生成自然语言表示。</li><li>示例：对于 &lt;Obama, born in, Hawaii&gt;，可以生成以下多种表述：<ol type="1"><li>"Obama was born in Hawaii."</li><li>"The birthplace of Obama is Hawaii."</li><li>"Hawaii is the place where Obama was born."</li></ol></li></ul></li></ul><h4 id="引入负样本">引入负样本**</h4><ul><li><p>负样本构建：</p><p>每个正样本生成 3个语义矛盾的负样本，通过替换三元组的关系或实体生成。</p><ul><li>示例：<ul><li>正样本：<code>"Obama was born in Hawaii."</code></li><li>负样本：<code>"Obama was died in Hawaii."</code> 或<code>"Obama worked in Hawaii."</code></li></ul></li><li>确保负样本逻辑上矛盾，但语言结构合理。</li></ul></li></ul><h2 id="evaluation">6. Evaluation</h2><h3 id="错误知识检测">6.1 错误知识检测</h3><p>作者从 T-REx 数据集中采样了 100 个和 1000 个事实三元组，并使用 CKA检测 T5-base 和 T5-large 模型中的错误知识。错误率(False Rate)定义为 CKA分数低于 1 的事实比例，结果表明 T5-base 的错误率约为 50%，T5-large略低，但仍处于较高水平，这表明 PLMs 中存在大量错误知识。</p><h3 id="知识校准">6.2 知识校准</h3><p>校准后的模型从知识建模能力（原始测试集和对抗测试集的困惑度）和语言建模能力（掩码测试数据的困惑度）两个方面进行评估。结果显示，CALINET显著降低了错误率，并提高了模型生成事实正确预测的能力，同时不影响其一般语言建模能力。</p><h3 id="可扩展性和泛化能力">6.3 可扩展性和泛化能力</h3><p>研究还探索了 CALINET 的可扩展性，发现其能够同时校准超过 5000 个事实的60%以上。此外，校准后的知识在开放域问答任务中表现良好，提高了模型对原本回答错误的问题的性能。</p><h2 id="conclusion">7. Conclusion</h2><p>本文展示了 CALINET作为一种轻量级且有效的方法，能够在无需重新训练的情况下对 PLMs中的事实性知识进行校准。CALINET成功减少了错误知识，并提高了下游任务中的事实正确性。此外，通过分析模型的记忆槽，研究还揭示了校准机制，表明CALINET 能够以泛化的方式调整模型的预测。</p><h2 id="notes">8. Notes</h2><h3 id="举例说明-calinet-的运行过程">8.1 举例说明 CaliNET的运行过程</h3><blockquote><h3 id="新知识错误的发现"><strong>1. 新知识错误的发现</strong></h3><p>假设你发现模型对于以下事实知识存在错误：</p><ul><li><strong>错误知识</strong>：<code>Obama was born in Beijing.</code></li><li><strong>正确知识</strong>：<code>Obama was born in Hawaii.</code></li></ul><hr /><h3 id="数据集的重新构建"><strong>2. 数据集的重新构建</strong></h3><p>为了校准这个新知识错误，需要为新错误构建校准数据集。</p><h4 id="步骤"><strong>步骤</strong>：</h4><ol type="1"><li><p><strong>正样本生成</strong>：</p><ul><li><p>使用多种自然语言模板生成正确事实的表述：</p><ul><li>示例：<ul><li><code>"Obama was born in Hawaii."</code></li><li><code>"The birthplace of Obama is Hawaii."</code></li><li><code>"Hawaii is the place where Obama was born."</code></li></ul></li></ul></li><li><p>将这些模板转换为模型的输入：</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs prolog">输入：<span class="hljs-symbol">Obama</span> was born in [<span class="hljs-symbol">MASK</span>].<br>目标输出：<span class="hljs-symbol">Hawaii</span><br></code></pre></td></tr></table></figure></li></ul></li><li><p><strong>负样本生成</strong>：</p><ul><li><p>替换关系或宾语，生成与正确事实语义矛盾的负样本：</p><ul><li>示例：<ul><li><code>"Obama was born in Beijing."</code></li><li><code>"Obama was born in Shanghai."</code></li><li><code>"Obama got married in Hawaii."</code></li></ul></li></ul></li><li><p>将这些负样本添加到校准数据集中：</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs prolog">输入：<span class="hljs-symbol">Obama</span> was born in [<span class="hljs-symbol">MASK</span>].<br>目标输出：<span class="hljs-symbol">Beijing</span><br></code></pre></td></tr></table></figure></li></ul></li><li><p><strong>数据集划分</strong>：</p><ul><li>确保新生成的数据集与原来的校准数据集不冲突。</li><li>划分训练集、验证集和测试集，确保模板无重叠。</li></ul></li></ol><h4 id="示例"><strong>示例</strong>：</h4><table><thead><tr class="header"><th><strong>类型</strong></th><th><strong>输入句子</strong></th><th><strong>目标输出</strong></th></tr></thead><tbody><tr class="odd"><td>正样本</td><td><code>Obama was born in [MASK].</code></td><td>Hawaii</td></tr><tr class="even"><td>负样本</td><td><code>Obama was born in [MASK].</code></td><td>Beijing</td></tr></tbody></table><hr /><h3 id="更新-calinet-的校准参数"><strong>3. 更新 CaliNET的校准参数</strong></h3><p>重新训练 <strong>CaliNET</strong> 对新错误进行校准。</p><h4 id="训练方法"><strong>训练方法</strong>：</h4><ol type="1"><li>冻结 PLM 的原始参数，只训练 CaliNET 的校准槽（Calibration MemorySlots）。</li><li>使用重新构建的数据集对新错误知识进行训练。</li><li>更新训练好的校准槽参数。</li></ol><hr /><h3 id="总结"><strong>4. 总结</strong></h3><p>当发现新的知识错误时：</p><ol type="1"><li><strong>生成校准数据集</strong>：针对新知识错误构建正负样本。</li><li><strong>重新训练校准槽</strong>：冻结原始模型参数，仅优化 CaliNET的校准槽。</li><li><strong>加载更新的校准槽</strong>：将新的校准槽参数加载到模型中即可实现对新知识错误的校准。</li></ol></blockquote>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>NLP</tag>
      
      <tag>知识编辑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Editing Factual Knowledge in Language Models</title>
    <link href="/2025/01/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Editing-Factual-Knowledge-in-Language-Models/"/>
    <url>/2025/01/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Editing-Factual-Knowledge-in-Language-Models/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: Editing Factual Knowledge in LanguageModels<br /><strong>Link</strong>: <a href="https://arxiv.org/pdf/2104.08164">KEPaper</a><br /><strong>Source</strong>: Empirical Methods in Natural LanguageProcessing (EMNLP)<br /><strong>Date</strong>: 2021.04.16</p><h2 id="summary">2. Summary</h2><p>本文提出了一种名为 KNOWLEDGEEDITOR 的方法，用于在语言模型 (LMs)中高效、可靠地编辑事实知识，无需重新训练或微调模型。该方法利用超网络(hyper-network)预测模型参数更新，确保修改特定事实时不会影响其他知识。此外，通过训练期间引入等价语句（如改写句）的方式，增强了修改一致性。作者在BERT 和 BART 模型上进行了实验，展示了 KNOWLEDGEEDITOR在事实核查和开放域问答任务上的高效性。</p><h2 id="background">3. Background</h2><p>预训练语言模型（如 BERT 和 GPT）在许多 NLP任务中表现优异，得益于其内部隐式存储的事实知识。然而，这些模型可能存储错误或过时的信息，难以直接修改。传统的解决方法（如重新训练或基于元学习的微调）代价高昂且不通用。因此，开发一种高效、可靠的方法来编辑这些模型的知识具有重要意义。</p><h2 id="research-objective">4. Research Objective</h2><ol type="1"><li>开发一种无需对原始模型进行特殊训练的知识编辑方法。</li><li>确保修改特定事实时保持其他知识的完整性。</li><li>保证对等价输入（如改写句）的预测结果一致。</li></ol><h2 id="method">5. Method</h2><h3 id="knowledgeeditor-方法概述">5.1 KNOWLEDGEEDITOR 方法概述</h3><p>KNOWLEDGEEDITOR方法的核心思想是将编辑模型知识的任务视为一个学习问题。具体来说，该方法使用一个超网络（hyper-network）来预测模型参数的更新，从而实现对特定事实的修改。超网络的训练目标是在修改特定事实的同时，保持其他预测不变。</p><h3 id="优化目标">5.2 优化目标</h3><p>给定一个输入 <span class="math inline">\(x\)</span> 和模型 <spanclass="math inline">\(f(x;θ)\)</span>，目标是找到新的参数 <spanclass="math inline">\(θ^′\)</span>，使得模型对 <spanclass="math inline">\(x\)</span> 的预测从 <spanclass="math inline">\(y\)</span> 变为 <spanclass="math inline">\(a\)</span>，同时对其他输入 <spanclass="math inline">\(x^{\prime} \neq x\)</span>的预测保持不变。这一目标可以通过以下优化问题来形式化： <spanclass="math display">\[\min _\phi \sum_{\hat{x} \in P_z} L\left(\theta^{\prime} ; \hat{x},a\right) \quad \text { s.t. } \quad C\left(\theta, \theta^{\prime}, f ;O_x\right) \leq m\]</span> 其中：</p><ul><li><span class="math inline">\(P_x\)</span> 是 <spanclass="math inline">\(x\)</span>的语义等价输入集合（如自动生成的释义）。</li><li><span class="math inline">\(O_x\)</span> 是除 <spanclass="math inline">\(x\)</span> 之外的其他输入集合。</li><li><span class="math inline">\(C\)</span>是约束条件，用于确保修改后的模型对其他输入的预测保持不变。</li><li><span class="math inline">\(m\)</span>是一个超参数，表示约束的边界。</li></ul><h3 id="约束条件的选择">5.3 约束条件的选择</h3><p>约束条件 <span class="math inline">\(C\)</span>的选择至关重要。本文提出了两种约束：</p><ol type="1"><li><strong>KL 散度约束</strong>：通过计算原始模型和修改后模型的 KL散度来约束输出分布的差异。</li></ol><p><span class="math display">\[C_{\mathrm{KL}}\left(\theta, \theta^{\prime}, f ;O_x\right)=\sum_{x^{\prime} \in O_x} \sum_{c \in Y} p_{Y \mid X}\left(c\mid x^{\prime}, \theta\right) \log \frac{p_{Y \mid X}\left(c \midx^{\prime}, \theta\right)}{p_{Y \mid X}\left(c \mid x^{\prime},\theta^{\prime}\right)}\]</span></p><ol start="2" type="1"><li><strong>Lp 范数约束</strong>：通过限制参数更新的 Lp范数来约束模型的变化。</li></ol><h3 id="超网络的设计">5.4 超网络的设计</h3><p>超网络的目标是预测参数更新 <span class="math inline">\(\Delta\theta\)</span>，使得 <spanclass="math inline">\(\theta^{\prime}=\theta+\Delta \theta\)</span>高效率，超网络利用梯度信息 <span class="math inline">\(\nabla_\thetaL(\theta ; x, a)\)</span> 来指导更新。具体来说，超网络的输出可以表示为：<span class="math display">\[\Delta W=\sigma(\eta) \cdot\left(\alpha \cdot \nabla_W L(W ; x,a)+\beta\right)\]</span> 其中：</p><ul><li><span class="math inline">\(\sigma(\eta)\)</span>是一个缩放因子，用于控制整体更新的强度。</li><li><span class="math inline">\(\alpha \cdot \nabla_W L(W ; x,a)\)</span> 表示对梯度信息的加权更新方向。</li><li><span class="math inline">\(\beta\)</span>是一个偏置项，用于对更新提供额外的修正。</li></ul><p>为了计算这些参数 <span class="math inline">\((\eta, \alpha,\beta)\)</span>，超网络需要对输入进行编码，并生成与目标权重矩阵 <spanclass="math inline">\(W\)</span> 维度相匹配的参数。</p><h4 id="计算过程">计算过程</h4><ol type="1"><li>输入的编码</li></ol><p>超网络的输入由以下部分组成：</p><ul><li>输入数据 <spanclass="math inline">\(x\)</span>，即需要修改预测的输入样本。</li><li>当前模型对输入的预测 <span class="math inline">\(y\)</span>。</li><li>目标预测值 <span class="math inline">\(a\)</span>，即希望模型对<span class="math inline">\(x\)</span> 的更新预测。</li></ul><p>超网络首先将 <span class="math inline">\((x, y, a)\)</span>连接成一个输入序列，并通过一个 <strong>BiLSTM</strong>网络进行编码。具体步骤为：</p><ul><li><p>将输入序列表示为嵌入向量（可能使用词向量或特征向量）。</p></li><li><p>通过双向 LSTM（BiLSTM）处理，得到最后一个隐藏状态（hiddenstate）。</p></li><li><p>将隐藏状态输入到一个前馈神经网络 (FFNN)，得到一个条件向量 <spanclass="math inline">\(h\)</span>。</p></li></ul><ol start="2" type="1"><li>参数生成</li></ol><p>基于编码向量 <spanclass="math inline">\(h\)</span>，超网络使用一系列前馈神经网络 (FFNN)来生成更新参数：</p><ul><li><strong>生成 <span class="math inline">\(\alpha\)</span>:</strong>使用一个 FFNN，输入为 <spanclass="math inline">\(h\)</span>，输出一个向量 <spanclass="math inline">\(\alpha \in\mathbb{R}^m\)</span>，表示对梯度的加权系数。</li><li><strong>生成 <span class="math inline">\(\beta\)</span>:</strong>使用另一个 FFNN，输入为 <spanclass="math inline">\(h\)</span>，输出一个向量 <spanclass="math inline">\(\beta \in\mathbb{R}^m\)</span>，表示更新的偏置项。</li><li><strong>生成 <span class="math inline">\(\eta\)</span>:</strong>使用一个单独的 FFNN，输入为 <spanclass="math inline">\(h\)</span>，输出一个标量 <spanclass="math inline">\(\eta\)</span>，控制整体更新的幅度。这里 <spanclass="math inline">\(\eta\)</span> 的范围通过 Sigmoid 激活函数限制在[0,1]。</li></ul><p>此外，为了进一步降低参数维度，作者对更新矩阵的生成进行了分解：</p><ol type="1"><li><span class="math inline">\(\alpha\)</span> 和 <spanclass="math inline">\(\beta\)</span> 的输出通过 Softmax 函数归一化，得到<span class="math inline">\(\alpha^\prime\)</span> 和 <spanclass="math inline">\(\beta^\prime\)</span>。</li><li>最终更新矩阵 <span class="math inline">\(\Delta W\)</span>是上述参数的组合，具体公式为：</li></ol><p><span class="math display">\[\Delta W=\sigma(\eta) \cdot\left(\operatorname{Softmax}(\alpha) \cdot\nabla_W+\operatorname{Softmax}(\beta)\right)\]</span></p><h3 id="边界退火策略">5.5 边界退火策略</h3><p>为了动态调整约束边界<em>m</em>，本文采用了边界退火策略。在训练过程中，如果模型在验证集上的表现超过一定阈值（如90%），则将边界 <em>m</em> 乘以一个衰减因子（如0.8），直到达到一个较小的值。这种策略可以在保证模型收敛的同时，逐步收紧约束。</p><h2 id="evaluation">6. Evaluation</h2><p>作者在以下任务上验证了 KNOWLEDGEEDITOR 的性能：</p><ol type="1"><li><strong>事实核查 (Fact-Checking):</strong> 使用 BERT 模型在 FEVER数据集上评估二分类任务。</li><li><strong>开放域问答 (Question Answering):</strong> 使用 BART 模型在zsRE 数据集上评估生成任务。</li></ol><h4 id="评估指标">评估指标：</h4><ul><li><strong>成功率 (Success Rate):</strong> 修改目标事实的准确率。</li><li><strong>保持准确率 (Retain Accuracy):</strong>未修改输入的预测结果保持不变的准确率。</li><li><strong>等价输入一致性 (Equivalence Accuracy):</strong>对等价输入的预测一致性。</li><li><strong>性能退化 (Performance Deterioration):</strong>更新后模型在原任务上的整体性能下降程度。</li></ul><h4 id="实验结果">实验结果：</h4><ul><li>KNOWLEDGEEDITOR在所有指标上均表现优异，尤其是在保持准确率和等价输入一致性上，相较于传统微调方法有显著提升。</li></ul><h2 id="conclusion">7. Conclusion</h2><p>KNOWLEDGEEDITOR提供了一种高效、通用的方法来编辑语言模型的隐式知识，同时最大限度地减少了对其他知识的影响。这项研究为NLP 模型的动态知识更新提供了新思路。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>NLP</tag>
      
      <tag>知识编辑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Can We Edit Multimodal Large Language Models</title>
    <link href="/2025/01/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Can-We-Edit-Multimodal-Large-Language-Models/"/>
    <url>/2025/01/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Can-We-Edit-Multimodal-Large-Language-Models/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: Can We Edit Multimodal Large LanguageModels?<br /><strong>Link</strong>: <a href="https://arxiv.org/pdf/2310.08475">MMEditPaper</a><br /><strong>Source</strong>: Empirical Methods in Natural LanguageProcessing (EMNLP)<br /><strong>Date</strong>: 2023.10.12</p><h2 id="summary">2. Summary</h2><p>论文提出了一个新的基准<strong>MMEdit</strong>，用于编辑多模态大语言模型(MLLMs)。研究为多模态模型编辑设计了一系列创新的评估指标（可靠性、局部性和泛化性）以及对应的数据集。实验结果表明，当前的方法在多模态编辑任务上效果有限，尤其是视觉模块的编辑难度较大，为后续研究提供了潜在方向。</p><h2 id="background">3. Background</h2><p>随着大语言模型（LLMs）的广泛应用，保持其知识的准确性和时效性变得至关重要。然而，重新训练模型成本高昂且难以实施，而微调可能导致过拟合和灾难性遗忘。因此，模型编辑技术应运而生，旨在高效、准确地修改模型中存储的事实知识。然而，以往的研究主要集中在单模态模型编辑，而多模态LLMs的编辑更具挑战性，因为其错误输出可能源于多种模态的协同作用。此外，目前缺乏针对多模态LLMs编辑的数据集和基准测试框架。</p><h2 id="research-objective">4. Research Objective</h2><p>本文的主要目标是探索多模态LLMs的编辑方法，并提供一个基准测试框架MMEdit，以促进该领域的研究。具体目标包括：</p><ol type="1"><li>构建一个多模态模型编辑的基准测试框架，涵盖视觉问答（VQA）和图像描述生成（ImageCaptioning）两个子任务。</li><li>提出一套创新的评估指标，包括可靠性（Reliability）、局部性（Locality）和泛化性（Generality），用于评估多模态模型编辑方法的效果。</li><li>通过实验验证现有编辑方法在多模态LLMs上的效果，并分析其优缺点。</li></ol><h2 id="method">5. Method</h2><h3 id="主要步骤">5.1 主要步骤</h3><ol type="1"><li><strong>指标设计</strong>：扩展单模态模型的可靠性、局部性和泛化性评估方法到多模态场景。</li><li><strong>数据集构建</strong>：基于 VQAv2 和 COCO Captions数据集，选择表现不佳的样本作为编辑目标。</li><li><strong>基线方法</strong>：使用包括 MEND、SERAC、Knowledge Editor以及 Fine-tuning 在内的多种方法。</li><li><strong>评估方法</strong>：结合文本与视觉的稳定性和泛化性指标，全面评估编辑方法的效果。</li></ol><h3 id="关键指标">5.2 关键指标</h3><ol type="1"><li><p><strong>可靠性</strong>：衡量编辑后模型对目标修改的准确性： <spanclass="math display">\[\mathcal{M}_{r e l}=\mathbb{E}_{\left(i_e, x_e, y_e\right) \sim\mathcal{D}_{\text {edit }}}\left[\mathbb{1}_{\left[f\left(i_e, x_e ;\theta_e\left(i_e, x_e, y_e\right)\right)=y_e\right]}\right]\]</span></p></li><li><p><strong>局部性</strong>：保证模型的无关知识在编辑后仍然保持稳定：</p><ul><li>文本局部性：</li></ul><p><span class="math display">\[\mathcal{M}_{l o c}^{\text {Text }}=\mathbb{E}_{\left(i_e, x_e,y_e\right) \sim \mathcal{D}_{\text {edit}}}\left[\mathbb{1}_{\left[f\left(x ; \theta_e\left(i_e, x_e,y_e\right)\right)=f(x, \theta)\right]}\right], (x, y) \sim\mathcal{D}_{\text {loc-t }}\]</span></p><ul><li>视觉局部性：</li></ul><p><span class="math display">\[\mathcal{M}_{l o c}^{\text {Img }}=\mathbb{E}_{\left(i_v, x_v,y_v\right) \sim\mathcal{D}_{\mathrm{loc}-\mathrm{v}}}\left[\mathbb{1}_{f\left(i_v, x_v; \theta_e\right)=f\left(i_v, x_v ; \theta\right)}\right]\]</span></p></li><li><p><strong>泛化性</strong>：保证模型能对重述或等效输入生成一致输出：</p><ul><li>文本泛化性：</li></ul><p><span class="math display">\[\mathcal{M}_{g e n}^{\text {Text }}=\mathbb{E}_{\left(x_r\right) \sim\mathcal{N}\left(x_e\right)}\left[\mathbb{1}_{f\left(i_e, x_r ;\theta_e\right)=f\left(i_e, x_e ; \theta_e\right)}\right]\]</span></p><ul><li>视觉泛化性：</li></ul><p><span class="math display">\[\mathcal{M}_{g e n}^{\text {Img }}=\mathbb{E}_{\left(i_r\right) \sim\mathcal{N}\left(i_e\right)}\left[\mathbb{1}_{f\left(i_r, x_e ;\theta_e\right)=f\left(i_e, x_e ; \theta_e\right)}\right]\]</span></p></li></ol><h2 id="evaluation">6. Evaluation</h2><p>模型在 <strong>可靠性</strong>、<strong>局部性</strong> 和<strong>泛化性</strong> 上进行了全面评估，主要结果如下：</p><ul><li>所有模型编辑方法在可靠性方面优于基线方法，但视觉模块的局部性和泛化性表现较差。</li><li>微调方法需要大量资源，且容易导致灾难性遗忘。</li><li>编辑视觉模块比编辑语言模块更具挑战性，这与多模态模型的复杂性和架构有关。</li></ul><h2 id="conclusion">7. Conclusion</h2><p>研究发现，现有的编辑方法在多模态LLMs上存在局限性，尤其是在编辑视觉模块时效果不佳。未来的工作需要探索更高效的编辑方法，以同时考虑不同模态的信息。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>知识编辑</tag>
      
      <tag>多模态</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Can We Edit Factual Knowledge by In-Context Learning</title>
    <link href="/2025/01/19/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Can-We-Edit-Factual-Knowledge-by-In-Context-Learning/"/>
    <url>/2025/01/19/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Can-We-Edit-Factual-Knowledge-by-In-Context-Learning/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: Can We Edit Factual Knowledge by In-ContextLearning?<br /><strong>Link</strong>: <a href="https://arxiv.org/pdf/2305.12740">IKEPaper</a><br /><strong>Source</strong>: Empirical Methods in Natural LanguageProcessing (EMNLP)<br /><strong>Date</strong>: 2023.05.22</p><h2 id="summary">2. Summary</h2><p>本文探讨了利用上下文学习 （In-Context Learning, ICL）来编辑大型语言模型（LLMs）中的事实知识的可能性。提出了一种名为IKE（In-Context Knowledge Editing）的方法，该方法无需更新模型参数，通过构建示范上下文来引导 LLMs进行知识编辑。实验表明，IKE在有效性（efficacy）、泛化性（generalization）和特异性（specificity）上达到了与基于梯度的方法相当甚至更优的性能，同时显著降低了计算开销和副作用。</p><h2 id="background">3. Background</h2><p>大规模语言模型在 NLP任务中表现出色，但其存储的事实知识可能过时或错误。传统的知识编辑方法需要通过梯度更新模型参数，存在计算成本高、副作用大的问题。上下文学习是一种无需参数更新的新方法，本研究探索其在知识编辑中的潜力。</p><h2 id="research-objective">4. Research Objective</h2><p>本研究的主要目标是探索 ICL 在 LLMs知识编辑中的潜力，具体包括两个方面：确保 LLMs能够对更新的知识进行泛化，即对同一知识的不同文本描述进行预测更新；确保编辑的特异性，即在修改目标知识事实时，不影响其他不相关的知识。</p><h2 id="method">5. Method</h2><h3 id="in-context-learning-icl">5.1 In-Context Learning (ICL)</h3><p>ICL 由 Brown 等人于2020年提出，旨在基于 <spanclass="math inline">\(k\)</span> 个示范 <spanclass="math inline">\(C=\left\{\left(x_1, y_1\right), \ldots,\left(x_k,y_k\right)\right\}\)</span>，在不更新任何参数的情况下，预测输入 <spanclass="math inline">\(x\)</span> 的输出 <spanclass="math inline">\(y\)</span>。</p><h3 id="in-context-knowledge-editing-ike">5.2 In-Context KnowledgeEditing (IKE)</h3><p>当向 LLMs 注入目标事实 <span class="math inline">\(f=\left(x^*,y^*\right)\)</span> 时，构建 <span class="math inline">\(k\)</span>个示范 <span class="math inline">\(C = \left\{c_1, \ldots,c_k\right\}\)</span>。知识编辑的目标是在目标提示 <spanclass="math inline">\(x^*\)</span> 的编辑范围内（即 <spanclass="math inline">\(x \in \mathcal{D}_{x^*}\)</span>），最大化 <spanclass="math inline">\(\mathcal{P}\left(y^* \mid x, f, C\right)\)</span>（泛化目标），并在<span class="math inline">\(x \notin\mathcal{D}_{x^*}\)</span>时，最小化 <spanclass="math inline">\(\mathcal{P}(y \mid x, f, C)\)</span> 和 <spanclass="math inline">\(\mathcal{P}(y \mid x)\)</span>之间的距离（特异性目标）。为了实现这些目标，示范的构建至关重要，研究者将其分解为两个子问题：如何设计每个示范的格式，以及如何选择和排序上下文示范。</p><h4 id="demonstration-formatting">5.2.1 Demonstration Formatting</h4><p>每个示范 <span class="math inline">\(c_i\)</span> 包含一个新事实<span class="math inline">\(f_i=\left(x_i^*,y_i^*\right)\)</span>，一个探测提示 <spanclass="math inline">\(x_i\)</span> 及其预测 <spanclass="math inline">\(y_i\)</span>。示范应教导 LLMs复制、更新和保留不同提示的预测：</p><ul><li><strong>copy</strong>：注入新事实的第一步是教导 LLMs复制新事实中的目标提示的预测。在 copy 示范中，<spanclass="math inline">\(x_i=x_i^*\)</span>，<spanclass="math inline">\(y_i=y_i^*\)</span>。</li><li><strong>update</strong>：知识编辑不仅仅是让 LLMs重复新事实。为了知识编辑的泛化，编辑范围内的提示的预测也应该更新。在update 示范中，<span class="math inline">\(x_i \in\mathcal{D}_{x_i^*}\)</span>，<spanclass="math inline">\(y_i=y_i^*\)</span>。</li><li><strong>retain</strong>：为了知识编辑的特异性，LLMs应在范围外的提示中保持其原始预测。在 retain 示范中，<spanclass="math inline">\(x_i \notin \mathcal{D}_{x_i^*}\)</span>，<spanclass="math inline">\(y_i\)</span> 应为其原始答案<spanclass="math inline">\(y^o_i\)</span>。</li></ul><p>IKE 的模板 <span class="math inline">\(T\)</span> 将 <spanclass="math inline">\(f\)</span>, <span class="math inline">\(x\)</span>和 <span class="math inline">\(y\)</span> 转换为自然语言：<spanclass="math inline">\(\mathcal{T}(f, x, y)\)</span> = New Fact: f,Prompt: x y。</p><h4 id="demonstration-organization">5.2.2 DemonstrationOrganization</h4><p>在编辑 LLMs 中的知识事实 <span class="math inline">\(f\)</span>时，从训练语料库中构建 <span class="math inline">\(k\)</span> 个示范<span class="math inline">\(C=\left\{c_1, \ldots,c_k\right\}\)</span>。使用无监督检索器根据输入提示 <spanclass="math inline">\(x^*\)</span> 与其原始答案 <spanclass="math inline">\(y^o\)</span> 和目标预测 <spanclass="math inline">\(y^*\)</span> 之间的余弦相似度选择 <spanclass="math inline">\(k\)</span>个最近邻。具体来说，使用预训练的句子编码器 <spanclass="math inline">\(E\)</span> 对新事实 <spanclass="math inline">\(f\)</span> 的提示 <spanclass="math inline">\(x^*\)</span>进行编码，以及训练语料库中的记录也将以相同方式进行编码，并基于余弦相似度检索k-NN 事实。上下文示范的排序也取决于余弦相似度：<spanclass="math inline">\(\cos \left(c_0, f\right)&lt;\cos \left(c_1,f\right)&lt;\ldots&lt;\cos \left(c_k, f\right)\)</span>，其中<spanclass="math inline">\(c_1, \ldots, c_k\)</span>从左到右放置在上下文中。</p><h2 id="conclusion">6. Conclusion</h2><p>IKE在无需参数修改的情况下实现了高效知识编辑，具备较好的泛化性和特异性。</p><p>方法适用于大规模模型，计算效率高，且副作用少。</p><h2 id="notes">7. Notes</h2><h3 id="详细介绍-copy-update-和-retain-示例的选取">7.1 详细介绍 copy,update 和 retain 示例的选取。</h3><blockquote><p>copy, update 和 retain 三种示例选择的比例为 1:3:4。</p><p>Copy 示例用于直接注入新知识，因为目标提示是唯一的，示例需求较少。</p><p>Update 示例用于增强泛化性，由于涉及多种变体问题，Update示例需求相对较多。</p><p>Retain示例用于确保无关知识保持不变，由于训练语料中无关提示的数量较多，需要更多示例来指导模型正确保留这些知识。</p><h4 id="copy-示例"><strong>Copy 示例</strong></h4><ul><li><p><strong>目的</strong>: 教模型直接记住目标知识，即在提示 <spanclass="math inline">\(x^*\)</span> 下预测 <spanclass="math inline">\(y^*\)</span>。</p></li><li><p>构造方式:</p><ul><li><span class="math inline">\(x_i =x^*\)</span>，即目标提示本身。</li><li><span class="math inline">\(y_i = y^*\)</span>，即目标答案。</li></ul><p>例如：</p><p>New Fact: The president of the US is Joe Biden.<br />Prompt: The president of the US is? Joe Biden.</p></li></ul><h4 id="update-示例"><strong>Update 示例</strong></h4><ul><li><p><strong>目的</strong>:增强模型的泛化能力，使模型能正确回答目标知识相关的变体问题。</p></li><li><p>构造方式:</p><ul><li>从训练语料中选择与目标提示 <span class="math inline">\(x^*\)</span>相关的提示 <span class="math inline">\(x_i \in D_{x^*}\)</span>。</li><li>将答案统一为目标答案 <span class="math inline">\(y^*\)</span>。</li></ul><p>例如：</p><p>New Fact: The president of the US is Joe Biden.<br />Prompt: Who is the president of the US? Joe Biden.<br />New Fact: The president of the US is Joe Biden.<br />Prompt: The current president of the US is? Joe Biden.</p></li></ul><h4 id="retain-示例"><strong>Retain 示例</strong></h4><ul><li><p><strong>目的</strong>:确保无关知识不会受到影响，保持模型原有预测能力。</p></li><li><p>构造方式:</p><ul><li>从训练语料中选择与目标提示无关的提示 <span class="math inline">\(x_i\notin D_{x^*}\)</span>。</li><li>保留其原始答案 <span class="math inline">\(y_i =y^o_i\)</span>。</li></ul><p>例如：</p><p>New Fact: The president of the US is Joe Biden.<br />Prompt: Who is the president of Russia? Putin.<br />New Fact: The president of the US is Joe Biden.<br />Prompt: Who created Apple Inc.? Steve Jobs.</p></li></ul></blockquote><h3 id="使用-ike-方法编辑模型示例">7.2 使用 IKE 方法编辑模型示例。</h3><blockquote><p>假设希望编辑语言模型中的一个事实：将“美国总统是奥巴马”更新为“美国总统是拜登”。</p><ul><li><strong>旧事实</strong>：The president of the US is Obama.</li><li><strong>新事实</strong>：The president of the US is Joe Biden.</li></ul><ol type="1"><li><p><strong>示范格式化（Demonstration Formatting）</strong>根据论文，需要设计三种类型的示范：<code>copy</code>、<code>update</code>和 <code>retain</code>。</p><ul><li><p><strong>Copy示范</strong>：直接注入新事实。</p><figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs vbnet"><span class="hljs-built_in">New</span> Fact: The president <span class="hljs-keyword">of</span> the US <span class="hljs-built_in">is</span> Joe Biden.<br><span class="hljs-symbol">Prompt:</span> The president <span class="hljs-keyword">of</span> the US <span class="hljs-built_in">is</span>?<br><span class="hljs-symbol">Answer:</span> Joe Biden.<br></code></pre></td></tr></table></figure></li><li><p><strong>Update示范</strong>：对与目标事实相关的提示进行更新。</p><figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs vbnet"><span class="hljs-built_in">New</span> Fact: The president <span class="hljs-keyword">of</span> the US <span class="hljs-built_in">is</span> Joe Biden.<br><span class="hljs-symbol">Prompt:</span> Who <span class="hljs-built_in">is</span> the current president <span class="hljs-keyword">of</span> the US?<br><span class="hljs-symbol">Answer:</span> Joe Biden.<br></code></pre></td></tr></table></figure></li><li><p><strong>Retain示范</strong>：保留与目标事实无关的原始预测。</p><figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs vbnet"><span class="hljs-built_in">New</span> Fact: The president <span class="hljs-keyword">of</span> the US <span class="hljs-built_in">is</span> Joe Biden.<br><span class="hljs-symbol">Prompt:</span> Who <span class="hljs-built_in">is</span> the president <span class="hljs-keyword">of</span> Russia?<br><span class="hljs-symbol">Answer:</span> Putin.<br></code></pre></td></tr></table></figure></li></ul></li><li><p><strong>示范组织（Demonstration Organization）</strong>从训练语料库中检索与目标事实最相关的示范（选取比例为1:3:4），并将它们按相关性排序。假设我们选择了以下示范：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-built_in">Context</span> C:<br>- New Fact: The president of the US is <span class="hljs-keyword">Joe </span><span class="hljs-keyword">Biden. </span>Prompt: The president of the US is? Answer: <span class="hljs-keyword">Joe </span><span class="hljs-keyword">Biden.</span><br><span class="hljs-keyword"></span>- New Fact: The president of the US is <span class="hljs-keyword">Joe </span><span class="hljs-keyword">Biden. </span>Prompt: Who is the current president of the US? Answer: <span class="hljs-keyword">Joe </span><span class="hljs-keyword">Biden.</span><br><span class="hljs-keyword"></span>- New Fact: The president of the US is <span class="hljs-keyword">Joe </span><span class="hljs-keyword">Biden. </span>Prompt: Who was the previous president of the US? Answer: Obama.<br>- New Fact: The president of the US is <span class="hljs-keyword">Joe </span><span class="hljs-keyword">Biden. </span>Prompt: Who is the president of Russia? Answer: Putin.<br></code></pre></td></tr></table></figure></li><li><p><strong>模型输入与输出</strong>将上述示范作为上下文输入到语言模型中，模型需要根据这些示范来调整其预测。</p><ul><li><p><strong>模型输入</strong>：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-built_in">Context</span> C:<br>New Fact: The president of the US is <span class="hljs-keyword">Joe </span><span class="hljs-keyword">Biden. </span>Prompt: The president of the US is? Answer: <span class="hljs-keyword">Joe </span><span class="hljs-keyword">Biden.</span><br><span class="hljs-keyword"></span>New Fact: The president of the US is <span class="hljs-keyword">Joe </span><span class="hljs-keyword">Biden. </span>Prompt: Who is the current president of the US? Answer: <span class="hljs-keyword">Joe </span><span class="hljs-keyword">Biden.</span><br><span class="hljs-keyword"></span>New Fact: The president of the US is <span class="hljs-keyword">Joe </span><span class="hljs-keyword">Biden. </span>Prompt: Who was the previous president of the US? Answer: Obama.<br>New Fact: The president of the US is <span class="hljs-keyword">Joe </span><span class="hljs-keyword">Biden. </span>Prompt: Who is the president of Russia? Answer: Putin.<br><span class="hljs-symbol"></span><br><span class="hljs-symbol">Query:</span> Who is the president of the US?<br></code></pre></td></tr></table></figure></li><li><p><strong>期望输出</strong>：</p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs avrasm"><span class="hljs-symbol">Answer:</span> Joe Biden.<br></code></pre></td></tr></table></figure></li></ul></li></ol></blockquote>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>NLP</tag>
      
      <tag>知识编辑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Memory-Based Model Editing at Scale</title>
    <link href="/2025/01/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Memory-Based-Model-Editing-at-Scale/"/>
    <url>/2025/01/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Memory-Based-Model-Editing-at-Scale/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: Memory-Based Model Editing at Scale<br /><strong>Link</strong>: <a href="https://arxiv.org/pdf/2206.06520">SERACPaper</a><br /><strong>Source</strong>: International Conference on Machine Learning(ICML)<br /><strong>Date</strong>: 2022.06.13</p><h2 id="summary">2. Summary</h2><p>本文提出了一个新的模型编辑方法 —— SERAC（Semi-Parametric Editing witha Retrieval-Augmented CounterfactualModel）。与传统基于梯度的方法不同，SERAC引入了显式编辑记忆，用于存储用户提供的编辑描述，并通过范围分类器和反事实模型来调整预测行为。实验表明，SERAC在包括问答、事实验证和对话生成的多个任务中显著优于现有方法。</p><h2 id="background">3. Background</h2><p>传统的神经网络通常被视为静态系统，难以在部署后快速修改其行为。现有的模型编辑方法虽然有效，但在编辑范围表达能力、处理多个编辑的能力以及计算效率方面仍有不足。</p><h2 id="research-objective">4. Research Objective</h2><p>开发一种高效、灵活且适用于多个模型的编辑方法，使其能够：</p><ul><li>对模型进行局部更新而不影响其他区域。</li><li>应对复杂的编辑范围需求。</li><li>在多编辑场景下保持性能稳定。</li></ul><h2 id="method">5. Method</h2><h3 id="模型结构">5.1 模型结构</h3><p>SERAC由以下三个主要组件组成：</p><ol type="1"><li><strong>显式编辑记忆（EditMemory）</strong>：存储用户提供的编辑描述。</li><li><strong>范围分类器（ScopeClassifier）</strong>：估计输入是否属于某个编辑的范围。</li><li><strong>反事实模型（CounterfactualModel）</strong>：根据编辑描述调整模型的输出。</li></ol><p align="center"><img src="1.png" style="zoom:30%;" /></p><h3 id="前向传播">5.2 前向传播</h3><p>对于给定的输入 <span class="math inline">\(x&#39;\)</span> 和编辑集合<span class="math inline">\(Z_e\)</span>，SERAC 的预测由以下公式定义：<span class="math display">\[\tilde{f}\left(x^{\prime}\right)= \begin{cases}f_{\text {base}}\left(x^{\prime}\right) &amp; \text { if } \beta&lt;0.5 \\h_\psi\left(z_e^*, x^{\prime}\right) &amp; \text { if } \beta \geq0.5\end{cases}\]</span> 其中：</p><ul><li><span class="math inline">\(\beta=g_\phi\left(z_e^*,x^{\prime}\right)\)</span> 是范围分类器的相似性得分。</li><li><span class="math inline">\(z_e^*=\arg \max _{z_e \in Z_e}g_\phi\left(z_e, x^{\prime}\right)\)</span>。</li></ul><h3 id="损失函数">5.3 损失函数</h3><ol type="1"><li><strong>范围分类器</strong>：用于解决二分类问题，其目标是将输入正确分为“在范围内”和“超出范围”。损失函数为：</li></ol><p><span class="math display">\[\mathcal{L}(\phi)=-\mathbb{E}_{z_e \sim D_e}\left[\log g_\phi\left(z_e,x_{\text {in }}\right)+\log \left(1-g_\phi\left(z_e, x_{\text {out}}\right)\right)\right]\]</span></p><ol type="1"><li><strong>反事实模型</strong>：用于预测范围内输入的标签，其目标是最小化以下负对数似然损失：</li></ol><p><span class="math display">\[\mathcal{L}(\psi)=-\mathbb{E}_{z_e \sim D_e} \mathbb{E}_{\left(x_{\text{in }}, y_{\text {in }}\right) \sim I\left(z_c ; D_e\right)}\left[\logp_\psi\left(y_{\text {in }} \mid z_e, x_{\text {in }}\right)\right]\]</span></p><h2 id="evaluation">6. Evaluation</h2><p>本文通过三个新的编辑问题来评估模型编辑器：基于问答、事实核查和对话生成的任务。这些任务旨在测试模型编辑器处理更难的范围内和范围外示例的能力。实验结果表明，SERAC在所有三个问题上均取得了高性能，显著优于现有方法。</p><h2 id="conclusion">7. Conclusion</h2><p>本文提出的 SERAC方法在多个编辑问题上均优于现有方法，特别是在处理多次编辑、复杂编辑范围和非输入输出对的编辑描述符时。SERAC不依赖于基础模型参数，不需要计算梯度来应用编辑，可以一次性训练并立即编辑具有不同架构的多个模型，并且可以处理自然语言指定的编辑。尽管SERAC具有这些有用的特性，但也存在一些局限性，例如作为可学习编辑器，它依赖于用于训练分类器和反事实模型的编辑数据集。未来的工作可能包括解决编辑记忆无限制增长的问题，以及探索更复杂的检索架构。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>NLP</tag>
      
      <tag>知识编辑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - FAST MODEL EDITING AT SCALE</title>
    <link href="/2024/12/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-FAST-MODEL-EDITING-AT-SCALE/"/>
    <url>/2024/12/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-FAST-MODEL-EDITING-AT-SCALE/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: FAST MODEL EDITING AT SCALE<br /><strong>Link</strong>: <a href="https://arxiv.org/pdf/2110.11309">MENDPaper</a><br /><strong>Source</strong>: International Conference on LearningRepresentations (ICLR)<br /><strong>Date</strong>: 2021.10.21</p><h2 id="summary">2. Summary</h2><p>本文介绍了一种名为模型编辑网络与梯度分解 <strong>MEND</strong>（ModelEditor Networks with GradientDecomposition）的新方法，用于高效编辑大型预训练模型，特别是那些参数超过100 亿的模型。MEND允许使用单个期望的输入输出对快速、局部地编辑模型行为，无需进一步微调。该方法利用<strong>微调梯度</strong>的<strong>低秩结构</strong>，实现可行且有效的大规模模型编辑。</p><h2 id="background">3. Background</h2><p>自然语言处理和计算机视觉中大型预训练模型的维护和更新挑战。尽管这些模型功能强大，但可能会出错或过时。传统的微调方法可能导致过拟合或计算成本高昂，尤其是对于非常大的模型。因此，需要一种方法能够在不影响模型在其他任务上的表现的情况下，纠正特定输入的模型输出。</p><h2 id="research-objective">4. Research Objective</h2><p>本研究的主要目标是开发一种可扩展的算法，使大型预训练模型能够快速进行事后编辑。目标包括确保编辑后的模型保持<strong>可靠性</strong>（对编辑输入的正确输出）、<strong>局部性</strong>（对不相关输入的影响最小）和<strong>普遍性</strong>（对相关输入的正确输出）。</p><h2 id="method">5. Method</h2><p align="center"><img src="1.png" style="zoom:80%;" /></p><h3 id="问题定义">5.1 问题定义</h3><p>在模型编辑问题中，目标是使用单个输入输出对 <spanclass="math inline">\(\left(x_e, y_e\right)\)</span>来改变基础模型对于特定输入的 <span class="math inline">\(x_e\)</span>输出，同时保持对不相关输入的模型行为不变。这要求编辑操作具有可靠性、局部性和普遍性。</p><h3 id="mend-架构">5.2 MEND 架构</h3><p>MEND 通过训练一系列轻量级的模型编辑网络 <spanclass="math inline">\(g_{\ell}\)</span>来实现编辑。这些网络接收<strong>标准微调的梯度</strong>作为输入，并输出对应的梯度更新，以此来编辑预训练模型的权重。MEND的关键特点是利用梯度的<strong>低秩结构</strong>，这使得参数化变得可行。</p><h3 id="梯度分解">5.3 梯度分解</h3><p>对于一个全连接层的梯度，MEND 利用梯度矩阵是 rank-1的特性，将梯度分解为其 rank-1 的外积形式。具体来说，对于第 <spanclass="math inline">\(l\)</span> 层的权重矩阵 <spanclass="math inline">\(W_l\)</span>，其梯度可以表示为： <spanclass="math display">\[\nabla_{W_l} L=\delta_{l+1} u_l^T\]</span> 这样，每个 MEND 网络的输入是微调梯度的两个因子 <spanclass="math inline">\(\left(\delta_{l+1}, u_l\right)\)</span>，其中<span class="math inline">\(\delta_{l+1}\)</span> 是第 <spanclass="math inline">\(l+1\)</span> 层的误差项，<spanclass="math inline">\(u_l\)</span> 是第 <spanclass="math inline">\(l\)</span> 层的输入。</p><h3 id="参数化">5.4 参数化</h3><p align="center"><img src="2.png" style="zoom:80%;" /></p><p>MEND 通过多层感知器（MLP）来参数化这些梯度映射函数。每个 MLP有一个隐藏层，并且使用低秩权重矩阵，以减少参数数量。具体来说，MEND网络的参数化形式为： <span class="math display">\[h_l=z_l+\sigma\left(s_1^l \odot\left(U_1 V_1 z_l+b\right)+o_1^l\right)\]</span></p><p><span class="math display">\[g\left(z_l\right)=h_l+\sigma\left(s_2^l \odot U_2 V_2 h_l+o_2^l\right)\]</span></p><p>其中，<span class="math inline">\(z_l=\operatorname{concat}\left(u_l,\delta_{l+1}\right)\)</span> 是输入向量，<spanclass="math inline">\(\sigma\)</span> 是激活函数，<spanclass="math inline">\(U_1, V_1, U_2, V_2\)</span>是低秩权重矩阵，它们通过低秩分解来减少模型的参数量。这种方法使得每个编辑网络可以用更少的参数来学习如何从梯度映射到参数更新，从而提高了计算效率。<spanclass="math inline">\(s_1^l, s_2^l, o_1^l, o_2^l\)</span>是层特定的缩放和偏移参数。</p><h3 id="训练">5.5 训练</h3><p>MEND 使用编辑训练集 <span class="math inline">\(D_{e d i t}^{tr}\)</span> 来学习每个编辑网络的参数。在训练过程中，对于每一层 <spanclass="math inline">\(l\)</span>，MEND 计算原始梯度 <spanclass="math inline">\(\nabla_{W_{\ell}}p_{\theta_{\mathcal{W}}}\left(y_{\mathrm{e}} \midx_{\mathrm{e}}\right)\)</span>，然后使用 MEND 网络计算权重更新量 <spanclass="math inline">\(\tilde{\nabla}_{W_{\ell}}\)</span>，最后更新权重<span class="math inline">\(\tilde{W}=W_{\ell}-\alpha_{\ell}\tilde{\nabla}_{W_{\ell}}\)</span>。</p><h3 id="损失函数">5.6 损失函数</h3><p>MEND 的训练损失包括编辑成功 <spanclass="math inline">\(\left(L_e\right)\)</span> 和编辑局部性 <spanclass="math inline">\(\left(L_{l o c}\right)\)</span>两个部分。具体来说： <span class="math display">\[L_{\mathrm{e}}=-\log p_{\theta_{\tilde{w}}}\left(y_{\mathrm{e}}^{\prime}\mid x_{\mathrm{e}}^{\prime}\right)\]</span></p><p><span class="math display">\[L_{\text {loc}}=\operatorname{KL}\left(p_{\theta_{\mathcal{W}}}\left(\cdot \midx_{\text {loc }}\right) \| p_{\theta_{\tilde{w}}}\left(\cdot \midx_{\text {loc }}\right)\right)\]</span></p><p>其中，<span class="math inline">\(\left(L_e\right)\)</span>衡量模型是否成功更新了编辑示例的输出，而衡 <spanclass="math inline">\(\left(L_{l o c}\right)\)</span>量编辑是否影响了不相关输入的模型行为，模型的总损失如下： <spanclass="math display">\[L_{\mathrm{MEND}}=c_{\mathrm{e}}L_{\mathrm{e}}\left(\theta_{\tilde{\mathcal{W}}}\right)+L_{\mathrm{loc}}\left(\theta_{\mathcal{W}},\theta_{\tilde{\mathcal{W}}}\right)\]</span> 在 MEND 中，使用编辑后的模型参数 <spanclass="math inline">\(\tilde{W}\)</span>来计算训练损失，这些损失将被反向传播到编辑网络中。这个过程不涉及预训练模型的参数优化。</p><h2 id="conclusion">6. Conclusion</h2><p>MEND通过梯度分解和轻量级编辑网络，实现了对大型预训练模型的快速和局部编辑。这种方法不仅减少了参数数量，而且通过利用梯度的低秩结构，使得编辑操作变得高效和可扩展。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>知识编辑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
    <link href="/2024/12/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale/"/>
    <url>/2024/12/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: An Image is Worth 16x16 Words: Transformersfor Image Recognition at Scale<br /><strong>Link</strong>: <a href="https://arxiv.org/pdf/2010.11929">ViTPaper</a><br /><strong>Source</strong>: International Conference on LearningRepresentations (ICLR)<br /><strong>Date</strong>: 2020.10.22</p><h2 id="summary">2. Summary</h2><p>本文介绍了 Vision Transformer（ViT），这是一种将 Transformer架构应用于图像识别任务的模型。主要贡献包括：</p><ul><li>证明 Transformer架构可以直接用于图像分类，无需依赖卷积神经网络（CNNs）。</li><li>展示当在大型数据集上预训练然后转移到小型基准测试时，ViT能够达到与最先进的CNN相媲美甚至更优的结果。</li><li>强调 ViT 在训练过程中需要的计算资源显著少于CNNs，使其成为图像识别任务的高效替代方案。</li></ul><h2 id="background">3. Background</h2><p>Transformer 模型在 NLP 任务中已取得了显著的成功，超越了传统的 LSTM 和CNN模型。然而，它在计算机视觉领域的应用却相对较少。在图像任务中，卷积神经网络（CNNs）由于其捕捉图像空间层次结构的能力，一直占据主导地位。ViT挑战了这一传统，通过用 Transformer 架构替代卷积操作，Transformer更灵活，能够建模长程依赖关系。</p><h2 id="research-objective">4. Research Objective</h2><p>本研究的主要目标是证明变换器在图像识别任务中的可行性。具体目标包括：</p><ul><li>提出基于变换器架构的新型图像识别模型——视觉变换器（ViT）。</li><li>探讨数据集大小在训练 ViT中的作用，并表明变换器需要大规模的数据集来有效学习。</li><li>研究预训练的 ViT 模型及其迁移学习的优点。</li></ul><h2 id="method">5. Method</h2><p align="center"><img src="1.png" style="zoom:60%;" /></p><h3 id="图像分块patch-embedding">5.1 <strong>图像分块（PatchEmbedding）</strong></h3><ul><li>将输入图像划分为 16 x 16 像素的固定大小块，这些块被视为 Transformer中的 “tokens”，类似于自然语言处理中的单词。</li><li>每个图像块通过一个线性层映射到一个 D维的空间，这个线性层的参数是可学习的。</li></ul><h3 id="位置编码">5.2 <strong>位置编码</strong></h3><ul><li>在 ViT中，位置编码使用一维的可学习向量，这些向量的长度与图像块嵌入向量的长度相同。</li><li>每个位置（即每个图像块）都有一个对应的位置编码向量，这些向量被随机初始化并在训练过程中与图像块的嵌入一起学习。</li></ul><h3 id="transformer-encoder">5.3 <strong>TransformerEncoder</strong></h3><ul><li>输入序列（图像块嵌入加上位置编码）被送入的 Transformer 编码器。</li><li>编码器由多个层组成，每层包括多头自注意力（Multi-HeadSelf-Attention，MSA）模块和多层感知机（MultilayerPerceptron，MLP）模块。</li><li>在每个 MSA 和 MLP 模块前应用层归一化（LayerNormalization），并在每个模块后应用残差连接。</li></ul><h3 id="分类标记class-token">5.4 <strong>分类标记（ClassToken）</strong></h3><ul><li>类似于 BERT 中的 [CLS] 标记，ViT 中引入了一个额外的可学习 “[class]”标记，它被添加到序列的开始位置。</li><li>在 Transformer 编码器的最后，“[class]”标记的状态被用作整个图像的表示，用于图像分类任务。</li></ul><h3 id="微调">5.5 微调</h3><ul><li>在微调阶段，ViT移除预训练时的分类头（即用于分类的全连接层），并用一个零初始化的新的全连接层替代，新的全连接层的输出维度是下游任务的类别数<span class="math inline">\(K\)</span>。</li></ul><h2 id="conclusion">6. Conclusion</h2><p>Transformer架构可以直接应用于图像识别任务，并且在大规模数据集上预训练后，ViT能够在多个图像识别基准上达到或超过当时的最先进性能。此外，ViT在训练过程中需要的计算资源显著少于传统的CNNs，显示出其在效率上的优势。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>CV</tag>
      
      <tag>Transformer</tag>
      
      <tag>Pre-training</tag>
      
      <tag>Fine-tuning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Locating and Editing Factual Associations in GPT</title>
    <link href="/2024/12/03/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Locating-and-Editing-Factual-Associations-in-GPT/"/>
    <url>/2024/12/03/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Locating-and-Editing-Factual-Associations-in-GPT/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: Locating and Editing Factual Associations inGPT<br /><strong>Link</strong>: <a href="https://arxiv.org/pdf/2202.05262">ROMEPaper</a><br /><strong>Source</strong>: Conference on Neural Information ProcessingSystems (NeurIPS)<br /><strong>Date</strong>: 2022.02.10</p><h2 id="summary">2. Summary</h2><p>本文提出了一种方法，用于在训练完成的 GPT模型中定位和编辑事实知识。该方法的核心思想是通过识别 Transformer网络中的关键组件（特别是 MLP和注意力层），找到存储事实知识的位置，并通过修改这些组件来插入、修改或删除特定的事实。通过在GPT-2上进行演示，表明可以通过操作隐藏激活值来编辑事实知识，而不需要重新训练整个模型。本文的主要贡献在于能够探查Transformer 的内部状态，并通过修改这些状态来编辑模型中的事实关联。</p><h2 id="background">3. Background</h2><p>最近，大型语言模型，特别是像 GPT 这样的 Transformer模型，在预训练过程中编码了大量的事实知识。然而，关于这些知识在模型内部的具体位置和结构仍不明确。能够在模型中定位并修改这些事实关联，将在错误修正、知识插入以及模型个性化等方面具有广泛的应用。虽然有一些研究探讨了如何探查和修改Transformer 模型，但直接编辑模型中具体事实知识的工作还相对较少。</p><h2 id="research-objective">4. Research Objective</h2><p>本研究的主要目标是提出一种方法，用于在预训练的GPT模型中定位和编辑事实知识。具体目标包括：</p><ul><li>识别模型的哪些组件（如层、注意力头等）存储了事实知识。</li><li>开发一种技术，通过修改特定的组件（如隐藏状态），改变模型中的事实知识。</li></ul><h2 id="method">5. Method</h2><p>本文提出的方法主要分为两个部分：因果追踪（Causal Tracing）和 Rank-OneModel Editing (ROME)。下面详细解释这两种方法：</p><h3 id="因果追踪causal-tracing">5.1 因果追踪（Causal Tracing）</h3><p>① <strong>知识元组表示</strong>：将每个事实表示为一个知识元组 <spanclass="math inline">\(t = (s, r, o)\)</span>，其中 <spanclass="math inline">\(s\)</span> 是主题，<spanclass="math inline">\(r\)</span> 是连接两者的关系，<spanclass="math inline">\(o\)</span> 是对象。</p><p>② <strong>模型输入</strong>：为了激发 GPT模型中的事实，研究者提供一个自然语言提示 <spanclass="math inline">\(p\)</span>，描述 <span class="math inline">\((s,r)\)</span>，并检查模型对 <span class="math inline">\(o\)</span>的预测。</p><p>③ <strong>内部激活收集</strong>：在“干净运行”中，将事实提示 <spanclass="math inline">\(x\)</span> 输入模型 <spanclass="math inline">\(G\)</span> 并收集所有隐藏激活： <spanclass="math display">\[\{ h_i^{(l)} \mid i \in [1, T], l \in [1, L] \}\]</span> 其中 <span class="math inline">\(h_i^{(l)}\)</span> 表示第<span class="math inline">\(l\)</span> 层的第 <spanclass="math inline">\(i\)</span> 个 token 的隐藏状态，<spanclass="math inline">\(T\)</span> 是序列中 token 的总数，<spanclass="math inline">\(L\)</span> 是模型的层数。</p><p>④ <strong>基线损坏运行</strong>：在“损坏运行”中，将主题 <spanclass="math inline">\(s\)</span>的嵌入向量损坏，然后让模型继续运行，收集损坏的激活。</p><p>⑤<strong>损坏与恢复运行</strong>：在“损坏与恢复运行”中，模型在损坏的嵌入上运行计算，但在某些标记<span class="math inline">\(\hat{i}\)</span> 和层 <spanclass="math inline">\(\hat{l}\)</span> 上，强制模型输出干净的隐藏状态<span class="math inline">\(h_{\hat{i}}^{(\hat{l})}\)</span>。</p><p>⑥<strong>因果效应量化</strong>：通过比较干净、损坏和损坏与恢复运行下的概率<span class="math inline">\(P[o]\)</span>，<spanclass="math inline">\(P^*[o]\)</span> 和 <spanclass="math inline">\(P_{\text{clean}}^{h(l)_i^*}[o]\)</span>，计算总效应（TE）和特定中介状态的间接效应（IE）。</p><ul><li><p><strong>总效应</strong>（TotalEffect，TE）：反映了损坏主题信息对模型预测的整体影响。通过比较干净运行和损坏运行下的预测概率差异来量化。<span class="math display">\[TE = P[o] - P^*[o]\]</span></p></li><li><p><strong>间接效应</strong>（IndirectEffect，IE）：反映了在特定层或位置上的恢复操作对最终预测的影响。通过比较损坏与恢复运行与损坏运行下的预测概率差异来量化。<span class="math display">\[IE = P_{\text{clean}}^{h(l)_i^*}[o] - P^*[o]\]</span></p></li></ul><p>通过这些度量，研究者能够量化和分析每个隐藏状态（尤其是在不同层次和token 位置的激活）对模型的事实推理和预测的贡献。</p><p align="center"><img src="1.gif" style="zoom:50%;" /></p><h3 id="rank-one-model-editing-rome">5.2 Rank-One Model Editing(ROME)</h3><p>基于上述因果追踪的发现，作者提出了一个假设：中间层的 MLP模块通过接受和处理关于某个主题的输入，逐步累积相关的属性信息（例如，对于“美国”主题，输出可能是与其相关的属性（如“总统是拜登”或“首都华盛顿”））。而这种信息会通过高层的注意力机制，传递到最后一个token 上，从而生成与该主题相关的最终输出。</p><p>将 MLP 的权重 <spanclass="math inline">\(W_{\text{proj}}^{(l)}\)</span>视为线性联想记忆，可以通过解决 <span class="math inline">\(W K \approxV\)</span> 来存储一系列向量键 <span class="math inline">\(K\)</span>和对应的向量值 <span class="math inline">\(V\)</span>。</p><p>下图展示了 Transformer 内的单个 MLP 模块，图中的(b)处的 <spanclass="math inline">\(D\)</span>维向量作为表示要了解的主体的<strong>键</strong>。图中的(c)处的 <spanclass="math inline">\(H\)</span>维输出作为编码有关主体的学习属性的<strong>值</strong>。</p><p align="center"><img src="2.png" style="zoom:80%;" /></p><p>目标是通过拉格朗日乘子法来求解最小二乘问题，其中对新的键值对 <spanclass="math inline">\((k_*, v_*)\)</span>插入记忆。我们要最小化目标函数，同时确保约束条件 <spanclass="math inline">\(\hat{W} k_* = v_*\)</span>被满足。通过这些步骤，我们最终可以计算出最优的权重矩阵 <spanclass="math inline">\(\hat{W}\)</span>，并利用拉格朗日乘子法解出相关的乘子<span class="math inline">\(\Lambda\)</span>。</p><h5 id="目标函数和约束">① <strong>目标函数和约束</strong></h5><p>我们有以下最小二乘目标函数： <span class="math display">\[\operatorname{minimize} \|\hat{W} K - V\|^2 \quad \text{subject to}\quad \hat{W} k_* = v_*\]</span> 这里 <span class="math inline">\(\hat{W}\)</span>是权重矩阵，<span class="math inline">\(K\)</span> 和 <spanclass="math inline">\(V\)</span> 是键和值矩阵，<spanclass="math inline">\(k_*\)</span> 是一个新的键，<spanclass="math inline">\(v_*\)</span> 是其对应的值。目标是最小化 <spanclass="math inline">\(\hat{W} K\)</span> 和 <spanclass="math inline">\(V\)</span> 之间的误差，并通过约束条件 <spanclass="math inline">\(\hat{W} k_* = v_*\)</span>强制插入新的键值对。</p><h5 id="构造拉格朗日函数">② <strong>构造拉格朗日函数</strong></h5><p>我们将目标函数和约束结合，构造拉格朗日函数： <spanclass="math display">\[\mathcal{L}(\hat{W}, \Lambda) = \|\hat{W} K - V\|^2 + \Lambda^T (\hat{W}k_* - v_*)\]</span> 其中 <span class="math inline">\(\Lambda\)</span>是拉格朗日乘子向量。</p><h5 id="对-hatw-求偏导数">③ <strong>对 <spanclass="math inline">\(\hat{W}\)</span> 求偏导数</strong></h5><p>对拉格朗日函数 <span class="math inline">\(\mathcal{L}\)</span> 关于<span class="math inline">\(\hat{W}\)</span> 求偏导数并令其为零： <spanclass="math display">\[0 = \frac{\partial \mathcal{L}}{\partial \hat{W}} = \hat{W} K K^T - VK^T - \Lambda k_*^T\]</span> 这给出了如下方程： <span class="math display">\[\hat{W} K K^T = V K^T + \Lambda k_*^T\]</span></p><h5 id="解出-hatw-的表达式">④ <strong>解出 <spanclass="math inline">\(\hat{W}\)</span> 的表达式</strong></h5><p>通过整理上式，可以得到： <span class="math display">\[(\hat{W} - W) K K^T = \Lambda k_*^T\]</span> 因此： <span class="math display">\[\hat{W} = W + \Lambda (C^{-1} k_*)^T\]</span> 其中 <span class="math inline">\(C = K K^T\)</span>，定义<span class="math inline">\(u^T = (C^{-1} k_*)^T\)</span> 可得： <spanclass="math display">\[\hat{W} = W + \Lambda u^T\]</span></p><h5 id="对-lambda-求解">⑤ <strong>对 <spanclass="math inline">\(\Lambda\)</span> 求解</strong></h5><p>根据约束 <span class="math inline">\(\hat{W} k_* = v_*\)</span>，将<span class="math inline">\(\hat{W} = W + \Lambda u^T\)</span> 代入得：<span class="math display">\[\hat{W} k_* = (W + \Lambda u^T) k_* = W k_* + \Lambda (u^T k_*) = v_*\]</span> 由此，得到拉格朗日乘子的表达式： <span class="math display">\[\Lambda = \frac{v_* - W k_*}{u^T k_*} = \frac{v_* - W k_*}{(C^{-1}k_*)^T k_*}\]</span></p><h5 id="最终的-hatw-表达式">⑥ <strong>最终的 <spanclass="math inline">\(\hat{W}\)</span> 表达式</strong></h5><p>最后，我们可以将求得的 <span class="math inline">\(\Lambda\)</span>值代入 <span class="math inline">\(\hat{W} = W + \Lambda u^T\)</span>中，得到最优的 <span class="math inline">\(\hat{W}\)</span>。</p><p>综上，只要计算得到新的键值对 <span class="math inline">\(\left(k_*,v_*\right)\)</span>，即可将事实插入现有模型。</p><h4 id="step-1计算-k_">Step 1：计算 <spanclass="math inline">\(k_*\)</span></h4><p>通过输入文本 <span class="math inline">\(x\)</span> 和主题 <spanclass="math inline">\(s\)</span>，通过 GPT模型传递这些数据，并在模型的特定层 <spanclass="math inline">\(l^*\)</span> 和最后一个主题 token 的位置 <spanclass="math inline">\(i\)</span>，计算其 MLP 层的激活值。</p><p>假设我们通过以下公式计算 <spanclass="math inline">\(k(x)\)</span>，这是表示输入文本 <spanclass="math inline">\(x\)</span> 和对应主题 <spanclass="math inline">\(s\)</span> 的激活值： <spanclass="math display">\[k(x)=\sigma\left(W_{f c}^{\left(l^*\right)}\gamma\left(a^{\left(l^*\right)}[x, i]+h^{\left(l^*-1\right)}[x,i]\right)\right)\]</span> 其中：</p><ul><li><span class="math inline">\(\sigma\)</span> 是激活函数（如 ReLU 或Tanh），</li><li><span class="math inline">\(W_{fc}^{(l^*)}\)</span> 是层 <spanclass="math inline">\(l^*\)</span> 中的全连接层权重，</li><li><span class="math inline">\(\gamma\)</span> 是对当前 token <spanclass="math inline">\(x\)</span> 和前一层的激活值 <spanclass="math inline">\(h^{(l^*-1)}[x, i]\)</span> 的变换，</li><li><span class="math inline">\(a^{(l^*)}[x, i]\)</span> 是在层 <spanclass="math inline">\(l^*\)</span> 中通过注意力机制计算的 token的激活值。</li></ul><p>通过对一组文本进行多次采样，得到所有输入 <spanclass="math inline">\(x_j\)</span> 和主题 <spanclass="math inline">\(s\)</span> 的平均值，计算出查找键 <spanclass="math inline">\(k^*\)</span>： <span class="math display">\[k^* = \frac{1}{N} \sum_{j=1}^N k(x_j + s)\]</span> 这个过程帮助我们识别模型中存储事实知识的组件。</p><h4 id="step-2计算-v_">Step 2：计算 <spanclass="math inline">\(v_*\)</span></h4><p>首先构造一个损失函数 <spanclass="math inline">\(\mathcal{L}(z)\)</span>： <spanclass="math display">\[\frac{1}{N} \sum_{j=1}^N \underbrace{-\log\mathbb{P}_{G\left(m_i^{\left(l^*\right)}:=z\right)}\left[o^* \midx_j+p\right]}_{\text {(a) Maximizing } o^* \text { probability}}+\underbrace{D_{\mathrm{KL}}\left(\mathbb{P}_{G\left(m_{i^{\prime}}^{\left(l^*\right)}:=z\right)}\left[x\mid p^{\prime}\right] \| \mathbb{P}_G\left[x \midp^{\prime}\right]\right)}_{\text {(b) Controlling essence drift }} .\]</span> （a）是为了最大化目标对象 <spanclass="math inline">\(o^*\)</span> 的概率, 鼓励模型在给定 <spanclass="math inline">\(z\)</span> 作为输出时，更倾向于预测目标对象 <spanclass="math inline">\(o^*\)</span>。</p><p>（b）通过最小化 KL 散度来控制模型对主题 <spanclass="math inline">\(s\)</span> 的本质理解的变化。</p><p>在优化过程中，不改变模型的参数，而是通过调整 <spanclass="math inline">\(z\)</span> 来最小化损失函数 <spanclass="math inline">\(L(z)\)</span>。最终得到的 <spanclass="math inline">\(z\)</span> 就是 <spanclass="math inline">\(v\)</span>，这个向量能够在不改变模型权重的情况下，改变模型对特定事实的预测。</p><h4 id="step-3使用-leftk_-v_right-对模型进行修改">Step 3：使用 <spanclass="math inline">\(\left(k_*, v_*\right)\)</span> 对模型进行修改</h4><h4id="举例说明将现有事实艾菲尔铁塔位于巴黎修改为艾菲尔铁塔位于罗马">举例说明：将现有事实“艾菲尔铁塔位于巴黎”修改为“艾菲尔铁塔位于罗马”</h4><p>第一步：首先输入多个包含埃菲尔铁塔的语句，例如：“美丽的埃菲尔铁塔”等，收集 <spanclass="math inline">\(k_*\)</span>。</p><p>第二步： 输入 <span class="math inline">\(x_j+p\)</span> 这里的 <spanclass="math inline">\(x_j\)</span>是包含“埃菲尔铁塔”的语句，可以是“埃菲尔铁塔位于”，模型通过优化调整 <spanclass="math inline">\(z\)</span>，使得最终输出 “罗马”。</p><p>第三步： 使用 <span class="math inline">\(k_*\)</span> 和 <spanclass="math inline">\(v_*\)</span>来修改模型中存储的原有事实。这意味着当模型再次遇到与“埃菲尔铁塔”相关的输入时，它将使用新的<span class="math inline">\(v_*\)</span>向量来生成输出，从而将“埃菲尔铁塔位于巴黎”修改为“埃菲尔铁塔位于罗马”。</p><h2 id="conclusion">6. Conclusion</h2><p>本文证明了可以通过操作 GPT 模型中 MLP层的隐藏状态，定位并编辑事实知识。该方法能够在不重新训练整个模型的情况下，进行特定事实的修改。该方法对个性化AI、模型纠错以及动态更新模型中的事实知识具有重要意义。</p><h2 id="notes">7. Notes</h2><h3 id="rome-文章主页">7.1 <a href="https://rome.baulab.info/">ROME文章主页</a></h3>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>NLP</tag>
      
      <tag>知识编辑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Improving Language Understanding by Generative Pre-Training</title>
    <link href="/2024/12/03/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Improving-Language-Understanding-by-Generative-Pre-Training/"/>
    <url>/2024/12/03/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Improving-Language-Understanding-by-Generative-Pre-Training/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: Improving Language Understanding byGenerative Pre-Training<br /><strong>Link</strong>: <ahref="https://hayate-lab.com/wp-content/uploads/2023/05/43372bfa750340059ad87ac8e538c53b.pdf">GPTV1 Paper</a><br /><strong>Source</strong>: OpenAI Technical Report<br /><strong>Date</strong>: 2018</p><h2 id="summary">2. Summary</h2><ul><li>本文提出了一种基于 Transformer解码器的生成式语言模型，该模型通过在大规模无监督数据集上进行预训练，从而显著提高了下游任务（如问答、文本分类等）的性能。</li><li>通过预训练和微调的框架，GPT 模型减少了对大规模标注数据的依赖。</li></ul><h2 id="background">3. Background</h2><p>研究解决了特定自然语言任务的标记数据稀缺问题，而大量未标记文本语料库却很丰富。它建立在利用未标记数据可以提供替代收集更多注释的方法，并提高NLP任务性能的理念上。其重要性在于减少对监督学习的依赖，并提高语言理解的技术水平。</p><h2 id="research-objective">4. Research Objective</h2><p>研究目标是学习一个能够很好地转移到广泛任务的通用表示。作者旨在证明，一个在未标记文本上预训练的模型在特定自然语言理解任务上进行微调后，可以显著提高性能。</p><h2 id="method">5. Method</h2><p align="center"><img src="1.png" style="zoom:50%;" /></p><h3 id="self-supervised-pre-training">5.1 Self-supervisedpre-training</h3><p>GPT采用了<strong>自回归</strong>模型，这意味着在每次预测时，模型仅依赖于左侧的上下文信息（单向注意力）。与双向的Transformer 模型（例如 BERT ）不同，GPT的训练过程仅基于从左到右的文本顺序。</p><p>GPT 使用的是基于 <strong>Transformer解码器</strong>的结构，而非完整的 Transformer。每个Transformer块包括多个注意力头和全连接层，层与层之间通过残差连接和层归一化进行处理。与原始的Transformer 解码器相比，GPT省略了交叉注意力（Cross-attention）部分。</p><p>优化目标是基于之前窗口内的词，最大化预测下一个词的概率，公式如下：<span class="math display">\[L_1(\mathcal{U})=\sum_i \log P\left(u_i \mid u_{i-k}, \ldots, u_{i-1} ;\Theta\right)\]</span></p><h3 id="supervised-fine-tuning">5.2 Supervised fine-tuning</h3><p>在预训练的模型后，添加了一个线性层，对最后一个 Token（<spanclass="math inline">\(x^m\)</span>） 的输出（<spanclass="math inline">\(h_l^m\)</span>）映射到任务特定的输出空间。通过这种方式，模型能够针对特定任务进行微调，以适应不同的下游任务需求。<span class="math display">\[P\left(y \mid x^1, \ldots, x^m\right)=\operatorname{softmax}\left(h_l^mW_y\right)\]</span> 优化目标是最大化下述概率： <span class="math display">\[L_2(\mathcal{C})=\sum_{(x, y)} \log P\left(y \mid x^1, \ldots,x^m\right)\]</span> 其中，<span class="math inline">\((x, y)\)</span>代表训练数据中的输入输出对。</p><p>此外综合考虑上述两个优化目标，可以 (a) 增强模型泛化性和 (b)加速模型收敛，公式如下： <span class="math display">\[L_3(\mathcal{C})=L_2(\mathcal{C})+\lambda * L_1(\mathcal{C})\]</span></p><h3 id="基于任务的输入转换">5.3 基于任务的输入转换</h3><p>在微调阶段，为了使预训练模型能够处理各种不同的下游任务，需要对输入数据进行特定的转换，以适应模型的输入要求。这些转换允许模型将结构化输入（例如，问答任务中的文档、问题和答案）转换为连续的标记序列，以便模型可以处理。这些输入转换使得模型能够在不同任务之间进行有效的迁移学习，而无需对模型架构进行大量修改。具体的转换如下：</p><p align="center"><img src="2.png" style="zoom:67%;" /></p><h2 id="conclusion">6. Conclusion</h2><p>生成预训练后进行判别式微调是实现强大自然语言理解的有效框架。该方法在预训练期间获得了世界知识和处理长距离依赖关系的能力，这些能力成功地转移到解决判别式任务上，提高了多个数据集上的技术水平。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Transformer</tag>
      
      <tag>Pre-training</tag>
      
      <tag>Fine-tuning</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
    <link href="/2024/11/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/"/>
    <url>/2024/11/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: BERT: Pre-training of Deep BidirectionalTransformers for Language Understanding<br /><strong>Link</strong>: <a href="https://arxiv.org/pdf/1810.04805">BERTPaper</a><br /><strong>Source</strong>: The North American Chapter of the Associationfor Computational Linguistics (NAACL)<br /><strong>Date</strong>: 2018.10.11</p><h2 id="summary">2. Summary</h2><p>本文提出了 BERT（Bidirectional Encoder Representations fromTransformers），一种用于语言理解的深度双向变换器预训练方法。BERT的创新之处在于其使用了双向上下文，与之前的单向上下文模型（如GPT）不同。该预训练模型可以通过最小的任务特定架构进行微调，从而在多个NLP 任务中实现了最先进的性能，如问答和语言推理。</p><h2 id="background">3. Background</h2><p>在 BERT 之前，许多 NLP 模型依赖单向变换器（如 GPT）或顺序模型（如LSTM）。这些模型只能从左到右或从右到左处理文本，限制了它们捕捉丰富的双向上下文的能力。BERT通过使用变换器架构并以双向方式进行训练，克服了这一限制，使得模型能够理解句子中每个词的完整上下文，从而显著提升了NLP 性能。</p><h2 id="research-objective">4. Research Objective</h2><p>本文的目标是提出一种新的预训练方法，用于变换器模型，能够有效捕捉双向上下文。作者旨在提升现有NLP模型的表现，并创建一种更加灵活的通用模型，能够通过最小的改动在特定任务上进行微调。</p><h2 id="method">5. Method</h2><p>BERT的创新方法主要体现在其<strong>预训练任务</strong>和<strong>模型架构</strong>两个方面。通过这两种方式，BERT突破了传统的 NLP模型，在多个下游任务上取得了最先进的效果。以下详细介绍了 BERT的架构、预训练任务和微调方法。</p><h3 id="bert-模型架构">5.1 BERT 模型架构</h3><p>BERT 的架构基于 <strong>Transformer</strong>，尤其是 Transformer的<strong>编码器部分</strong>。Transformer 架构由Google 在《Attention IsAll YouNeed》论文中提出，具有很强的并行计算能力和建模长期依赖关系的能力，尤其适用于NLP 任务。BERT 仅使用了 Transformer中的编码器部分，并进行了双向训练。BERT的核心特点是<strong>双向训练</strong>，它通过使用 Transformer的自注意力机制来同时考虑上下文中的前后信息。</p><h4 id="transformer编码器">5.1.1 <strong>Transformer编码器</strong></h4><ul><li><strong>自注意力机制（Self-Attention）</strong>：自注意力机制计算输入序列中每个词对其他词的注意力权重。每个词的表示都基于它与所有其他词的关系。</li><li><strong>多头注意力（Multi-HeadAttention）</strong>：将自注意力机制扩展为多个“头”，每个头学习不同的子空间的注意力表示。所有头的结果被拼接起来并通过线性变换得到最终表示。</li><li><strong>前馈网络（Feed-Forward Networks）</strong>：每个 Transformer层还包括一个前馈神经网络，负责对每个位置的表示进行独立的非线性变换。</li></ul><p>BERT 采用了多层 Transformer编码器堆叠，每一层都包括自注意力机制和前馈网络，最终通过这些层来生成上下文相关的词表示。</p><h4 id="bert的嵌入embeddings">5.1.2<strong>BERT的嵌入（Embeddings）</strong></h4><p>在 BERT模型中，输入的每个单词或子词（token）都需要通过一定的嵌入（embedding）映射到一个高维空间，这些嵌入提供了对词语的丰富表示。BERT的输入嵌入由三部分组成：<strong>TokenEmbeddings</strong>（词嵌入）、<strong>SegmentEmbeddings</strong>（段落嵌入）和 <strong>PositionEmbeddings</strong>（位置嵌入）。这三种嵌入结合起来，帮助模型理解词汇、句子结构以及词语的顺序信息。</p><p align="center"><img src="1.png" style="zoom:60%;" /></p><h5 id="token-embeddings词嵌入"><strong>TokenEmbeddings（词嵌入）</strong></h5><p>Token Embeddings 是 BERT 模型中最基本的输入嵌入，主要是将每个输入的token（例如词或子词）映射到一个固定的向量空间。BERT 的 token嵌入使用了词表（vocabulary）中的每个 token 对应的一个嵌入向量。在 BERT中，token 通常使用 <strong>WordPiece</strong>分词法来切分，这意味着每个词或词组会被进一步拆分为子词单元，因此每个输入的token（子词）都会有对应的嵌入向量,通常是通过查表的方式获取。</p><h5 id="segment-embeddings段落嵌入"><strong>SegmentEmbeddings（段落嵌入）</strong></h5><p>BERT能够处理成对输入的任务，例如问答或句子对分类任务。在这些任务中，输入有两个部分，BERT需要区分这两个部分。为此，BERT 引入了 Segment Embeddings，它为每个 token分配一个标识符，指示它属于第一个句子（Segment A）还是第二个句子（SegmentB）,Segment Embeddings 不参与模型训练，是固定的值。</p><ul><li>对于第一个句子中的每个 token，它的 Segment Embedding 值为 0。</li><li>对于第二个句子中的每个 token，它的 Segment Embedding 值为 1。</li></ul><h5 id="position-embeddings位置嵌入"><strong>PositionEmbeddings（位置嵌入）</strong></h5><p>BERT 的 Transformer 架构本身并不具备处理词序信息的能力（不像 RNN 或LSTM那样顺序处理输入），因此需要引入位置嵌入来显式地编码每个词在序列中的位置信息。PositionEmbeddings 为每个 token 提供一个表示它在输入序列中位置的向量。</p><p>BERT 使用了<strong>可学习的位置编码</strong>（不像 Transformer的相对位置编码）。每个 token的位置都对应着一个唯一的嵌入向量，并且位置嵌入是与 token 嵌入、segment嵌入一起相加的。</p><h3 id="bert的预训练任务">5.2 <strong>BERT的预训练任务</strong></h3><p>BERT 的预训练任务非常关键，决定了模型如何学习语言的上下文信息。BERT的预训练任务包含两个主要部分：<strong>掩蔽语言建模（Masked LanguageModeling，MLM）</strong>和<strong>下一句预测（Next SentencePrediction，NSP）</strong>。</p><h4 id="掩蔽语言建模-mlm">5.2.1 <strong>掩蔽语言建模 (MLM)</strong></h4><p>掩蔽语言建模是 BERT的一项核心创新，它允许模型在训练过程中通过学习上下文来预测被掩蔽的词。与传统的语言建模方法（如GPT）不同，BERT并非仅从左到右或从右到左进行语言建模，而是通过掩蔽掉输入中的一些词并预测这些词来进行训练。</p><ul><li><p><strong>掩蔽过程</strong>：BERT 随机选择输入中的 15%的词进行掩蔽，并要求模型根据上下文预测这些被掩蔽的词。被选择的 15% 词中80% 的情况下，掩蔽的词被替换为“[MASK]”，10%的情况下，掩蔽的词保持不变，10%的情况下，掩蔽的词被替换为一个随机的词。</p></li><li><p><strong>训练目标</strong>：BERT的目标是最大化每个被掩蔽单词的预测概率。具体来说，给定输入文本 <spanclass="math inline">\(X = (x_1, x_2, ..., x_n)\)</span>，BERT会掩蔽其中的某些词，并要求模型预测这些词的值： <spanclass="math display">\[\hat{x}_i=\operatorname{BERT}\left(\tilde{X}_{-i}\right)\]</span> 其中，<span class="math inline">\(\tilde{X}_{-i}\)</span>表示去除第 <span class="math inline">\(i\)</span> 个词后的文本，<spanclass="math inline">\(\hat{x}_i\)</span> 是模型对第 <spanclass="math inline">\(i\)</span> 个词的预测。</p></li></ul><p>掩蔽语言建模的主要目标是让模型能够在上下文中获取对每个词的全局理解，从而增强语义理解能力。</p><h4 id="下一句预测-nsp">5.2.2 <strong>下一句预测 (NSP)</strong></h4><p>除了掩蔽语言建模，BERT还使用了下一句预测任务，这一任务用于捕捉句子之间的关系。NSP任务帮助模型理解句子对之间的语义关联，尤其是在理解长文档时。</p><ul><li><p><strong>任务描述</strong>：NSP 任务的目标是判断给定的一对句子<span class="math inline">\(S_1\)</span> 和 <spanclass="math inline">\(S_2\)</span> 是否在原始文档中是连续的，即判断<span class="math inline">\(S_2\)</span> 是否是 <spanclass="math inline">\(S_1\)</span> 的后续句子。如果 <spanclass="math inline">\(S_2\)</span> 是 <spanclass="math inline">\(S_1\)</span> 的后续句子，则标签为 1；否则为0。</p></li><li><p><strong>模型输入</strong>：将一对句子 <spanclass="math inline">\((S_1, S_2)\)</span> 拼接在一起，并通过 BERT进行处理。然后，输出一个二分类的标签，表示第二个句子是否是第一个句子的后续句子。</p><p>换句话说，给定一对句子 <span class="math inline">\(S_1\)</span> 和<span class="math inline">\(S_2\)</span>，模型的目标是计算： <spanclass="math display">\[P\left(\operatorname{IsNext} \mid S_1,S_2\right)=\operatorname{sigmoid}\left(W^T h_2+b\right)\]</span> 其中，<span class="math inline">\(h_2\)</span> 是第二个句子<span class="math inline">\(S_2\)</span> 在 BERT 模型中的表示，<spanclass="math inline">\(W\)</span> 和 <spanclass="math inline">\(b\)</span> 是训练的权重和偏置，sigmoid函数用于输出一个概率值，表示 <span class="math inline">\(S_2\)</span>是否是 <span class="math inline">\(S_1\)</span> 的后续句子。</p></li></ul><p>NSP任务对于句子级别的推理（例如，问答任务中的句子对匹配、文本蕴涵等任务）特别重要。</p><h3 id="bert的微调-fine-tuning">5.3 <strong>BERT的微调(Fine-tuning)</strong></h3><p>BERT 的一个重要特点是其强大的<strong>微调能力</strong>。BERT的预训练模型可以通过微调来适应特定的 NLP 任务。在微调时，BERT只需要少量的任务特定架构修改即可，因此能够在多种任务上展现出强大的性能。</p><ul><li><p><strong>微调过程</strong>：微调过程非常简单，通常只需在 BERT的顶部添加一个简单的输出层（例如分类层），然后根据具体任务进行训练。这使得BERT能够轻松适应各种任务，如文本分类、命名实体识别（NER）、问答、情感分析等。</p></li><li><p><strong>任务特定的输出层</strong>：不同任务需要不同的输出层。例如，对于分类任务，BERT的输出层通常是一个 softmax分类器；对于序列标注任务（如命名实体识别），输出层可能是一个标签的序列。</p></li><li><p><strong>微调公式</strong>：微调过程中，BERT的参数和输出层的参数都会共同更新。假设目标任务的损失函数为 <spanclass="math inline">\(\mathcal{L}_{\text {task}}\)</span>，则微调时的目标是最小化该任务的损失： <spanclass="math display">\[\mathcal{L}_{\text {total }}=\mathcal{L}_{\text {task }}\]</span>对于不同的任务，损失函数可以是交叉熵损失、均方误差损失等。</p></li></ul><h2 id="conclusion">6. Conclusion</h2><p>BERT 通过提出一种新的双向变换器预训练方法，在多个 NLP任务中设立了新的基准。它的通用性使其能够通过微调适应不同的任务，从而证明了双向预训练在语言理解中的强大能力。BERT的成功也展示了 NLP 中迁移学习的潜力。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Transformer</tag>
      
      <tag>Pre-training</tag>
      
      <tag>Fine-tuning</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Attention Is All You Need</title>
    <link href="/2024/11/28/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/"/>
    <url>/2024/11/28/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: Attention Is All You Need<br /><strong>Link</strong>: <ahref="https://arxiv.org/pdf/1706.03762">Transformer</a><br /><strong>Source</strong>: Conference on Neural Information ProcessingSystems (NeurIPS)<br /><strong>Date</strong>: 2017.06.12</p><h2 id="summary">2. Summary</h2><p>本文提出了 <strong>Transformer</strong>模型，这是一种用于序列转换任务（特别是机器翻译）的新型深度学习架构。Transformer的核心创新是<strong>自注意力机制</strong>，该机制使得模型能够高效地处理序列数据，而不依赖于循环神经网络（RNN）或卷积神经网络（CNN）。该模型在编码和解码阶段均使用自注意力和前馈神经网络，显著提高了并行化能力，减少了训练时间。实验结果表明，Transformer在机器翻译任务中超越了基于 LSTM 和 GRU的传统模型，取得了更好的性能。</p><h2 id="background">3. Background</h2><p>在 Transformer提出之前，序列到序列任务（如机器翻译）通常由<strong>循环神经网络（RNN）及其改进版本（如LSTM 和GRU）</strong>来处理。这些模型是逐步处理数据，因此存在并行化难度，长依赖关系的处理也存在瓶颈。<strong>卷积神经网络（CNN）</strong>在某些序列处理任务中也有应用，但它们在捕捉长程依赖关系时的能力较弱。</p><h2 id="research-objective">4. Research Objective</h2><p>本文的研究目标是提出一种新的模型架构，能够在不依赖循环或卷积层的情况下高效处理序列转换任务，尤其是机器翻译。具体目标包括：</p><ol type="1"><li>设计一个易于并行化的模型，从而提高训练效率。</li><li>开发一个能够捕捉长程依赖关系的模型，不受传统RNN模型的限制。</li><li>证明该模型在标准基准任务中超越传统RNN模型的表现。</li></ol><h2 id="method">5. Method</h2><h3 id="自注意力机制self-attention">5.1自注意力机制（Self-Attention）</h3><p>自注意力机制是 Transformer的核心创新之一。它允许模型在处理每个输入位置时，对序列中所有位置的其他元素进行加权求和，从而捕捉词语间的依赖关系。自注意力机制通过计算每个词与其他词之间的相似度，来决定其在表示中的重要性。</p><h5 id="计算过程">计算过程</h5><p>自注意力机制的关键计算过程可以通过以下三个向量来描述：</p><ul><li><strong>Query（查询向量）</strong>：代表当前词需要从其他词中获取信息的“查询”。</li><li><strong>Key（键向量）</strong>：表示当前词的特征，用于与查询进行比较。</li><li><strong>Value（值向量）</strong>：包含关于当前词的实际信息，用于加权求和。</li></ul><p>具体公式如下： <span class="math display">\[\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V\]</span></p><ul><li><span class="math inline">\(Q\)</span> 是查询矩阵，<spanclass="math inline">\(K\)</span> 是键矩阵，<spanclass="math inline">\(V\)</span> 是值矩阵。</li><li><span class="math inline">\(d_k\)</span> 是键向量的维度。</li><li><span class="math inline">\(Q K^T\)</span>计算查询和键之间的点积相似度，然后通过 <spanclass="math inline">\(\sqrt{d_k}\)</span>进行缩放，以防止点积值过大。</li><li>经过 softmax 操作后，获得的权重矩阵用于对值向量 <spanclass="math inline">\(V\)</span> 进行加权求和。</li></ul><p>这个公式的核心思想是通过对每个词的查询向量与所有其他词的键向量进行点积，得到一个权重分布，再利用该权重对值向量进行加权和求和，从而得到每个词的新的表示。</p><h3 id="多头注意力机制multi-head-attention">5.2多头注意力机制（Multi-Head Attention）</h3><p>多头注意力机制是对自注意力的扩展，它通过多个并行的自注意力计算来捕捉输入序列中的不同特征。每个注意力头（head）学习一个不同的投影，从不同的角度来关注输入序列中的各个部分。</p><p>具体操作如下：</p><ol type="1"><li>输入的查询、键和值分别被映射到多个不同的子空间，每个子空间对应一个注意力头。</li><li>每个注意力头独立计算自注意力（如上述公式所示），得到一个加权的表示。</li><li>最后，将所有头的输出拼接在一起，并通过一个线性变换来得到最终的输出。</li></ol><p>多头注意力机制的公式如下： <span class="math display">\[\operatorname{MultiHead}(Q, K,V)=\operatorname{Concat}\left(\operatorname{head}_1, \ldots, \text {head }_h\right) W^O\]</span> 其中，<spanclass="math inline">\(\operatorname{head}_i=\operatorname{Attention}\left(QW_i^Q, K W_i^K, V W_i^V\right)\)</span>，表示第 <spanclass="math inline">\(i\)</span> 个注意力头的计算过程。</p><ul><li><span class="math inline">\(W_i^Q, W_i^K, W_i^V\)</span>是用于映射查询、键、值的权重矩阵。</li><li><span class="math inline">\(W^O\)</span> 是输出映射的权重矩阵。</li></ul><p>通过这种方式，模型能够并行地关注输入序列的不同部分，并从多角度提取信息。</p><h3 id="位置编码positional-encoding">5.3 位置编码（PositionalEncoding）</h3><p>由于 Transformer 没有像 RNN那样的序列顺序处理机制，因此需要引入位置编码来保持序列中每个单词的位置信息。位置编码将每个词的位置映射为一个向量，并加到词嵌入（embedding）中。位置编码的设计保证了每个词的位置信息可以被捕捉到，而不会影响自注意力的计算。</p><p>位置编码使用正弦和余弦函数来生成不同频率的编码，公式如下： <spanclass="math display">\[\begin{gathered}\mathrm{PE}(p o s, 2 i)=\sin \left(\frac{p o s}{10000^{2 i / d_{\text{model }}}}\right) \\\mathrm{PE}(p o s, 2 i+1)=\cos \left(\frac{p o s}{10000^{2 i / d_{\text{model }}}}\right)\end{gathered}\]</span></p><ul><li><span class="math inline">\(pos\)</span> 是单词的位置，<spanclass="math inline">\(i\)</span> 是维度索引，<spanclass="math inline">\(d_{\text {model }}\)</span> 是词嵌入的维度。</li></ul><p>这种编码方式能够确保对于较远的词，正弦和余弦函数的值会呈现不同的周期性，能够捕捉到不同尺度的位置信息。</p><h3 id="前馈神经网络feed-forward-neural-networks">5.4前馈神经网络（Feed-Forward Neural Networks）</h3><p>Transformer中的每个层除了自注意力层之外，还包含一个前馈神经网络（Feed-ForwardNeural Network）。该网络包含两个线性变换和一个 ReLU 激活函数，公式如下：<span class="math display">\[\operatorname{FFN}(x)=\max \left(0, x W_1+b_1\right) W_2+b_2\]</span></p><ul><li><span class="math inline">\(W_1, W_2\)</span> 和 <spanclass="math inline">\(b_1, b_2\)</span> 是可学习的权重和偏置。</li><li><span class="math inline">\(\max (0, x)\)</span> 是 ReLU激活函数。</li></ul><p>前馈网络在每个位置独立地应用，因此它能够进一步增强每个词表示的非线性特征。</p><h3id="残差连接和层归一化residual-connection-and-layer-normalization">5.5残差连接和层归一化（Residual Connection and Layer Normalization）</h3><p>为了避免深层网络中的梯度消失问题，Transformer使用了<strong>残差连接（ResidualConnection）</strong>。每个子层（如注意力层、前馈神经网络层）的输出都会加上其输入，即：<span class="math display">\[\text { output }=\text { LayerNorm }(x+\operatorname{Sublayer}(x))\]</span> 这里，Sublayer(x) 是该子层的输出，<spanclass="math inline">\(x\)</span>是该子层的输入。残差连接有助于确保信息能够直接流过网络的每一层，避免梯度消失问题，同时层归一化（LayerNorm）有助于加速训练。</p><h2 id="evaluation">6. Evaluation</h2><p>作者通过以下几个标准数据集对 Transformer 模型进行了评估：</p><ul><li><strong>WMT 2014 英德翻译和英法翻译任务</strong>。</li><li>与传统的基于 RNN 的模型（如 LSTM）和 CNN 模型进行了比较。</li></ul><p>主要评估指标包括：</p><ul><li><strong>BLEU（Bilingual EvaluationUnderstudy）</strong>分数，这是衡量机器翻译质量的标准指标。</li><li><strong>训练时间</strong>：由于 Transformer的高度并行化，训练时间显著低于基于 RNN 的模型。</li></ul><p>实验结果表明，Transformer 在翻译质量和训练效率上都显著超越了传统的RNN 模型。</p><h2 id="conclusion">7. Conclusion</h2><p>Transformer模型是序列到序列任务中的一项重大创新，证明了自注意力机制可以取代传统的循环和卷积层。本文的主要结论如下：</p><ol type="1"><li><strong>高效性</strong>：Transformer在性能超越的同时，还具备了高度的并行化能力，显著降低了训练时间。</li><li><strong>有效性</strong>：在机器翻译任务中，Transformer 超越了传统RNN 模型，能够有效捕捉长程依赖关系。</li><li><strong>灵活性</strong>：Transformer架构具有很好的通用性，能够应用于其他序列相关任务，之后的 BERT、GPT、T5等模型也基于Transformer 架构发展而来。</li></ol><p>Transformer为自然语言处理和其他领域的深度学习模型奠定了新的基础。</p><h2 id="notes">8. Notes</h2><h3 id="transformer-为什么使用-ln-而不使用-bn">8.1 Transformer为什么使用 LN 而不使用 BN?</h3><blockquote><p>在 Transformer 模型中，使用<strong>层归一化（LayerNormalization，LN）</strong>而非<strong>批量归一化（BatchNormalization，BN）</strong>，主要是因为以下几个原因：</p><ol type="1"><li><p><strong>序列数据的依赖性</strong></p><p>Transformer是处理序列数据的模型，其中每个输入元素可能与其他元素有复杂的依赖关系。<strong>批量归一化（BN）</strong>依赖于批次内的统计量（均值和方差），这在卷积神经网络（CNN）中有效，但对于序列数据存在问题：</p><ul><li><strong>依赖批次数据</strong>：BN计算基于批次的统计量，而每个时间步的输入需要独立处理其上下文信息。不同样本的分布可能会干扰每个时间步的计算。</li><li><strong>小批次问题</strong>：Transformer通常使用小批次处理长序列，BN在小批次下表现较差，因为统计量不稳定，可能影响训练的稳定性。</li></ul></li><li><p><strong>训练稳定性</strong>：层归一化（LN）可以提高训练过程的稳定性，特别是在深度神经网络中。相比之下，批量归一化（BN）受批次大小的影响，尤其是在小批次情况下，均值和方差的计算可能不准确，从而影响训练稳定性。而LN对每个样本独立归一化，避免了这个问题，因此训练过程更加稳定。</p></li><li><p><strong>并行计算的要求</strong>：Transformer的一个显著特点是其<strong>高度并行化</strong>的计算方式，特别是在计算自注意力时。BN在训练时需要在整个批次上计算均值和方差，这通常是针对整个批次的操作。而LN 是对单个样本的操作，因此更加适合在训练时进行并行计算，尤其是在 GPU 或TPU 上训练时，能够更好地加速训练过程。</p></li></ol></blockquote><h3 id="transformer-中前馈神经网络里面的-x-和-w-的-shape-分别是多少">8.2Transformer 中前馈神经网络里面的 <span class="math inline">\(x\)</span>和 <span class="math inline">\(W\)</span> 的 shape 分别是多少？</h3><blockquote><p>在 Transformer 的前馈神经网络（Feed-Forward Network，FFN）中，输入<span class="math inline">\(x\)</span> 和权重矩阵 <spanclass="math inline">\(W\)</span> 的形状如下：</p><ol type="1"><li><strong>输入 <span class="math inline">\(x\)</span>的形状</strong>：</li></ol><p>前馈神经网络的输入是自注意力（Self-Attention）层的输出。假设输入的张量<span class="math inline">\(x\)</span> 的形状为： <spanclass="math display">\[x \in \mathbb{R}^{B \times S \times D}\]</span></p><ul><li><span class="math inline">\(B\)</span> 是批次大小（batch size）</li><li><span class="math inline">\(S\)</span> 是序列长度（sequencelength）</li><li><span class="math inline">\(D\)</span>是每个时间步的特征维度，通常称为模型的隐藏维度（<spanclass="math inline">\(d_{\text {model }}\)</span>）</li></ul><ol start="2" type="1"><li><strong>权重矩阵 <span class="math inline">\(W\)</span>的形状</strong>：</li></ol><p>前馈神经网络通常由两个线性层组成，假设第一个线性层将输入的特征维度<span class="math inline">\(D\)</span> 映射到更高的维度 <spanclass="math inline">\(d_{\mathrm{ff}}\)</span>，然后第二个线性层将其映射回<span class="math inline">\(D\)</span>。</p><ul><li><p>第一个线性层的权重矩阵 <span class="math inline">\(W_1\)</span>的形状为： <span class="math display">\[W_1 \in \mathbb{R}^{D \times d_{\mathrm{ff}}}\]</span> 这里，<span class="math inline">\(D\)</span>是输入特征维度，<span class="math inline">\(d_{\mathrm{ff}}\)</span>是前馈网络的隐藏层维度。</p></li><li><p>第二个线性层的权重矩阵 <span class="math inline">\(W_2\)</span>的形状为： <span class="math display">\[W_2 \in \mathbb{R}^{d_{\mathrm{ff}} \times D}\]</span> 这里，<span class="math inline">\(d_{\mathrm{ff}}\)</span>是前馈网络的隐藏层维度，<span class="math inline">\(D\)</span>是输出的特征维度，通常与输入的特征维度 <spanclass="math inline">\(D\)</span> 相同。</p></li></ul><p><strong>在整个计算过程中，前馈神经网络对每个时间步的特征进行独立处理，输入维度<span class="math inline">\(D\)</span> 被映射到更高的维度 <spanclass="math inline">\(d_{\mathrm{ff}}\)</span>，然后再映射回原始维度<span class="math inline">\(D\)</span></strong>。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Transformer</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</title>
    <link href="/2024/11/27/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-ShuffleNet-V2-Practical-Guidelines-for-Efficient-CNN-Architecture-Design/"/>
    <url>/2024/11/27/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-ShuffleNet-V2-Practical-Guidelines-for-Efficient-CNN-Architecture-Design/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: ShuffleNet V2: Practical Guidelines forEfficient CNN Architecture Design<br /><strong>Link</strong>: <ahref="https://openaccess.thecvf.com/content_ECCV_2018/papers/Ningning_Light-weight_CNN_Architecture_ECCV_2018_paper.pdf">ShuffleNetV2 Paper</a><br /><strong>Source</strong>: European Conference on Computer Vision(ECCV)<br /><strong>Date</strong>: 2018.07.30</p><h2 id="summary">2. Summary</h2><p>本文提出了 ShuffleNetV2，一种新的轻量级卷积神经网络架构，旨在提高计算效率，特别适用于计算能力受限的移动设备。研究指出，直接度量（如速度）不仅取决于FLOPs（乘加数），还受到存储访问量和硬件平台特性等因素的影响。因此，本文基于直接度量评估目标平台上的性能，并提出了几个实用指南来指导高效的网络设计。</p><h2 id="background">3. Background</h2><p>卷积神经网络（CNN）在计算机视觉任务中取得了显著的成功，但其庞大的模型体积和高计算需求，使其在移动设备和嵌入式系统等资源受限的设备上应用时面临挑战。之前的轻量级CNN 模型，如 MobileNet 和ShuffleNet，尝试通过减少参数和计算量来解决这一问题。然而，这些模型在速度和资源使用上的效率仍然存在不足，尤其是在移动和嵌入式设备应用场景中。</p><h2 id="research-objective">4. Research Objective</h2><p>本研究的目标是提供一套设计高效 CNN架构的实用指南，尤其是在移动设备和嵌入式系统应用中。作者旨在通过分析模型的准确度、计算复杂度和资源约束之间的关系，提出ShuffleNet V2 作为一种更高效的轻量级替代方案。</p><h2 id="method">5. Method</h2><p>在设计轻量化网络时不应简单的只考虑间接指标（FLOPs），需要满足以下的设计原则：</p><ol type="1"><li><strong>相同通道宽度能够最小化内存访问成本（MAC）</strong>：当卷积层的输入和输出通道数相同时，所需的内存访问成本（MAC）最小。这是因为在这种情况下，卷积操作可以更高效地利用缓存，减少内存访问次数。</li><li><strong>过度的组卷积会增加MAC</strong>：使用大的分组数进行卷积运算会增加内存访问成本，因为不同组之间的数据无法共享，导致更多的内存访问。</li><li><strong>网络碎片化会降低并行程度</strong>：网络中过多的分支和基本单元会导致计算资源无法充分利用，降低并行计算的效率。</li><li><strong>Element-wise 操作的影响不可忽略</strong>：Element-wise操作（如 ReLU激活函数）虽然计算量不大，但在网络中广泛存在，对整体性能有不可忽视的影响。</li></ol><p>根据上述设计原则，ShuffleNet V2 对 ShuffleNet V1进行了若干优化，提升了模型的计算效率和内存访问效率。以下是对 V2网络架构的改进：</p><p align="center"><img src="1.png" style="zoom:67%;" /></p><p>在 ShuffleNet V1 中，网络结构基于通道混洗和深度可分卷积（depthwiseseparable convolution）两大核心技术进行设计。然而，ShuffleNet V1中的某些设计存在计算效率上的瓶颈，特别是在内存访问和网络碎片化方面。</p><h5 id="shufflenet-v2-的改进"><strong>ShuffleNet V2 的改进</strong></h5><ol type="1"><li><strong>通道划分与跳跃链接</strong> 在 ShuffleNet V2中，首先对特征图进行通道划分，一部分通道通过跳跃链接直接传递到下一层，另一部分通道经过卷积处理。在卷积操作中，V2去除了 1x1的分组卷积操作，因为在通道划分阶段，已经对通道进行了分组，从而不再需要额外的分组卷积。这一优化不仅降低了计算量，还减小了内存访问成本。</li><li><strong>替换 Add 操作为 Concat 操作</strong> ShuffleNet V2中将原有的 Add 操作替换成了 Concat操作。在V1中，Add操作通常会导致不同分支的特征图相加，可能带来一些计算上的冗余。而通过Concat操作，将特征图在通道维度上拼接，不仅能够保留更多的信息，还能更好地进行后续的通道混洗操作。</li><li><strong>通道混洗的优化</strong> 在 V2中，进行了优化的通道混洗操作进一步提升了特征图之间的信息交换效率。通道混洗确保了不同通道之间的信息能够流畅传递，而不至于造成信息瓶颈。通过减少计算冗余和提升信息流动效率，通道混洗有效提升了模型的表达能力，同时保持了较低的计算成本。</li><li><strong>下采样操作优化</strong> 对于需要进行下采样的部分，ShuffleNetV2 去除了原始 ShuffleNet V1中的通道分离步骤，而是通过加倍输出通道数来进行下采样。这一变化使得网络结构更加简洁，同时减少了冗余的操作步骤。通过优化下采样策略，V2能够在不牺牲计算效率的情况下，更加平滑地进行空间尺寸的压缩。</li></ol><h2 id="conclusion">6. Conclusion</h2><p>ShuffleNetV2提出了一种实用且高效的CNN架构，通过优化原ShuffleNet的操作，针对移动设备和嵌入式系统的应用需求，提供了一种更轻量、高效的方案。本文提出的设计原则为未来轻量级CNN的研究提供了重要参考，能够在减少计算量的同时保持较高的准确度。与现有的轻量级模型如MobileNetV2相比，ShuffleNetV2在多个方面表现出色，具有较大的应用潜力。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>算法优化</tag>
      
      <tag>轻量化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - MobileNets: Efficient Convolutional Neural Networks for Mobile </title>
    <link href="/2024/11/27/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile/"/>
    <url>/2024/11/27/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: MobileNets: Efficient Convolutional NeuralNetworks for Mobile Vision Applications<br /><strong>Link</strong>: <ahref="https://arxiv.org/pdf/1704.04861">MobileNets: Efficient CNNs forMobile Applications</a><br /><strong>Source</strong>: IEEE Conference on Computer Vision and PatternRecognition (CVPR)<br /><strong>Date</strong>: 2017.04.17</p><h2 id="summary">2. Summary</h2><p>MobileNets提出了一种轻量化、高效的卷积神经网络架构，专为移动设备和嵌入式视觉应用设计。论文的核心贡献在于引入<strong>深度可分离卷积（DepthwiseSeparableConvolutions）</strong>，大幅减少模型的计算量和参数量，同时保证精度的相对稳定。此外，作者还设计了两个超参数（宽度乘子和分辨率乘子），以实现模型精度与效率之间的灵活权衡，满足不同设备的资源限制需求。</p><h2 id="background">3. Background</h2><p>传统的卷积神经网络（如 AlexNet 或VGG）计算量庞大，对内存和处理能力要求较高，不适用于移动设备。而随着移动设备对实时图像处理和智能应用需求的增加，设计高效且准确的深度学习模型成为一个关键研究方向。MobileNets的研究背景正是针对这一需求，提出了一种在资源受限环境下优化卷积层的方法。</p><h2 id="research-objective">4. Research Objective</h2><p>本研究的目标包括：</p><ol type="1"><li>设计一种轻量化、高效的卷积神经网络架构，适用于移动和嵌入式视觉任务。</li><li>提供可调超参数以平衡模型的精度和计算效率。</li><li>验证 MobileNets在图像分类、目标检测和语义分割等任务中的性能表现。</li></ol><h2 id="method">5. Method</h2><p>核心方法为<strong>深度可分离卷积</strong>，将标准卷积分解为以下两个步骤：</p><ol type="1"><li><strong>深度卷积（DepthwiseConvolution）</strong>：对每个输入通道单独应用一个卷积核。</li><li><strong>逐点卷积（Pointwise Convolution）</strong>：通过 1x1卷积将深度卷积的输出通道进行线性组合。 此分解方法将计算复杂度降低约 8-9倍。</li></ol><p>此外，作者引入了两个超参数以进一步优化网络：</p><ul><li><strong>宽度乘子（α）</strong>：缩小每层卷积核的数量。</li><li><strong>分辨率乘子（ρ）</strong>：缩小输入图像的分辨率。</li></ul><p>通过调节 α 和 ρ，MobileNets可以灵活适应不同的硬件资源和任务需求。</p><h2 id="conclusion">6. Conclusion</h2><p>MobileNets提供了一种适用于移动和嵌入式设备的高效深度学习模型解决方案。通过引入深度可分离卷积和可调超参数，架构显著减少了计算量和内存占用，同时保持了良好的精度表现。这项工作为资源受限环境下的实时视觉任务提供了广泛的应用可能性。</p><h2 id="notes">7. Notes</h2><h3 id="宽度乘子是如何实现对模型的修改的">7.1宽度乘子是如何实现对模型的修改的？</h3><blockquote><p><strong>宽度乘子（WidthMultiplier）作用于模型的所有卷积层</strong>，它通过全局性地缩减网络中每一层的通道数，达到减少模型参数量和计算量的目的。具体来说，宽度乘子会调整每一层的输入通道数和输出通道数，使得整个网络的计算复杂度和内存占用显著降低。</p></blockquote><h3 id="为什么需要全局作用于所有层">7.2为什么需要全局作用于所有层？</h3><blockquote><ul><li><strong>保持网络结构的一致性：</strong>如果宽度乘子仅作用于部分层，网络其他层的输入/输出通道数无法对应，可能会导致维度不匹配的错误。</li><li><strong>降低整体计算成本：</strong>卷积操作的计算复杂度与通道数呈线性关系。全局性缩减通道数，可以显著减少FLOPs（浮点运算次数）和参数量。</li><li><strong>平衡各层贡献：</strong>如果只对部分层缩减通道数，可能导致某些层的权重显著减少，而其他层的计算量仍然很高，全局性缩减可以避免这种不平衡。</li></ul></blockquote>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>算法优化</tag>
      
      <tag>轻量化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</title>
    <link href="/2024/11/26/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-ShuffleNet-An-Extremely-Efficient-Convolutional-Neural-Network-for-Mobile-Devices/"/>
    <url>/2024/11/26/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-ShuffleNet-An-Extremely-Efficient-Convolutional-Neural-Network-for-Mobile-Devices/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: ShuffleNet: An Extremely EfficientConvolutional Neural Network for Mobile Devices<br /><strong>Link</strong>: <ahref="https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf">ShuffleNetPaper</a><br /><strong>Source</strong>: IEEE Conference on Computer Vision and PatternRecognition (CVPR)<br /><strong>Date</strong>: 2017.07.04</p><h2 id="summary">2. Summary</h2><p>本文提出了ShuffleNet，一种针对移动设备的高效卷积神经网络。核心创新是<strong>通道混洗（channelshuffle）</strong>操作，它在不牺牲准确性的情况下，减少了计算量和模型大小。文章提出了两项关键技术：</p><ol type="1"><li><strong>逐点组卷积（pointwise group convolution）</strong>：通过将1x1 卷积操作分组，减少了参数和计算量。</li><li><strong>通道混洗（channelshuffle）</strong>：这一操作在组卷积后重排特征通道，提升了网络的表示能力。</li></ol><h2 id="background">3. Background</h2><p>由于移动设备面临资源受限的挑战，如何设计高效的神经网络模型变得尤为重要。传统的CNN 架构计算开销大，限制了它们在移动平台上的应用。</p><h2 id="research-objective">4. Research Objective</h2><p>本研究的主要目标是设计一种计算高效且能够保持高准确率的CNN架构，特别是针对移动设备的部署。具体来说，研究目标是：</p><ol type="1"><li>在保证高准确度的同时，减少模型的计算开销（FLOPs）。</li><li>确保模型在内存使用和模型大小方面高效，以便在实际移动设备中使用。</li></ol><h2 id="method">5. Method</h2><h4 id="channel-shuffle-for-group-convolutions">Channel Shuffle forGroup Convolutions</h4><p>现有的模型在使用组卷积时，往往忽视了 1×1卷积（逐点卷积），导致该操作占据了大量计算开销。本文提出了在 1×1卷积中使用分组卷积，以减少计算量。</p><p>然而，分组卷积会使得每个通道的输出仅来自部分输入通道。为了解决这一问题，作者提出了通道混洗技术，如下图（b）和（c）所示。假设某个卷积层被划分为g 组，每组的输出特征维度为 n，首先将其堆叠成形状为（g,n）的张量，再对其进行转置操作，最后将其拉平，完成通道之间的信息融合。</p><p align="center"><img src="1.png" style="zoom:67%;" /></p><h4 id="shufflenet-unit">ShuffleNet Unit</h4><p>基于通道混洗操作，本文提出了 ShuffleNet 单元，如下图所示。</p><p align="center"><img src="2.png" style="zoom:67%;" /></p><p>其中，(a) 为 ResNet 中的基本块，本文将其中的 1×1卷积替换为分组卷积，并在第一个分组卷积之后加入了通道混洗操作。</p><p>在需要缩减图像尺寸的情况下，如图(c)所示，在直接连接路径上使用 3×3的平均池化操作，并将原本在 ResNet中的相加操作替换为通道串联操作，从而增加通道维度。</p><h2 id="evaluation">6. Evaluation</h2><p>作者通过以下方式评估了ShuffleNet的性能：</p><ul><li>在 ImageNet 数据集上的<strong>Top-1和Top-5准确率</strong>。</li><li><strong>FLOPs</strong>（浮点运算）用于衡量计算效率。</li><li><strong>模型大小</strong>和<strong>内存使用</strong>，评估其在移动设备上的可行性。</li></ul><p>ShuffleNet 在准确率和效率上超越了其他高效 CNN 架构（如MobileNet），实现了性能与计算开销的良好平衡。</p><h2 id="conclusion">7. Conclusion</h2><p>ShuffleNet通过创新地结合组卷积和通道混洗操作，设计出了极高效的卷积神经网络，特别适合移动设备使用。实验结果表明，ShuffleNet在计算开销大幅降低的同时，依然能够保持良好的准确率。该方法在实时移动应用中具有很大的潜力，尤其是在速度和内存效率至关重要的场景下。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>算法优化</tag>
      
      <tag>轻量化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Densely Connected Convolutional Networks</title>
    <link href="/2024/11/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Densely-Connected-Convolutional-Networks/"/>
    <url>/2024/11/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Densely-Connected-Convolutional-Networks/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: Densely Connected ConvolutionalNetworks<br /><strong>Link</strong>: <ahref="https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf">DenseNetPaper</a><br /><strong>Source</strong>: IEEE Conference on Computer Vision and PatternRecognition (CVPR)<br /><strong>Date</strong>: 2016.08.25</p><h2 id="summary">2. Summary</h2><p>本文提出了DenseNet，一种新的深度学习架构，在该架构中，每一层都接收来自所有前面层的输入，形成密集的连接。与传统的卷积神经网络只传递前一层的信息不同，DenseNet为每一层创建了来自所有前一层的直接连接。该设计显著改善了梯度流动并促进了特征重用，从而实现了更高效的网络，并且参数更少。</p><h2 id="background">3. Background</h2><p>DenseNet 基于传统 CNN架构，解决了梯度消失和参数冗余等关键问题。密集连接的概念受到残差网络（ResNet）成功的启发，但DenseNet通过完全连接每一层的方式进行了更为激进的改进。这项研究在深度学习领域具有重要意义，特别是在提高图像分类、物体检测等任务的网络效率和性能方面。</p><h2 id="research-objective">4. Research Objective</h2><p>本研究的主要目标是提出并验证 DenseNet作为一种更高效的深度学习架构，旨在缓解传统 CNN中梯度消失和特征冗余的问题。研究旨在展示 DenseNet在准确性和参数效率方面相较于传统架构的优势。</p><h2 id="method">5. Method</h2><p>DenseNet架构由多个层块组成，层块内的每一层都与之前的所有层进行连接，如下图所示：</p><p align="center"><img src="1.png" style="zoom:80%;" /></p><p>在每个层块内，特征图的大小保持不变，而层块之间通过过渡层实现特征图的下采样。</p><p>DenseNet的显著特点之一是每一层的输出特征图维度非常窄，输出通道数被称为增长率<span class="math inline">\(k\)</span>。虽然每层的输出维度固定为 <spanclass="math inline">\(k\)</span>，但输入特征图的维度随着网络深度的增加而逐步增多。为提升计算效率，DenseNet在每个 3×3 卷积之前引入 1×1 卷积作为瓶颈层，以减少输入特征图的数量。</p><p>此外，通过在过渡层引入压缩因子 <spanclass="math inline">\(\theta\)</span>，进一步提升了模型的紧凑性，使DenseNet 更高效地利用计算资源。</p><h2 id="evaluation">6. Evaluation</h2><p>作者使用标准数据集（如 CIFAR-10、CIFAR-100 和 ImageNet）对 DenseNet进行了评估，结果表明 DenseNet 在准确性和参数数量方面都显著优于传统CNN，显示出更强的泛化能力。通过广泛的消融研究，分析了密集连接对性能的影响。</p><h2 id="conclusion">7. Conclusion</h2><p>DenseNet通过引入密集层连接，提供了一种有效的解决方案，提升了深度神经网络的效率。这一创新使得网络训练更快、准确性更高，同时参数更少。研究表明，DenseNet可以成为处理大数据集和深度学习任务中的一个有价值的架构。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>训练加速</tag>
      
      <tag>算法优化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Aggregated Residual Transformations for Deep Neural Networks</title>
    <link href="/2024/11/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Aggregated-Residual-Transformations-for-Deep-Neural-Networks/"/>
    <url>/2024/11/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Aggregated-Residual-Transformations-for-Deep-Neural-Networks/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: Aggregated Residual Transformations for DeepNeural Networks<br /><strong>Link</strong>: <ahref="https://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf">ResNeXtPaper</a><br /><strong>Source</strong>: IEEE Conference on Computer Vision and PatternRecognition (CVPR)<br /><strong>Date</strong>: 2016.11.16</p><h2 id="summary">2. Summary</h2><p>本文提出了<strong>ResNeXt</strong>，一种改进的残差网络架构，通过引入<strong>聚合变换（AggregatedTransformations）</strong>增强模型表达能力。核心创新是引入了<strong>基数（Cardinality）</strong>的概念，即网络中路径的数量。ResNeXt在显著提升性能的同时，保持了较低的计算复杂度，并在 ImageNet、CIFAR-10 和CIFAR-100 等基准数据集上达到了最先进的性能。</p><h2 id="background">3. Background</h2><p>深度神经网络（如ResNet）通过堆叠多个层及跳跃连接在视觉任务中取得了显著成功。然而，仅增加网络的深度或宽度带来的性能提升逐渐减小，同时计算成本迅速增加。受Inception 等多分支架构的启发，ResNeXt提出了一个更简单且更高效的聚合策略，通过<strong>并行路径的聚合变换</strong>在性能与效率之间取得了平衡。</p><h2 id="research-objective">4. Research Objective</h2><p>本文的研究目标是通过引入<strong>基数（Cardinality）</strong>提升深度网络的表达能力，同时保持计算效率。具体目标包括：</p><ol type="1"><li>提供一种更简洁的多分支架构设计方法；</li><li>验证 ResNeXt 的可扩展性及其在不同任务中的通用性；</li><li>在不显著增加计算成本的情况下实现最先进的性能。</li></ol><h2 id="method">5. Method</h2><ul><li><p><strong>关键创新</strong>：</p><p>引入了基数的概念，下图左边为原始的 ResNet架构；右边为增加了基数的改进版本，每个残差模块包含多条路径，路径数由基数控制，所有路径的输出聚合后再与输入进行残差连接。</p><p align="center"><p><img src="1.png" style="zoom:60%;" /></p></p><p>引入<strong>分组卷积（GroupedConvolution）</strong>以实现多路径聚合变换，下图 (a) 和 (b) 表示相同，图(c) 使用了分组卷积，降低了计算复杂度。</p><ul><li><p>分组卷积将输入通道划分为多个组，每组独立进行卷积操作，降低了计算复杂度。</p></li><li><p>各组的输出通过聚合操作合并，实现模块化和可扩展性。</p></li></ul><p align="center"><p><img src="2.png" style="zoom:60%;" /></p></p></li><li><p><strong>架构设计</strong>：</p><ul><li>ResNeXt 的残差模块在 ResNet的基础上，将瓶颈层的单一卷积替换为分组卷积。</li><li>基数（Cardinality）表示分组的数量，是控制并行路径数量的超参数。</li></ul></li><li><p><strong>与其他架构的比较</strong>：</p><ul><li>ResNeXt在深度（ResNet）和宽度（VGG）之外，提出了新的扩展维度——基数，通过增加基数实现性能提升，同时保持计算效率。</li></ul></li></ul><h2 id="evaluation">6. Evaluation</h2><ul><li><p><strong>数据集</strong>：ImageNet、CIFAR-10 和CIFAR-100。</p></li><li><p><strong>评估指标</strong>：分类任务的 Top-1 和 Top-5准确率。</p></li><li><p><strong>实验结果</strong>：</p><ul><li><p>在相同的深度和宽度下，ResNeXt 比 ResNet表现更优，准确率更高。</p></li><li><p>实验证明，增加基数相比单纯增加深度或宽度更能显著提升网络性能。</p></li></ul></li><li><p><strong>消融实验</strong>：验证了分组卷积的有效性，以及基数对模型性能的影响。</p></li></ul><h2 id="conclusion">7. Conclusion</h2><p>ResNeXt提供了一种通过聚合变换改进神经网络性能的简单方法。通过增加基数，ResNeXt实现了更高的准确率，同时保持了较低的计算成本。研究表明，基数是扩展深度网络性能的重要维度，提供了增加深度或宽度之外的一种灵活替代方案。</p><h2 id="notes">8. Notes</h2><h3 id="什么是分组卷积">8.1 <strong>什么是分组卷积</strong>？</h3><blockquote><p>分组卷积的核心思想是将卷积操作的输入通道和输出通道分组，然后在每组上独立执行卷积操作，最后将各组的输出拼接在一起。</p><ul><li><strong>传统卷积</strong>：<ul><li>输入特征图和卷积核的所有通道之间会进行完全连接的卷积操作。</li><li>假设输入的通道数为 <span class="math inline">\(C_{\text {in}}\)</span>，输出通道数为 <span class="math inline">\(C_{\text {out}}\)</span>，卷积核大小为 <span class="math inline">\(k \timesk\)</span>，传统卷积需要的参数量为：</li></ul></li></ul><p><span class="math display">\[C_{\text {in }} \times C_{\text {out }} \times k \times k\]</span></p><ul><li><strong>分组卷积</strong>：<ul><li>将输入通道划分为 <span class="math inline">\(g\)</span> 组（每组有<span class="math inline">\(C_{\text {in }}\)</span> / <spanclass="math inline">\(g\)</span> 个通道），输出通道也划分为 <spanclass="math inline">\(g\)</span> 组（每组有 <spanclass="math inline">\(C_{\text {out }}\)</span> / <spanclass="math inline">\(g\)</span> 个通道）。</li><li>每组卷积仅计算输入通道的一部分，从而减少了计算量。</li><li>参数量为：</li></ul></li></ul><p><span class="math display">\[\left(C_{\text {in }} / g\right) \times\left(C_{\text {out }} / g\right)\times k \times k \times g=\frac{C_{\text {in }} \times C_{\text {out }}\times k \times k}{g}\]</span></p></blockquote><h3 id="resnext-与-inception-resnet-的对比">8.2 <strong>ResNeXt 与Inception-ResNet 的对比</strong></h3><blockquote><table><thead><tr class="header"><th><strong>特性</strong></th><th><strong>ResNeXt</strong></th><th><strong>Inception-ResNet</strong></th></tr></thead><tbody><tr class="odd"><td><strong>路径结构</strong></td><td>等价路径（分组卷积）</td><td>非等价路径（不同卷积核和池化操作）</td></tr><tr class="even"><td><strong>模块复杂度</strong></td><td>简单、模块化</td><td>复杂、需手动调优</td></tr><tr class="odd"><td><strong>计算复杂度</strong></td><td>更低，参数更少</td><td>较高，参数较多</td></tr><tr class="even"><td><strong>特征表达能力</strong></td><td>高效，通过增加基数捕获更多特征</td><td>优秀，能捕获多尺度特征</td></tr><tr class="odd"><td><strong>扩展性和通用性</strong></td><td>高，易于在深层网络中扩展</td><td>中等，适合特定任务（如多尺度特征处理）</td></tr><tr class="even"><td><strong>适用场景</strong></td><td>大规模训练、深层分类网络</td><td>需要多尺度特征融合的任务（如检测、分割）</td></tr></tbody></table></blockquote>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>训练加速</tag>
      
      <tag>算法优化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Rethinking the Inception Architecture for Computer Vision</title>
    <link href="/2024/11/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Rethinking-the-Inception-Architecture-for-Computer-Vision/"/>
    <url>/2024/11/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Rethinking-the-Inception-Architecture-for-Computer-Vision/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: Rethinking the Inception Architecture forComputer Vision<br /><strong>Link</strong>: <ahref="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf">InceptionV3 Paper</a><br /><strong>Source</strong>: IEEE Conference on Computer Vision and PatternRecognition (CVPR)<br /><strong>Date</strong>: 2015.12.02</p><h2 id="summary">2. Summary</h2><p>本文重新审视了 Inception V1架构并提出了一系列改进，提升了其效率和性能，从而推出了 Inception V3模型。这些改进包括优化计算成本、减少参数数量，以及在保持高效表示的同时增加网络的深度和宽度。在ImageNet 数据集上，这些模型在减少计算预算的同时实现了最先进的性能。</p><h2 id="background">3. Background</h2><p>Inception网络是一类在每层内通过多尺度特征提取实现计算效率与性能平衡的架构。最初的GoogLeNet（Inception V1）引入了模块化设计，但也带来了参数数量增多和计算开销增加的问题。本研究在Inception V1的成功基础上，提出了解决这些局限性的方法，并进一步提升了视觉任务中的准确率和效率。</p><h2 id="research-objective">4. Research Objective</h2><ol type="1"><li>减少网络计算成本和参数数量。</li><li>平衡网络深度与宽度以实现最优性能。</li><li>探索新的技术，例如卷积因式分解和辅助损失层。</li></ol><h2 id="method">5. Method</h2><h3 id="卷积因式分解"><strong>卷积因式分解</strong></h3><p>用多个较小的卷积（如 3×3 或 1×1 ）替代较大的卷积（如 5×5），以降低计算成本。</p><p align="center"><img src="1.png" style="zoom:80%;" /></p><p>将较大的卷积核（5×5）分解为两个小卷积核（如 1×5 和 5×1），降低参数量和计算复杂度。</p><p align="center"><img src="2.png" style="zoom:80%;" /></p><h3 id="高效网格尺寸缩减"><strong>高效网格尺寸缩减</strong></h3><p>如图 9所示，左侧方法虽然降低了特征图尺寸，但容易引发表征瓶颈；右侧方法虽能缓解表征瓶颈，却带来了较高的计算开销。而图10 中的方法在避免表征瓶颈的基础上，成功实现了计算开销的显著降低。</p><p align="center"><img src="3.png" style="zoom:80%;" /></p><h3 id="辅助分类器"><strong>辅助分类器</strong></h3><p>早期的 GoogLeNet 使用辅助分类器主要是为了缓解梯度消失问题。但Inception V3中研究了它的<strong>正则化效果</strong>，发现即使在没有梯度消失问题的情况下，辅助分类器仍然能提高网络的泛化能力。</p><h3 id="label-smoothing技术"><strong>Label Smoothing技术</strong></h3><p>在传统的分类任务中，目标函数通常使用交叉熵损失（Cross-EntropyLoss），其中真实类别的标签被表示为 <strong>one-hot编码</strong>（即目标类别为 1，其他类别为0）。这种方式在训练时可能导致模型对目标类别的预测概率非常接近1，而对其他类别接近 0，从而导致过拟合问题。Label Smoothing是一种通过调整目标分布的方法，避免模型过度自信。它将原始的 one-hot分布替换为一个“平滑”的分布，即目标类别的标签值被减小，而非目标类别的标签值被稍微增加：<span class="math display">\[q^{\prime}(k)= \begin{cases}1-\epsilon+\frac{\epsilon}{K}, &amp; \text {if } k=y \\ \frac{\epsilon}{K}, &amp; \text { if } k \neq y\end{cases}\]</span></p><ul><li><spanclass="math inline">\(q^{\prime}(k)\)</span>：经过平滑后的标签分布。</li><li><spanclass="math inline">\(\epsilon\)</span>：平滑因子，通常是一个较小的正值，例如0.1。</li><li><span class="math inline">\(K\)</span>：类别总数。</li><li><span class="math inline">\(y\)</span>：真实类别索引。</li></ul><p>这样，目标分布会从严格的 one-hot 变成一个软分布。</p><h2 id="evaluation">6. Evaluation</h2><p>模型在<strong>ImageNet数据集</strong>上进行评估，这是大规模图像分类的基准。</p><ul><li><strong>评估指标</strong>：Top-1 和 Top-5 分类准确率。</li><li><strong>结果</strong>：Inception V3在与前代架构相比时显著提升了准确率，同时降低了计算成本。</li><li><strong>实验验证</strong>：提出的技术（如卷积因式分解和批量归一化）对性能提升起到了关键作用。</li></ul><h2 id="conclusion">7. Conclusion</h2><p>本文成功提出了一种改进的 Inception架构，在计算效率和性能之间实现了平衡。因式分解卷积和改进的训练策略等创新，为设计高效的深度学习架构设立了新的标准。这些发现对构建可扩展、高效的深度学习模型具有广泛意义。</p><h2 id="notes">8. Notes</h2><h3 id="inception-v2-模型命名">8.1 Inception V2 模型命名</h3><p>虽然 Inception V2 和 Inception V3 都是在同一篇论文 "Rethinking theInception Architecture for Computer Vision" 中提及，但出于对 BatchNormalization 的强调，很多人习惯性地将 <strong>BN-Inception</strong>称为 Inception V2。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>算法优化</tag>
      
      <tag>CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Deep Residual Learning for Image Recognition</title>
    <link href="/2024/11/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deep-Residual-Learning-for-Image-Recognition/"/>
    <url>/2024/11/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deep-Residual-Learning-for-Image-Recognition/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: Deep Residual Learning for ImageRecognition<br /><strong>Link</strong>: <ahref="https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">ResNetPaper</a><br /><strong>Source</strong>: IEEE Conference on Computer Vision and PatternRecognition (CVPR)<br /><strong>Date</strong>: 2015.10.10</p><h2 id="summary">2. Summary</h2><p>本文提出了一种新型的深度学习架构——残差网络（ResNet），它通过引入<strong>残差连接</strong>来解决传统深度神经网络在训练过程中遇到的梯度消失和退化问题。作者通过实验证明，残差网络在多个图像识别任务中，特别是在ImageNet图像分类任务上，超越了现有的深度网络架构，达到了更好的效果。该方法可以加速深层网络的训练，并显著提高模型的性能。</p><h2 id="background">3. Background</h2><p>随着深度学习技术的发展，深度神经网络的应用逐渐取得了许多成功。然而，当网络层数增多时，模型的训练难度也随之增加，通常会遇到梯度消失、过拟合、训练退化等问题。传统的做法是通过增加层数来提升模型的表现，但实际效果往往没有预期那么好，这也成为深度学习研究中的一个瓶颈。因此，如何构建更深且更易训练的网络结构成为研究的热点。</p><h2 id="research-objective">4. Research Objective</h2><p>本研究的主要目标是通过引入<strong>残差学习</strong>来改进非常深的网络训练，使其能够更好地进行图像识别任务。</p><h2 id="method">5. Method</h2><ul><li><strong>残差学习</strong>：ResNet不是直接学习输入和输出之间的映射，而是学习输入与输出之间的残差，即 <spanclass="math inline">\(\mathcal{F}(x)=H(x)-x\)</span>，其中 <spanclass="math inline">\(H(x)\)</span> 是期望的映射。</li></ul><p align="center"><img src="1.png" style="zoom:67%;" /></p><ul><li><p><strong>网络模块</strong>：该架构由<strong>残差块（ResidualBlock）</strong>组成，每个残差块中包含一个快捷连接，该连接绕过一个或多个层。</p></li><li><p><strong>深层网络</strong>：通过残差学习，ResNet能够构建任意深度的网络（如152 层），同时避免性能退化。</p></li><li><p><strong>优化</strong>：快捷连接确保了在反向传播中梯度能够顺利流动，从而更容易训练非常深的网络。</p></li></ul><h2 id="evaluation">6. Evaluation</h2><ul><li>作者在<strong>ImageNet 2012 分类挑战</strong>中评估了 ResNet架构，并取得了最先进的结果。ResNet-152 模型的 top-5 错误率为3.0%，远超之前的模型。</li><li>作者还在 COCO 检测和分割任务中验证了该方法，表现也十分优秀。</li><li>他们将 ResNet 与传统的深度 CNN模型以及其他网络进行了比较，证明了更深的 ResNet在准确率上始终优于浅层网络。</li></ul><h2 id="conclusion">7. Conclusion</h2><ul><li><strong>残差网络（ResNets）</strong>使得构建非常深的网络成为可能，并且能够避免训练中的性能退化问题，解决了深层神经网络的梯度消失和优化问题。</li><li><strong>残差学习框架</strong>既简单又有效，可以广泛应用于各种任务，除了图像分类，还包括目标检测和图像分割等。</li><li>残差网络的成功表明，更深的网络结构并非一定会导致性能下降，只要能够有效地传递梯度。</li></ul>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>训练加速</tag>
      
      <tag>算法优化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Batch Normalization: Accelerating Deep Network Training by  Reducing Internal Covariate Shift</title>
    <link href="/2024/11/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/"/>
    <url>/2024/11/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: Batch Normalization: Accelerating DeepNetwork Training by Reducing Internal Covariate Shift<br /><strong>Link</strong>: <ahref="https://asvk.cs.msu.ru/~sveta/%D1%80%D0%B5%D1%84%D0%B5%D1%80%D0%B0%D1%82/batch_normalization.pdf">BatchNormalization Paper</a><br /><strong>Source</strong>: Neural Information Processing Systems(NeurIPS)<br /><strong>Date</strong>: 2015.02.11</p><h2 id="summary">2. Summary</h2><p>本文提出了批量归一化（BatchNormalization，BN）技术，通过减少深度神经网络中的内部协变量偏移（internalcovariateshift），加速网络训练。内部协变量偏移是指在训练过程中，层输入的分布发生变化。BN的核心创新是对每一层的输入进行归一化，使其均值为 0，方差为1，然后进行一个学习的线性变换。BN加速了收敛速度，允许使用更高的学习率，并且缓解了梯度消失和梯度爆炸的问题。BN还具有一定的正则化效果，减少了对 Dropout 的需求。</p><h2 id="background">3. Background</h2><p>深度神经网络在训练时常常面临内部协变量偏移问题，即随着参数的更新，层输入的分布发生变化。这种不稳定性会导致优化过程效率低下，需要精心设计的初始化和学习率调优。之前的解决方法，如预训练和权重初始化，间接解决了这一问题，而BN 通过直接归一化层输入来解决根本问题。</p><h2 id="research-objective">4. Research Objective</h2><ul><li>提出批量归一化方法来减少内部协变量偏移。</li><li>证明BN能够加速训练并提升模型性能。</li><li>评估BN与常用优化方法（如 SGD）兼容性。</li><li>探讨BN的正则化效果及对超参数调节的影响。</li></ul><h2 id="method">5. Method</h2><ul><li><p><strong>归一化</strong>： 对每个 mini-batch 中的激活值 <spanclass="math inline">\(x\)</span>，进行归一化： <spanclass="math display">\[\hat{x}=\frac{x-\mu_B}{\sqrt{\sigma_B^2+\epsilon}}\]</span> 其中，<span class="math inline">\(\mu_B\)</span> 和 <spanclass="math inline">\(\sigma_B^2\)</span> 分别是 mini-batch的均值和方差，<span class="math inline">\(\epsilon\)</span>用于防止除零错误。</p></li><li><p><strong>仿射变换</strong>： 归一化之后，使用可学习的参数 <spanclass="math inline">\(\gamma\)</span> （缩放）和 <spanclass="math inline">\(\beta\)</span>（平移）对激活值进行缩放和偏移：</p></li></ul><p><span class="math display">\[y=\gamma \hat{x}+\beta\]</span></p><ul><li><p><strong>训练阶段</strong>： 在训练过程中，使用 mini-batch的统计量进行归一化。同时，利用移动平均对均值 <spanclass="math inline">\(\mu_B\)</span> 和方差 <spanclass="math inline">\(\sigma_B^2\)</span>进行估算，以便在推理时使用。</p></li><li><p><strong>推理阶段</strong>：在推理阶段，使用固定的均值和方差（训练时的全局统计量）进行归一化，确保输出是确定的。</p></li></ul><h2 id="evaluation">6. Evaluation</h2><ul><li><p><strong>数据集</strong>：在 CIFAR-10、ImageNet等基准数据集上进行测试。</p></li><li><p><strong>结果</strong>：</p><ul><li><p>BN显著加速了收敛（例如在 ImageNet 上，训练时间减少了多达 14倍）。</p></li><li><p>与没有BN的模型相比，BN 提高了模型的准确率。</p></li></ul></li><li><p><strong>消融实验</strong>：</p><ul><li><p>显示了 <span class="math inline">\(\gamma\)</span> 和 <spanclass="math inline">\(\beta\)</span> 的重要性，以及 mini-batch归一化的有效性。</p></li><li><p>BN 具有一定的正则化效果，减少了对 Dropout 的需求。</p></li></ul></li></ul><h2 id="conclusion">7. Conclusion</h2><p>批量归一化提出了一种简单有效的方法来稳定并加速深度神经网络的训练。它通过减少内部协变量偏移，能够加快收敛速度并提高泛化能力。BN的普适性和有效性使其成为现代深度学习架构中的标准组件。研究强调了解决网络内部分布变化问题对提升训练效率的重要性。</p><h2 id="notes">8. Notes</h2><h3 id="为什么要进行归一化">8.1 为什么要进行归一化？</h3><blockquote><ul><li><p><strong>避免梯度消失或梯度爆炸：</strong></p><ul><li><strong>梯度消失：</strong> 对于偏大的通道值，激活函数（如 Sigmoid或 Tanh）的输出可能趋近其饱和区间（例如，Sigmoid 趋近于 0 或1）。在饱和区域，导数接近于 0，导致梯度几乎消失，权重无法有效更新。</li><li><strong>梯度爆炸：</strong>对于偏小的通道值，激活函数的导数可能非常大，导致梯度在反向传播过程中不断累积并放大，最终引起梯度爆炸。</li></ul><p>这些现象会使优化过程变得极其不稳定，甚至使模型无法收敛。</p></li><li><p><strong>平衡通道值范围：</strong></p><ul><li>如果不同通道的值范围差异显著：<ul><li><strong>梯度更新受大值主导：</strong>较大的值会主导梯度更新方向，网络可能优先调整这些通道的权重。</li><li><strong>忽略小值信息：</strong>较小值的通道可能被忽略，导致网络无法充分利用所有特征信息。</li></ul></li></ul><p>这种不平衡会降低模型的学习效率，延长训练时间，并难以达到最佳性能。</p></li><li><p><strong>简化损失函数的优化过程：</strong></p><ul><li>通道间值差异较大时，损失函数的形状可能会变得复杂（例如，陡峭的谷底或平坦的高原）。</li><li>优化器可能需要更小的学习率逐渐调整权重，从而减慢模型的收敛速度。</li></ul></li></ul></blockquote>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>训练加速</tag>
      
      <tag>算法优化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Going Deeper with Convolutions</title>
    <link href="/2024/11/19/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Going-Deeper-with-Convolutions/"/>
    <url>/2024/11/19/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Going-Deeper-with-Convolutions/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: Going Deeper with Convolutions<br /><strong>Link</strong>: <ahref="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf">InceptionV1 Paper</a><br /><strong>Source</strong>: IEEE Conference on Computer Vision and PatternRecognition (CVPR)<br /><strong>Date</strong>: 2014.09.17</p><h2 id="summary">2. Summary</h2><p>本文提出了一种名为 <strong>Inception</strong>的深度卷积神经网络架构，在提高模型深度和宽度的同时，保持计算开销较低。基于此架构设计的<strong>GoogLeNet</strong>在图像分类和目标检测任务中取得了显著的性能提升。其核心思想是通过多个并行计算路径近似局部稀疏结构，兼顾了计算效率和模型精度。</p><h2 id="background">3. Background</h2><ul><li><p>深度学习的发展依赖于更强大的硬件、更大的数据集以及更高效的网络架构。然而，在移动设备或嵌入式环境中，功耗和内存限制要求算法需更高效。</p></li><li><p>增大网络规模虽能提升性能，但带来了两个问题：</p><ol type="1"><li><p>容易过拟合，需要昂贵的高质量标注数据。</p></li><li><p>参数利用率低，造成计算资源浪费。</p></li></ol></li><li><p>稀疏网络可减少计算量，但现代硬件在稀疏计算上效率不高。</p></li></ul><h2 id="research-objective">4. Research Objective</h2><p>设计一种高效的网络架构，在降低计算复杂度和参数量的同时，保留深度模型的表达能力。通过使用密集的并行模块近似稀疏性，解决传统稀疏结构难以高效并行的问题。</p><h2 id="method">5. Method</h2><ul><li><p><strong>核心思想</strong>：</p><ol type="1"><li>使用 <strong>1×1、3×3 和 5×5 卷积</strong>提取多尺度特征，同时结合池化操作以捕获全局信息。</li><li>在大卷积核之前加入 <strong>1×1卷积</strong>，用于降维和提升非线性表达能力。</li><li>通过模块化设计，平衡计算成本和特征提取能力。</li></ol></li><li><p><strong>网络结构</strong>：</p><ul><li>初版 Inception模块中并行使用不同卷积核和池化操作，会导致通道数增加过快。</li></ul></li></ul><p align="center"><img src="1.png" style="zoom:50%;" /></p><ul><li>改进版通过在每条路径前增加 <strong>1×1卷积降维</strong>，有效控制通道数，降低参数量。</li></ul><p align="center"><img src="2.png" style="zoom:50%;" /></p><ul><li>GoogLeNet 总体架构：<ul><li>采用多层 Inception 模块堆叠，深度增加但计算效率较高。</li><li>引入辅助分类器（仅训练时使用）缓解梯度消失问题。</li></ul></li></ul><p align="center"><img src="3.png" style="zoom:50%;" /></p><h2 id="evaluation">6. Evaluation</h2><h3 id="图像分类任务">① 图像分类任务</h3><ul><li>数据集：ImageNet</li><li>GoogLeNet 在分类任务中取得了 6.67% 的 top-5 错误率，相比 AlexNet 和VGG 显著提升。</li></ul><p align="center"><img src="4.png" style="zoom:50%;" /></p><h3 id="目标检测任务">② 目标检测任务</h3><ul><li>数据集：PASCAL VOC 和 COCO</li><li>在目标检测任务中，结合 Inception 的 R-CNN模型在精度和效率上表现出色。</li></ul><p align="center"><img src="5.png" style="zoom:50%;" /></p><h2 id="conclusion">7. Conclusion</h2><ul><li><strong>稀疏性近似</strong>：通过并行使用多尺度卷积和池化操作，Inception模块模拟局部稀疏结构，既降低了计算复杂度，又避免了稀疏计算的硬件瓶颈。</li><li><strong>模块化设计</strong>：使用 1×1卷积降维，控制通道数增长，有效减少参数量和内存占用。</li><li><strong>高效性能</strong>：GoogLeNet在分类和检测任务上均实现了卓越的性能，是一种计算资源友好的深度学习模型。</li></ul><h2 id="notes">8. Notes</h2><h3 id="卷积的作用">8.1 <strong>1×1 卷积的作用</strong>：</h3><blockquote><ul><li>降维与升维</li><li>降低参数量</li><li>跨通道信息融合</li><li>提高非线性表达能力</li></ul></blockquote><h3 id="辅助分类器的设计注意事项">8.2<strong>辅助分类器的设计注意事项</strong>：</h3><blockquote><ul><li><p>如果设计不当，可能干扰主分类器优化。</p></li><li><p>解决方法包括降低辅助分类器损失权重或简化其结构。</p></li></ul></blockquote>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>算法优化</tag>
      
      <tag>CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
