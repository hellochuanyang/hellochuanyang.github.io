<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>论文笔记 - An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
    <link href="/2024/12/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale/"/>
    <url>/2024/12/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: An Image is Worth 16x16 Words: Transformersfor Image Recognition at Scale<br /><strong>Link</strong>: <a href="https://arxiv.org/pdf/2010.11929">ViTPaper</a><br /><strong>Source</strong>: International Conference on LearningRepresentations (ICLR)<br /><strong>Date</strong>: 2021</p><h2 id="summary">2. Summary</h2><p>本文介绍了 Vision Transformer（ViT），这是一种将 Transformer架构应用于图像识别任务的模型。主要贡献包括：</p><ul><li>证明 Transformer架构可以直接用于图像分类，无需依赖卷积神经网络（CNNs）。</li><li>展示当在大型数据集上预训练然后转移到小型基准测试时，ViT能够达到与最先进的CNN相媲美甚至更优的结果。</li><li>强调 ViT 在训练过程中需要的计算资源显著少于CNNs，使其成为图像识别任务的高效替代方案。</li></ul><h2 id="background">3. Background</h2><p>Transformer 模型在 NLP 任务中已取得了显著的成功，超越了传统的 LSTM 和CNN模型。然而，它在计算机视觉领域的应用却相对较少。在图像任务中，卷积神经网络（CNNs）由于其捕捉图像空间层次结构的能力，一直占据主导地位。ViT挑战了这一传统，通过用 Transformer 架构替代卷积操作，Transformer更灵活，能够建模长程依赖关系。</p><h2 id="research-objective">4. Research Objective</h2><p>本研究的主要目标是证明变换器在图像识别任务中的可行性。具体目标包括：</p><ul><li>提出基于变换器架构的新型图像识别模型——视觉变换器（ViT）。</li><li>探讨数据集大小在训练 ViT中的作用，并表明变换器需要大规模的数据集来有效学习。</li><li>研究预训练的 ViT 模型及其迁移学习的优点。</li></ul><h2 id="method">5. Method</h2><p align="center"><img src="1.png" style="zoom:60%;" /></p><h3 id="图像分块patch-embedding">5.1 <strong>图像分块（PatchEmbedding）</strong></h3><ul><li>将输入图像划分为 16 x 16 像素的固定大小块，这些块被视为 Transformer中的 “tokens”，类似于自然语言处理中的单词。</li><li>每个图像块通过一个线性层映射到一个 D维的空间，这个线性层的参数是可学习的。</li></ul><h3 id="位置编码">5.2 <strong>位置编码</strong></h3><ul><li>在 ViT中，位置编码使用一维的可学习向量，这些向量的长度与图像块嵌入向量的长度相同。</li><li>每个位置（即每个图像块）都有一个对应的位置编码向量，这些向量被随机初始化并在训练过程中与图像块的嵌入一起学习。</li></ul><h3 id="transformer-encoder">5.3 <strong>TransformerEncoder</strong></h3><ul><li>输入序列（图像块嵌入加上位置编码）被送入的 Transformer 编码器。</li><li>编码器由多个层组成，每层包括多头自注意力（Multi-HeadSelf-Attention，MSA）模块和多层感知机（MultilayerPerceptron，MLP）模块。</li><li>在每个 MSA 和 MLP 模块前应用层归一化（LayerNormalization），并在每个模块后应用残差连接。</li></ul><h3 id="分类标记class-token">5.4 <strong>分类标记（ClassToken）</strong></h3><ul><li>类似于 BERT 中的 [CLS] 标记，ViT 中引入了一个额外的可学习 “[class]”标记，它被添加到序列的开始位置。</li><li>在 Transformer 编码器的最后，“[class]”标记的状态被用作整个图像的表示，用于图像分类任务。</li></ul><h3 id="微调">5.5 微调</h3><ul><li>在微调阶段，ViT移除预训练时的分类头（即用于分类的全连接层），并用一个零初始化的新的全连接层替代，新的全连接层的输出维度是下游任务的类别数<span class="math inline">\(K\)</span>。</li></ul><h2 id="conclusion">6. Conclusion</h2><p>Transformer架构可以直接应用于图像识别任务，并且在大规模数据集上预训练后，ViT能够在多个图像识别基准上达到或超过当时的最先进性能。此外，ViT在训练过程中需要的计算资源显著少于传统的CNNs，显示出其在效率上的优势。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>CV</tag>
      
      <tag>Transformer</tag>
      
      <tag>Pre-training</tag>
      
      <tag>Fine-tuning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Improving Language Understanding by Generative Pre-Training</title>
    <link href="/2024/12/03/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Improving-Language-Understanding-by-Generative-Pre-Training/"/>
    <url>/2024/12/03/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Improving-Language-Understanding-by-Generative-Pre-Training/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: Improving Language Understanding byGenerative Pre-Training<br /><strong>Link</strong>: <ahref="https://hayate-lab.com/wp-content/uploads/2023/05/43372bfa750340059ad87ac8e538c53b.pdf">GPTV1 Paper</a><br /><strong>Date</strong>: 2018</p><h2 id="summary">2. Summary</h2><ul><li>本文提出了一种基于 Transformer解码器的生成式语言模型，该模型通过在大规模无监督数据集上进行预训练，从而显著提高了下游任务（如问答、文本分类等）的性能。</li><li>通过预训练和微调的框架，GPT 模型减少了对大规模标注数据的依赖。</li></ul><h2 id="background">3. Background</h2><p>研究解决了特定自然语言任务的标记数据稀缺问题，而大量未标记文本语料库却很丰富。它建立在利用未标记数据可以提供替代收集更多注释的方法，并提高NLP任务性能的理念上。其重要性在于减少对监督学习的依赖，并提高语言理解的技术水平。</p><h2 id="research-objective">4. Research Objective</h2><p>研究目标是学习一个能够很好地转移到广泛任务的通用表示。作者旨在证明，一个在未标记文本上预训练的模型在特定自然语言理解任务上进行微调后，可以显著提高性能。</p><h2 id="method">5. Method</h2><p align="center"><img src="1.png" style="zoom:50%;" /></p><h3 id="self-supervised-pre-training">5.1 Self-supervisedpre-training</h3><p>GPT采用了<strong>自回归</strong>模型，这意味着在每次预测时，模型仅依赖于左侧的上下文信息（单向注意力）。与双向的Transformer 模型（例如 BERT ）不同，GPT的训练过程仅基于从左到右的文本顺序。</p><p>GPT 使用的是基于 <strong>Transformer解码器</strong>的结构，而非完整的 Transformer。每个Transformer块包括多个注意力头和全连接层，层与层之间通过残差连接和层归一化进行处理。与原始的Transformer 解码器相比，GPT省略了交叉注意力（Cross-attention）部分。</p><p>优化目标是基于之前窗口内的词，最大化预测下一个词的概率，公式如下：<span class="math display">\[L_1(\mathcal{U})=\sum_i \log P\left(u_i \mid u_{i-k}, \ldots, u_{i-1} ;\Theta\right)\]</span></p><h3 id="supervised-fine-tuning">5.2 Supervised fine-tuning</h3><p>在预训练的模型后，添加了一个线性层，对最后一个 Token（<spanclass="math inline">\(x^m\)</span>） 的输出（<spanclass="math inline">\(h_l^m\)</span>）映射到任务特定的输出空间。通过这种方式，模型能够针对特定任务进行微调，以适应不同的下游任务需求。<span class="math display">\[P\left(y \mid x^1, \ldots, x^m\right)=\operatorname{softmax}\left(h_l^mW_y\right)\]</span> 优化目标是最大化下述概率： <span class="math display">\[L_2(\mathcal{C})=\sum_{(x, y)} \log P\left(y \mid x^1, \ldots,x^m\right)\]</span> 其中，<span class="math inline">\((x, y)\)</span>代表训练数据中的输入输出对。</p><p>此外综合考虑上述两个优化目标，可以 (a) 增强模型泛化性和 (b)加速模型收敛，公式如下： <span class="math display">\[L_3(\mathcal{C})=L_2(\mathcal{C})+\lambda * L_1(\mathcal{C})\]</span></p><h3 id="基于任务的输入转换">5.3 基于任务的输入转换</h3><p>在微调阶段，为了使预训练模型能够处理各种不同的下游任务，需要对输入数据进行特定的转换，以适应模型的输入要求。这些转换允许模型将结构化输入（例如，问答任务中的文档、问题和答案）转换为连续的标记序列，以便模型可以处理。这些输入转换使得模型能够在不同任务之间进行有效的迁移学习，而无需对模型架构进行大量修改。具体的转换如下：</p><p align="center"><img src="2.png" style="zoom:67%;" /></p><h2 id="conclusion">6. Conclusion</h2><p>生成预训练后进行判别式微调是实现强大自然语言理解的有效框架。该方法在预训练期间获得了世界知识和处理长距离依赖关系的能力，这些能力成功地转移到解决判别式任务上，提高了多个数据集上的技术水平。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Transformer</tag>
      
      <tag>Pre-training</tag>
      
      <tag>Fine-tuning</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Locating and Editing Factual Associations in GPT</title>
    <link href="/2024/12/03/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Locating-and-Editing-Factual-Associations-in-GPT/"/>
    <url>/2024/12/03/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Locating-and-Editing-Factual-Associations-in-GPT/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: Locating and Editing Factual Associations inGPT<br /><strong>Link</strong>: <a href="https://arxiv.org/pdf/2202.05262">ROMEPaper</a><br /><strong>Source</strong>: Conference on Neural Information ProcessingSystems (NeurIPS)<br /><strong>Date</strong>: 2022</p><h2 id="summary">2. Summary</h2><p>本文提出了一种方法，用于在训练完成的 GPT模型中定位和编辑事实知识。该方法的核心思想是通过识别 Transformer网络中的关键组件（特别是 MLP和注意力层），找到存储事实知识的位置，并通过修改这些组件来插入、修改或删除特定的事实。通过在GPT-2上进行演示，表明可以通过操作隐藏激活值来编辑事实知识，而不需要重新训练整个模型。本文的主要贡献在于能够探查Transformer 的内部状态，并通过修改这些状态来编辑模型中的事实关联。</p><h2 id="background">3. Background</h2><p>最近，大型语言模型，特别是像 GPT 这样的 Transformer模型，在预训练过程中编码了大量的事实知识。然而，关于这些知识在模型内部的具体位置和结构仍不明确。能够在模型中定位并修改这些事实关联，将在错误修正、知识插入以及模型个性化等方面具有广泛的应用。虽然有一些研究探讨了如何探查和修改Transformer 模型，但直接编辑模型中具体事实知识的工作还相对较少。</p><h2 id="research-objective">4. Research Objective</h2><p>本研究的主要目标是提出一种方法，用于在预训练的GPT模型中定位和编辑事实知识。具体目标包括：</p><ul><li>识别模型的哪些组件（如层、注意力头等）存储了事实知识。</li><li>开发一种技术，通过修改特定的组件（如隐藏状态），改变模型中的事实知识。</li></ul><h2 id="method">5. Method</h2><p>本文提出的方法主要分为两个部分：因果追踪（Causal Tracing）和 Rank-OneModel Editing (ROME)。下面详细解释这两种方法：</p><h3 id="因果追踪causal-tracing">5.1 因果追踪（Causal Tracing）</h3><p>① <strong>知识元组表示</strong>：将每个事实表示为一个知识元组 <spanclass="math inline">\(t = (s, r, o)\)</span>，其中 <spanclass="math inline">\(s\)</span> 是主题，<spanclass="math inline">\(r\)</span> 是连接两者的关系，<spanclass="math inline">\(o\)</span> 是对象。</p><p>② <strong>模型输入</strong>：为了激发 GPT模型中的事实，研究者提供一个自然语言提示 <spanclass="math inline">\(p\)</span>，描述 <span class="math inline">\((s,r)\)</span>，并检查模型对 <span class="math inline">\(o\)</span>的预测。</p><p>③ <strong>内部激活收集</strong>：在“干净运行”中，将事实提示 <spanclass="math inline">\(x\)</span> 输入模型 <spanclass="math inline">\(G\)</span> 并收集所有隐藏激活： <spanclass="math display">\[\{ h_i^{(l)} \mid i \in [1, T], l \in [1, L] \}\]</span> 其中 <span class="math inline">\(h_i^{(l)}\)</span> 表示第<span class="math inline">\(l\)</span> 层的第 <spanclass="math inline">\(i\)</span> 个 token 的隐藏状态，<spanclass="math inline">\(T\)</span> 是序列中 token 的总数，<spanclass="math inline">\(L\)</span> 是模型的层数。</p><p>④ <strong>基线损坏运行</strong>：在“损坏运行”中，将主题 <spanclass="math inline">\(s\)</span>的嵌入向量损坏，然后让模型继续运行，收集损坏的激活。</p><p>⑤<strong>损坏与恢复运行</strong>：在“损坏与恢复运行”中，模型在损坏的嵌入上运行计算，但在某些标记<span class="math inline">\(\hat{i}\)</span> 和层 <spanclass="math inline">\(\hat{l}\)</span> 上，强制模型输出干净的隐藏状态<span class="math inline">\(h_{\hat{i}}^{(\hat{l})}\)</span>。</p><p>⑥<strong>因果效应量化</strong>：通过比较干净、损坏和损坏与恢复运行下的概率<span class="math inline">\(P[o]\)</span>，<spanclass="math inline">\(P^*[o]\)</span> 和 <spanclass="math inline">\(P_{\text{clean}}^{h(l)_i^*}[o]\)</span>，计算总效应（TE）和特定中介状态的间接效应（IE）。</p><ul><li><p><strong>总效应</strong>（TotalEffect，TE）：反映了损坏主题信息对模型预测的整体影响。通过比较干净运行和损坏运行下的预测概率差异来量化。<span class="math display">\[TE = P[o] - P^*[o]\]</span></p></li><li><p><strong>间接效应</strong>（IndirectEffect，IE）：反映了在特定层或位置上的恢复操作对最终预测的影响。通过比较损坏与恢复运行与损坏运行下的预测概率差异来量化。<span class="math display">\[IE = P_{\text{clean}}^{h(l)_i^*}[o] - P^*[o]\]</span></p></li></ul><p>通过这些度量，研究者能够量化和分析每个隐藏状态（尤其是在不同层次和token 位置的激活）对模型的事实推理和预测的贡献。</p><p align="center"><img src="1.gif" style="zoom:50%;" /></p><h3 id="rank-one-model-editing-rome">5.2 Rank-One Model Editing(ROME)</h3><p>基于上述因果追踪的发现，作者提出了一个假设：中间层的 MLP模块通过接受和处理关于某个主题的输入，逐步累积相关的属性信息（例如，对于“美国”主题，输出可能是与其相关的属性（如“总统是拜登”或“首都华盛顿”））。而这种信息会通过高层的注意力机制，传递到最后一个token 上，从而生成与该主题相关的最终输出。</p><p>将 MLP 的权重 <spanclass="math inline">\(W_{\text{proj}}^{(l)}\)</span>视为线性联想记忆，可以通过解决 <span class="math inline">\(W K \approxV\)</span> 来存储一系列向量键 <span class="math inline">\(K\)</span>和对应的向量值 <span class="math inline">\(V\)</span>。</p><p>下图展示了 Transformer 内的单个 MLP 模块，图中的(b)处的 <spanclass="math inline">\(D\)</span>维向量作为表示要了解的主体的<strong>键</strong>。图中的(c)处的 <spanclass="math inline">\(H\)</span>维输出作为编码有关主体的学习属性的<strong>值</strong>。</p><p align="center"><img src="2.png" style="zoom:80%;" /></p><p>目标是通过拉格朗日乘子法来求解最小二乘问题，其中对新的键值对 <spanclass="math inline">\((k_*, v_*)\)</span>插入记忆。我们要最小化目标函数，同时确保约束条件 <spanclass="math inline">\(\hat{W} k_* = v_*\)</span>被满足。通过这些步骤，我们最终可以计算出最优的权重矩阵 <spanclass="math inline">\(\hat{W}\)</span>，并利用拉格朗日乘子法解出相关的乘子<span class="math inline">\(\Lambda\)</span>。</p><h5 id="目标函数和约束">① <strong>目标函数和约束</strong></h5><p>我们有以下最小二乘目标函数： <span class="math display">\[\operatorname{minimize} \|\hat{W} K - V\|^2 \quad \text{subject to}\quad \hat{W} k_* = v_*\]</span> 这里 <span class="math inline">\(\hat{W}\)</span>是权重矩阵，<span class="math inline">\(K\)</span> 和 <spanclass="math inline">\(V\)</span> 是键和值矩阵，<spanclass="math inline">\(k_*\)</span> 是一个新的键，<spanclass="math inline">\(v_*\)</span> 是其对应的值。目标是最小化 <spanclass="math inline">\(\hat{W} K\)</span> 和 <spanclass="math inline">\(V\)</span> 之间的误差，并通过约束条件 <spanclass="math inline">\(\hat{W} k_* = v_*\)</span>强制插入新的键值对。</p><h5 id="构造拉格朗日函数">② <strong>构造拉格朗日函数</strong></h5><p>我们将目标函数和约束结合，构造拉格朗日函数： <spanclass="math display">\[\mathcal{L}(\hat{W}, \Lambda) = \|\hat{W} K - V\|^2 + \Lambda^T (\hat{W}k_* - v_*)\]</span> 其中 <span class="math inline">\(\Lambda\)</span>是拉格朗日乘子向量。</p><h5 id="对-hatw-求偏导数">③ <strong>对 <spanclass="math inline">\(\hat{W}\)</span> 求偏导数</strong></h5><p>对拉格朗日函数 <span class="math inline">\(\mathcal{L}\)</span> 关于<span class="math inline">\(\hat{W}\)</span> 求偏导数并令其为零： <spanclass="math display">\[0 = \frac{\partial \mathcal{L}}{\partial \hat{W}} = \hat{W} K K^T - VK^T - \Lambda k_*^T\]</span> 这给出了如下方程： <span class="math display">\[\hat{W} K K^T = V K^T + \Lambda k_*^T\]</span></p><h5 id="解出-hatw-的表达式">④ <strong>解出 <spanclass="math inline">\(\hat{W}\)</span> 的表达式</strong></h5><p>通过整理上式，可以得到： <span class="math display">\[(\hat{W} - W) K K^T = \Lambda k_*^T\]</span> 因此： <span class="math display">\[\hat{W} = W + \Lambda (C^{-1} k_*)^T\]</span> 其中 <span class="math inline">\(C = K K^T\)</span>，定义<span class="math inline">\(u^T = (C^{-1} k_*)^T\)</span> 可得： <spanclass="math display">\[\hat{W} = W + \Lambda u^T\]</span></p><h5 id="对-lambda-求解">⑤ <strong>对 <spanclass="math inline">\(\Lambda\)</span> 求解</strong></h5><p>根据约束 <span class="math inline">\(\hat{W} k_* = v_*\)</span>，将<span class="math inline">\(\hat{W} = W + \Lambda u^T\)</span> 代入得：<span class="math display">\[\hat{W} k_* = (W + \Lambda u^T) k_* = W k_* + \Lambda (u^T k_*) = v_*\]</span> 由此，得到拉格朗日乘子的表达式： <span class="math display">\[\Lambda = \frac{v_* - W k_*}{u^T k_*} = \frac{v_* - W k_*}{(C^{-1}k_*)^T k_*}\]</span></p><h5 id="最终的-hatw-表达式">⑥ <strong>最终的 <spanclass="math inline">\(\hat{W}\)</span> 表达式</strong></h5><p>最后，我们可以将求得的 <span class="math inline">\(\Lambda\)</span>值代入 <span class="math inline">\(\hat{W} = W + \Lambda u^T\)</span>中，得到最优的 <span class="math inline">\(\hat{W}\)</span>： <spanclass="math display">\[\hat{W} = W + \Lambda u^T\]</span> 其中 <span class="math inline">\(\Lambda\)</span>的值由上述公式给出。</p><p>综上，只要计算得到新的键值对 <span class="math inline">\(\left(k_*,v_*\right)\)</span>，即可将事实插入现有模型。</p><h4 id="step-1计算-k_">Step 1：计算 <spanclass="math inline">\(k_*\)</span></h4><p>通过输入文本 <span class="math inline">\(x\)</span> 和主题 <spanclass="math inline">\(s\)</span>，通过 GPT模型传递这些数据，并在模型的特定层 <spanclass="math inline">\(l^*\)</span> 和最后一个主题 token 的位置 <spanclass="math inline">\(i\)</span>，计算其 MLP 层的激活值。</p><p>假设我们通过以下公式计算 <spanclass="math inline">\(k(x)\)</span>，这是表示输入文本 <spanclass="math inline">\(x\)</span> 和对应主题 <spanclass="math inline">\(s\)</span> 的激活值： <spanclass="math display">\[k(x)=\sigma\left(W_{f c}^{\left(l^*\right)}\gamma\left(a^{\left(l^*\right)}[x, i]+h^{\left(l^*-1\right)}[x,i]\right)\right)\]</span> 其中：</p><ul><li><span class="math inline">\(\sigma\)</span> 是激活函数（如 ReLU 或Tanh），</li><li><span class="math inline">\(W_{fc}^{(l^*)}\)</span> 是层 <spanclass="math inline">\(l^*\)</span> 中的全连接层权重，</li><li><span class="math inline">\(\gamma\)</span> 是对当前 token <spanclass="math inline">\(x\)</span> 和前一层的激活值 <spanclass="math inline">\(h^{(l^*-1)}[x, i]\)</span> 的变换，</li><li><span class="math inline">\(a^{(l^*)}[x, i]\)</span> 是在层 <spanclass="math inline">\(l^*\)</span> 中通过注意力机制计算的 token的激活值。</li></ul><p>通过对一组文本进行多次采样，得到所有输入 <spanclass="math inline">\(x_j\)</span> 和主题 <spanclass="math inline">\(s\)</span> 的平均值，计算出查找键 <spanclass="math inline">\(k^*\)</span>： <span class="math display">\[k^* = \frac{1}{N} \sum_{j=1}^N k(x_j + s)\]</span> 这个过程帮助我们识别模型中存储事实知识的组件。</p><h4 id="step-2计算-v_">Step 2：计算 <spanclass="math inline">\(v_*\)</span></h4><p>首先构造一个损失函数 <spanclass="math inline">\(\mathcal{L}(z)\)</span>： <spanclass="math display">\[\frac{1}{N} \sum_{j=1}^N \underbrace{-\log\mathbb{P}_{G\left(m_i^{\left(l^*\right)}:=z\right)}\left[o^* \midx_j+p\right]}_{\text {(a) Maximizing } o^* \text { probability}}+\underbrace{D_{\mathrm{KL}}\left(\mathbb{P}_{G\left(m_{i^{\prime}}^{\left(l^*\right)}:=z\right)}\left[x\mid p^{\prime}\right] \| \mathbb{P}_G\left[x \midp^{\prime}\right]\right)}_{\text {(b) Controlling essence drift }} .\]</span> （a）是为了最大化目标对象 <spanclass="math inline">\(o^*\)</span> 的概率, 鼓励模型在给定 <spanclass="math inline">\(z\)</span> 作为输出时，更倾向于预测目标对象 <spanclass="math inline">\(o^*\)</span>。</p><p>（b）通过最小化 KL 散度来控制模型对主题 <spanclass="math inline">\(s\)</span> 的本质理解的变化。</p><p>在优化过程中，不改变模型的参数，而是通过调整 <spanclass="math inline">\(z\)</span> 来最小化损失函数 <spanclass="math inline">\(L(z)\)</span>。最终得到的 <spanclass="math inline">\(z\)</span> 就是 <spanclass="math inline">\(v\)</span>，这个向量能够在不改变模型权重的情况下，改变模型对特定事实的预测。</p><h4 id="step-3使用-leftk_-v_right-对模型进行修改">Step 3：使用 <spanclass="math inline">\(\left(k_*, v_*\right)\)</span> 对模型进行修改</h4><h4id="举例说明将现有事实艾菲尔铁塔位于巴黎修改为艾菲尔铁塔位于罗马">举例说明：将现有事实“艾菲尔铁塔位于巴黎”修改为“艾菲尔铁塔位于罗马”</h4><p>第一步：首先输入多个包含埃菲尔铁塔的语句，例如：“美丽的埃菲尔铁塔”等，收集 <spanclass="math inline">\(k_*\)</span>。</p><p>第二步： 输入 <span class="math inline">\(x_j+p\)</span> 这里的 <spanclass="math inline">\(x_j\)</span>是包含“埃菲尔铁塔”的语句，可以是“埃菲尔铁塔位于”，模型通过优化调整 <spanclass="math inline">\(z\)</span>，使得最终输出 “罗马”。</p><p>第三步： 使用 <span class="math inline">\(k_*\)</span> 和 <spanclass="math inline">\(v_*\)</span>来修改模型中存储的原有事实。这意味着当模型再次遇到与“埃菲尔铁塔”相关的输入时，它将使用新的<span class="math inline">\(v_*\)</span>向量来生成输出，从而将“埃菲尔铁塔位于巴黎”修改为“埃菲尔铁塔位于罗马”。</p><h2 id="conclusion">6. Conclusion</h2><p>本文证明了可以通过操作 GPT 模型中 MLP层的隐藏状态，定位并编辑事实知识。该方法能够在不重新训练整个模型的情况下，进行特定事实的修改。该方法对个性化AI、模型纠错以及动态更新模型中的事实知识具有重要意义。</p><h2 id="notes">7. Notes</h2><ol type="1"><li><a href="https://rome.baulab.info/">ROME 文章主页</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>NLP</tag>
      
      <tag>知识编辑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
    <link href="/2024/11/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/"/>
    <url>/2024/11/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: BERT: Pre-training of Deep BidirectionalTransformers for Language Understanding <strong>Link</strong>: <ahref="https://arxiv.org/pdf/1810.04805">BERT Paper</a><strong>Source</strong>: The North American Chapter of the Associationfor Computational Linguistics (NAACL) <strong>Date</strong>: 2019</p><h2 id="summary">2. Summary</h2><p>本文提出了 BERT（Bidirectional Encoder Representations fromTransformers），一种用于语言理解的深度双向变换器预训练方法。BERT的创新之处在于其使用了双向上下文，与之前的单向上下文模型（如GPT）不同。该预训练模型可以通过最小的任务特定架构进行微调，从而在多个NLP 任务中实现了最先进的性能，如问答和语言推理。</p><h2 id="background">3. Background</h2><p>在 BERT 之前，许多 NLP 模型依赖单向变换器（如 GPT）或顺序模型（如LSTM）。这些模型只能从左到右或从右到左处理文本，限制了它们捕捉丰富的双向上下文的能力。BERT通过使用变换器架构并以双向方式进行训练，克服了这一限制，使得模型能够理解句子中每个词的完整上下文，从而显著提升了NLP 性能。</p><h2 id="research-objective">4. Research Objective</h2><p>本文的目标是提出一种新的预训练方法，用于变换器模型，能够有效捕捉双向上下文。作者旨在提升现有NLP模型的表现，并创建一种更加灵活的通用模型，能够通过最小的改动在特定任务上进行微调。</p><h2 id="method">5. Method</h2><p>BERT的创新方法主要体现在其<strong>预训练任务</strong>和<strong>模型架构</strong>两个方面。通过这两种方式，BERT突破了传统的 NLP模型，在多个下游任务上取得了最先进的效果。以下详细介绍了 BERT的架构、预训练任务和微调方法。</p><h3 id="bert-模型架构">5.1 BERT 模型架构</h3><p>BERT 的架构基于 <strong>Transformer</strong>，尤其是 Transformer的<strong>编码器部分</strong>。Transformer 架构由Google 在《Attention IsAll YouNeed》论文中提出，具有很强的并行计算能力和建模长期依赖关系的能力，尤其适用于NLP 任务。BERT 仅使用了 Transformer中的编码器部分，并进行了双向训练。BERT的核心特点是<strong>双向训练</strong>，它通过使用 Transformer的自注意力机制来同时考虑上下文中的前后信息。</p><h4 id="transformer编码器">5.1.1 <strong>Transformer编码器</strong></h4><ul><li><strong>自注意力机制（Self-Attention）</strong>：自注意力机制计算输入序列中每个词对其他词的注意力权重。每个词的表示都基于它与所有其他词的关系。</li><li><strong>多头注意力（Multi-HeadAttention）</strong>：将自注意力机制扩展为多个“头”，每个头学习不同的子空间的注意力表示。所有头的结果被拼接起来并通过线性变换得到最终表示。</li><li><strong>前馈网络（Feed-Forward Networks）</strong>：每个 Transformer层还包括一个前馈神经网络，负责对每个位置的表示进行独立的非线性变换。</li></ul><p>BERT 采用了多层 Transformer编码器堆叠，每一层都包括自注意力机制和前馈网络，最终通过这些层来生成上下文相关的词表示。</p><h4 id="bert的嵌入embeddings">5.1.2<strong>BERT的嵌入（Embeddings）</strong></h4><p>在 BERT模型中，输入的每个单词或子词（token）都需要通过一定的嵌入（embedding）映射到一个高维空间，这些嵌入提供了对词语的丰富表示。BERT的输入嵌入由三部分组成：<strong>TokenEmbeddings</strong>（词嵌入）、<strong>SegmentEmbeddings</strong>（段落嵌入）和 <strong>PositionEmbeddings</strong>（位置嵌入）。这三种嵌入结合起来，帮助模型理解词汇、句子结构以及词语的顺序信息。</p><p align="center"><img src="1.png" style="zoom:60%;" /></p><h5 id="token-embeddings词嵌入"><strong>TokenEmbeddings（词嵌入）</strong></h5><p>Token Embeddings 是 BERT 模型中最基本的输入嵌入，主要是将每个输入的token（例如词或子词）映射到一个固定的向量空间。BERT 的 token嵌入使用了词表（vocabulary）中的每个 token 对应的一个嵌入向量。在 BERT中，token 通常使用 <strong>WordPiece</strong>分词法来切分，这意味着每个词或词组会被进一步拆分为子词单元，因此每个输入的token（子词）都会有对应的嵌入向量,通常是通过查表的方式获取。</p><h5 id="segment-embeddings段落嵌入"><strong>SegmentEmbeddings（段落嵌入）</strong></h5><p>BERT能够处理成对输入的任务，例如问答或句子对分类任务。在这些任务中，输入有两个部分，BERT需要区分这两个部分。为此，BERT 引入了 Segment Embeddings，它为每个 token分配一个标识符，指示它属于第一个句子（Segment A）还是第二个句子（SegmentB）,Segment Embeddings 不参与模型训练，是固定的值。</p><ul><li>对于第一个句子中的每个 token，它的 Segment Embedding 值为 0。</li><li>对于第二个句子中的每个 token，它的 Segment Embedding 值为 1。</li></ul><h5 id="position-embeddings位置嵌入"><strong>PositionEmbeddings（位置嵌入）</strong></h5><p>BERT 的 Transformer 架构本身并不具备处理词序信息的能力（不像 RNN 或LSTM那样顺序处理输入），因此需要引入位置嵌入来显式地编码每个词在序列中的位置信息。PositionEmbeddings 为每个 token 提供一个表示它在输入序列中位置的向量。</p><p>BERT 使用了<strong>可学习的位置编码</strong>（不像 Transformer的相对位置编码）。每个 token的位置都对应着一个唯一的嵌入向量，并且位置嵌入是与 token 嵌入、segment嵌入一起相加的。</p><h3 id="bert的预训练任务">5.2 <strong>BERT的预训练任务</strong></h3><p>BERT 的预训练任务非常关键，决定了模型如何学习语言的上下文信息。BERT的预训练任务包含两个主要部分：<strong>掩蔽语言建模（Masked LanguageModeling，MLM）</strong>和<strong>下一句预测（Next SentencePrediction，NSP）</strong>。</p><h4 id="掩蔽语言建模-mlm">5.2.1 <strong>掩蔽语言建模 (MLM)</strong></h4><p>掩蔽语言建模是 BERT的一项核心创新，它允许模型在训练过程中通过学习上下文来预测被掩蔽的词。与传统的语言建模方法（如GPT）不同，BERT并非仅从左到右或从右到左进行语言建模，而是通过掩蔽掉输入中的一些词并预测这些词来进行训练。</p><ul><li><p><strong>掩蔽过程</strong>：BERT 随机选择输入中的 15%的词进行掩蔽，并要求模型根据上下文预测这些被掩蔽的词。被选择的 15% 词中80% 的情况下，掩蔽的词被替换为“[MASK]”，10%的情况下，掩蔽的词保持不变，10%的情况下，掩蔽的词被替换为一个随机的词。</p></li><li><p><strong>训练目标</strong>：BERT的目标是最大化每个被掩蔽单词的预测概率。具体来说，给定输入文本 <spanclass="math inline">\(X = (x_1, x_2, ..., x_n)\)</span>，BERT会掩蔽其中的某些词，并要求模型预测这些词的值： <spanclass="math display">\[\hat{x}_i=\operatorname{BERT}\left(\tilde{X}_{-i}\right)\]</span> 其中，<span class="math inline">\(\tilde{X}_{-i}\)</span>表示去除第 <span class="math inline">\(i\)</span> 个词后的文本，<spanclass="math inline">\(\hat{x}_i\)</span> 是模型对第 <spanclass="math inline">\(i\)</span> 个词的预测。</p></li></ul><p>掩蔽语言建模的主要目标是让模型能够在上下文中获取对每个词的全局理解，从而增强语义理解能力。</p><h4 id="下一句预测-nsp">5.2.2 <strong>下一句预测 (NSP)</strong></h4><p>除了掩蔽语言建模，BERT还使用了下一句预测任务，这一任务用于捕捉句子之间的关系。NSP任务帮助模型理解句子对之间的语义关联，尤其是在理解长文档时。</p><ul><li><p><strong>任务描述</strong>：NSP 任务的目标是判断给定的一对句子<span class="math inline">\(S_1\)</span> 和 <spanclass="math inline">\(S_2\)</span> 是否在原始文档中是连续的，即判断<span class="math inline">\(S_2\)</span> 是否是 <spanclass="math inline">\(S_1\)</span> 的后续句子。如果 <spanclass="math inline">\(S_2\)</span> 是 <spanclass="math inline">\(S_1\)</span> 的后续句子，则标签为 1；否则为0。</p></li><li><p><strong>模型输入</strong>：将一对句子 <spanclass="math inline">\((S_1, S_2)\)</span> 拼接在一起，并通过 BERT进行处理。然后，输出一个二分类的标签，表示第二个句子是否是第一个句子的后续句子。</p><p>换句话说，给定一对句子 <span class="math inline">\(S_1\)</span> 和<span class="math inline">\(S_2\)</span>，模型的目标是计算： <spanclass="math display">\[P\left(\operatorname{IsNext} \mid S_1,S_2\right)=\operatorname{sigmoid}\left(W^T h_2+b\right)\]</span> 其中，<span class="math inline">\(h_2\)</span> 是第二个句子<span class="math inline">\(S_2\)</span> 在 BERT 模型中的表示，<spanclass="math inline">\(W\)</span> 和 <spanclass="math inline">\(b\)</span> 是训练的权重和偏置，sigmoid函数用于输出一个概率值，表示 <span class="math inline">\(S_2\)</span>是否是 <span class="math inline">\(S_1\)</span> 的后续句子。</p></li></ul><p>NSP任务对于句子级别的推理（例如，问答任务中的句子对匹配、文本蕴涵等任务）特别重要。</p><h3 id="bert的微调-fine-tuning">5.3 <strong>BERT的微调(Fine-tuning)</strong></h3><p>BERT 的一个重要特点是其强大的<strong>微调能力</strong>。BERT的预训练模型可以通过微调来适应特定的 NLP 任务。在微调时，BERT只需要少量的任务特定架构修改即可，因此能够在多种任务上展现出强大的性能。</p><ul><li><p><strong>微调过程</strong>：微调过程非常简单，通常只需在 BERT的顶部添加一个简单的输出层（例如分类层），然后根据具体任务进行训练。这使得BERT能够轻松适应各种任务，如文本分类、命名实体识别（NER）、问答、情感分析等。</p></li><li><p><strong>任务特定的输出层</strong>：不同任务需要不同的输出层。例如，对于分类任务，BERT的输出层通常是一个 softmax分类器；对于序列标注任务（如命名实体识别），输出层可能是一个标签的序列。</p></li><li><p><strong>微调公式</strong>：微调过程中，BERT的参数和输出层的参数都会共同更新。假设目标任务的损失函数为 <spanclass="math inline">\(\mathcal{L}_{\text {task}}\)</span>，则微调时的目标是最小化该任务的损失： <spanclass="math display">\[\mathcal{L}_{\text {total }}=\mathcal{L}_{\text {task }}\]</span>对于不同的任务，损失函数可以是交叉熵损失、均方误差损失等。</p></li></ul><h2 id="conclusion">6. Conclusion</h2><p>BERT 通过提出一种新的双向变换器预训练方法，在多个 NLP任务中设立了新的基准。它的通用性使其能够通过微调适应不同的任务，从而证明了双向预训练在语言理解中的强大能力。BERT的成功也展示了 NLP 中迁移学习的潜力。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Transformer</tag>
      
      <tag>Pre-training</tag>
      
      <tag>Fine-tuning</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Attention Is All You Need</title>
    <link href="/2024/11/28/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/"/>
    <url>/2024/11/28/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: Attention Is All You Need<br /><strong>Link</strong>: <ahref="https://arxiv.org/pdf/1706.03762">Transformer</a><br /><strong>Source</strong>: Conference on Neural Information ProcessingSystems (NeurIPS)<br /><strong>Date</strong>: 2017</p><h2 id="summary">2. Summary</h2><p>本文提出了 <strong>Transformer</strong>模型，这是一种用于序列转换任务（特别是机器翻译）的新型深度学习架构。Transformer的核心创新是<strong>自注意力机制</strong>，该机制使得模型能够高效地处理序列数据，而不依赖于循环神经网络（RNN）或卷积神经网络（CNN）。该模型在编码和解码阶段均使用自注意力和前馈神经网络，显著提高了并行化能力，减少了训练时间。实验结果表明，Transformer在机器翻译任务中超越了基于 LSTM 和 GRU的传统模型，取得了更好的性能。</p><h2 id="background">3. Background</h2><p>在 Transformer提出之前，序列到序列任务（如机器翻译）通常由<strong>循环神经网络（RNN）及其改进版本（如LSTM 和GRU）</strong>来处理。这些模型是逐步处理数据，因此存在并行化难度，长依赖关系的处理也存在瓶颈。<strong>卷积神经网络（CNN）</strong>在某些序列处理任务中也有应用，但它们在捕捉长程依赖关系时的能力较弱。</p><h2 id="research-objective">4. Research Objective</h2><p>本文的研究目标是提出一种新的模型架构，能够在不依赖循环或卷积层的情况下高效处理序列转换任务，尤其是机器翻译。具体目标包括：</p><ol type="1"><li>设计一个易于并行化的模型，从而提高训练效率。</li><li>开发一个能够捕捉长程依赖关系的模型，不受传统RNN模型的限制。</li><li>证明该模型在标准基准任务中超越传统RNN模型的表现。</li></ol><h2 id="method">5. Method</h2><h3 id="自注意力机制self-attention">5.1自注意力机制（Self-Attention）</h3><p>自注意力机制是 Transformer的核心创新之一。它允许模型在处理每个输入位置时，对序列中所有位置的其他元素进行加权求和，从而捕捉词语间的依赖关系。自注意力机制通过计算每个词与其他词之间的相似度，来决定其在表示中的重要性。</p><h5 id="计算过程">计算过程</h5><p>自注意力机制的关键计算过程可以通过以下三个向量来描述：</p><ul><li><strong>Query（查询向量）</strong>：代表当前词需要从其他词中获取信息的“查询”。</li><li><strong>Key（键向量）</strong>：表示当前词的特征，用于与查询进行比较。</li><li><strong>Value（值向量）</strong>：包含关于当前词的实际信息，用于加权求和。</li></ul><p>具体公式如下： <span class="math display">\[\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V\]</span></p><ul><li><span class="math inline">\(Q\)</span> 是查询矩阵，<spanclass="math inline">\(K\)</span> 是键矩阵，<spanclass="math inline">\(V\)</span> 是值矩阵。</li><li><span class="math inline">\(d_k\)</span> 是键向量的维度。</li><li><span class="math inline">\(Q K^T\)</span>计算查询和键之间的点积相似度，然后通过 <spanclass="math inline">\(\sqrt{d_k}\)</span>进行缩放，以防止点积值过大。</li><li>经过 softmax 操作后，获得的权重矩阵用于对值向量 <spanclass="math inline">\(V\)</span> 进行加权求和。</li></ul><p>这个公式的核心思想是通过对每个词的查询向量与所有其他词的键向量进行点积，得到一个权重分布，再利用该权重对值向量进行加权和求和，从而得到每个词的新的表示。</p><h3 id="多头注意力机制multi-head-attention">5.2多头注意力机制（Multi-Head Attention）</h3><p>多头注意力机制是对自注意力的扩展，它通过多个并行的自注意力计算来捕捉输入序列中的不同特征。每个注意力头（head）学习一个不同的投影，从不同的角度来关注输入序列中的各个部分。</p><p>具体操作如下：</p><ol type="1"><li>输入的查询、键和值分别被映射到多个不同的子空间，每个子空间对应一个注意力头。</li><li>每个注意力头独立计算自注意力（如上述公式所示），得到一个加权的表示。</li><li>最后，将所有头的输出拼接在一起，并通过一个线性变换来得到最终的输出。</li></ol><p>多头注意力机制的公式如下： <span class="math display">\[\operatorname{MultiHead}(Q, K,V)=\operatorname{Concat}\left(\operatorname{head}_1, \ldots, \text {head }_h\right) W^O\]</span> 其中，<spanclass="math inline">\(\operatorname{head}_i=\operatorname{Attention}\left(QW_i^Q, K W_i^K, V W_i^V\right)\)</span>，表示第 <spanclass="math inline">\(i\)</span> 个注意力头的计算过程。</p><ul><li><span class="math inline">\(W_i^Q, W_i^K, W_i^V\)</span>是用于映射查询、键、值的权重矩阵。</li><li><span class="math inline">\(W^O\)</span> 是输出映射的权重矩阵。</li></ul><p>通过这种方式，模型能够并行地关注输入序列的不同部分，并从多角度提取信息。</p><h3 id="位置编码positional-encoding">5.3 位置编码（PositionalEncoding）</h3><p>由于 Transformer 没有像 RNN那样的序列顺序处理机制，因此需要引入位置编码来保持序列中每个单词的位置信息。位置编码将每个词的位置映射为一个向量，并加到词嵌入（embedding）中。位置编码的设计保证了每个词的位置信息可以被捕捉到，而不会影响自注意力的计算。</p><p>位置编码使用正弦和余弦函数来生成不同频率的编码，公式如下： <spanclass="math display">\[\begin{gathered}\mathrm{PE}(p o s, 2 i)=\sin \left(\frac{p o s}{10000^{2 i / d_{\text{model }}}}\right) \\\mathrm{PE}(p o s, 2 i+1)=\cos \left(\frac{p o s}{10000^{2 i / d_{\text{model }}}}\right)\end{gathered}\]</span></p><ul><li><span class="math inline">\(pos\)</span> 是单词的位置，<spanclass="math inline">\(i\)</span> 是维度索引，<spanclass="math inline">\(d_{\text {model }}\)</span> 是词嵌入的维度。</li></ul><p>这种编码方式能够确保对于较远的词，正弦和余弦函数的值会呈现不同的周期性，能够捕捉到不同尺度的位置信息。</p><h3 id="前馈神经网络feed-forward-neural-networks">5.4前馈神经网络（Feed-Forward Neural Networks）</h3><p>Transformer中的每个层除了自注意力层之外，还包含一个前馈神经网络（Feed-ForwardNeural Network）。该网络包含两个线性变换和一个 ReLU 激活函数，公式如下：<span class="math display">\[\operatorname{FFN}(x)=\max \left(0, x W_1+b_1\right) W_2+b_2\]</span></p><ul><li><span class="math inline">\(W_1, W_2\)</span> 和 <spanclass="math inline">\(b_1, b_2\)</span> 是可学习的权重和偏置。</li><li><span class="math inline">\(\max (0, x)\)</span> 是 ReLU激活函数。</li></ul><p>前馈网络在每个位置独立地应用，因此它能够进一步增强每个词表示的非线性特征。</p><h3id="残差连接和层归一化residual-connection-and-layer-normalization">5.5残差连接和层归一化（Residual Connection and Layer Normalization）</h3><p>为了避免深层网络中的梯度消失问题，Transformer使用了<strong>残差连接（ResidualConnection）</strong>。每个子层（如注意力层、前馈神经网络层）的输出都会加上其输入，即：<span class="math display">\[\text { output }=\text { LayerNorm }(x+\operatorname{Sublayer}(x))\]</span> 这里，Sublayer(x) 是该子层的输出，<spanclass="math inline">\(x\)</span>是该子层的输入。残差连接有助于确保信息能够直接流过网络的每一层，避免梯度消失问题，同时层归一化（LayerNorm）有助于加速训练。</p><h2 id="evaluation">6. Evaluation</h2><p>作者通过以下几个标准数据集对 Transformer 模型进行了评估：</p><ul><li><strong>WMT 2014 英德翻译和英法翻译任务</strong>。</li><li>与传统的基于 RNN 的模型（如 LSTM）和 CNN 模型进行了比较。</li></ul><p>主要评估指标包括：</p><ul><li><strong>BLEU（Bilingual EvaluationUnderstudy）</strong>分数，这是衡量机器翻译质量的标准指标。</li><li><strong>训练时间</strong>：由于 Transformer的高度并行化，训练时间显著低于基于 RNN 的模型。</li></ul><p>实验结果表明，Transformer 在翻译质量和训练效率上都显著超越了传统的RNN 模型。</p><h2 id="conclusion">7. Conclusion</h2><p>Transformer模型是序列到序列任务中的一项重大创新，证明了自注意力机制可以取代传统的循环和卷积层。本文的主要结论如下：</p><ol type="1"><li><strong>高效性</strong>：Transformer在性能超越的同时，还具备了高度的并行化能力，显著降低了训练时间。</li><li><strong>有效性</strong>：在机器翻译任务中，Transformer 超越了传统RNN 模型，能够有效捕捉长程依赖关系。</li><li><strong>灵活性</strong>：Transformer架构具有很好的通用性，能够应用于其他序列相关任务，之后的 BERT、GPT、T5等模型也基于Transformer 架构发展而来。</li></ol><p>Transformer为自然语言处理和其他领域的深度学习模型奠定了新的基础。</p><h2 id="notes">8. Notes</h2><ol type="1"><li>Transformer 为什么使用 LN 而不使用 BN?</li></ol><blockquote><p>在 Transformer 模型中，使用<strong>层归一化（LayerNormalization，LN）</strong>而非<strong>批量归一化（BatchNormalization，BN）</strong>，主要是因为以下几个原因：</p><ol type="1"><li><p><strong>序列数据的依赖性</strong></p><p>Transformer是处理序列数据的模型，其中每个输入元素可能与其他元素有复杂的依赖关系。<strong>批量归一化（BN）</strong>依赖于批次内的统计量（均值和方差），这在卷积神经网络（CNN）中有效，但对于序列数据存在问题：</p><ul><li><strong>依赖批次数据</strong>：BN计算基于批次的统计量，而每个时间步的输入需要独立处理其上下文信息。不同样本的分布可能会干扰每个时间步的计算。</li><li><strong>小批次问题</strong>：Transformer通常使用小批次处理长序列，BN在小批次下表现较差，因为统计量不稳定，可能影响训练的稳定性。</li></ul></li><li><p><strong>训练稳定性</strong>：层归一化（LN）可以提高训练过程的稳定性，特别是在深度神经网络中。相比之下，批量归一化（BN）受批次大小的影响，尤其是在小批次情况下，均值和方差的计算可能不准确，从而影响训练稳定性。而LN对每个样本独立归一化，避免了这个问题，因此训练过程更加稳定。</p></li><li><p><strong>并行计算的要求</strong>：Transformer的一个显著特点是其<strong>高度并行化</strong>的计算方式，特别是在计算自注意力时。BN在训练时需要在整个批次上计算均值和方差，这通常是针对整个批次的操作。而LN 是对单个样本的操作，因此更加适合在训练时进行并行计算，尤其是在 GPU 或TPU 上训练时，能够更好地加速训练过程。</p></li></ol></blockquote><ol start="2" type="1"><li>Transformer 中前馈神经网络里面的 <spanclass="math inline">\(x\)</span> 和 <spanclass="math inline">\(W\)</span> 的 shape 分别是多少？</li></ol><blockquote><p>在 Transformer 的前馈神经网络（Feed-Forward Network，FFN）中，输入<span class="math inline">\(x\)</span> 和权重矩阵 <spanclass="math inline">\(W\)</span> 的形状如下：</p><ol type="1"><li><strong>输入 <span class="math inline">\(x\)</span>的形状</strong>：</li></ol><p>前馈神经网络的输入是自注意力（Self-Attention）层的输出。假设输入的张量<span class="math inline">\(x\)</span> 的形状为： <spanclass="math display">\[x \in \mathbb{R}^{B \times S \times D}\]</span></p><ul><li><span class="math inline">\(B\)</span> 是批次大小（batch size）</li><li><span class="math inline">\(S\)</span> 是序列长度（sequencelength）</li><li><span class="math inline">\(D\)</span>是每个时间步的特征维度，通常称为模型的隐藏维度（<spanclass="math inline">\(d_{\text {model }}\)</span>）</li></ul><ol start="2" type="1"><li><strong>权重矩阵 <span class="math inline">\(W\)</span>的形状</strong>：</li></ol><p>前馈神经网络通常由两个线性层组成，假设第一个线性层将输入的特征维度<span class="math inline">\(D\)</span> 映射到更高的维度 <spanclass="math inline">\(d_{\mathrm{ff}}\)</span>，然后第二个线性层将其映射回<span class="math inline">\(D\)</span>。</p><ul><li><p>第一个线性层的权重矩阵 <span class="math inline">\(W_1\)</span>的形状为： <span class="math display">\[W_1 \in \mathbb{R}^{D \times d_{\mathrm{ff}}}\]</span> 这里，<span class="math inline">\(D\)</span>是输入特征维度，<span class="math inline">\(d_{\mathrm{ff}}\)</span>是前馈网络的隐藏层维度。</p></li><li><p>第二个线性层的权重矩阵 <span class="math inline">\(W_2\)</span>的形状为： <span class="math display">\[W_2 \in \mathbb{R}^{d_{\mathrm{ff}} \times D}\]</span> 这里，<span class="math inline">\(d_{\mathrm{ff}}\)</span>是前馈网络的隐藏层维度，<span class="math inline">\(D\)</span>是输出的特征维度，通常与输入的特征维度 <spanclass="math inline">\(D\)</span> 相同。</p></li></ul><p><strong>在整个计算过程中，前馈神经网络对每个时间步的特征进行独立处理，输入维度<span class="math inline">\(D\)</span> 被映射到更高的维度 <spanclass="math inline">\(d_{\mathrm{ff}}\)</span>，然后再映射回原始维度<span class="math inline">\(D\)</span></strong>。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Transformer</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</title>
    <link href="/2024/11/27/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-ShuffleNet-V2-Practical-Guidelines-for-Efficient-CNN-Architecture-Design/"/>
    <url>/2024/11/27/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-ShuffleNet-V2-Practical-Guidelines-for-Efficient-CNN-Architecture-Design/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: ShuffleNet V2: Practical Guidelines forEfficient CNN Architecture Design<br /><strong>Link</strong>: <ahref="https://openaccess.thecvf.com/content_ECCV_2018/papers/Ningning_Light-weight_CNN_Architecture_ECCV_2018_paper.pdf">ShuffleNetV2 Paper</a><br /><strong>Source</strong>: European Conference on Computer Vision(ECCV)<br /><strong>Date</strong>: 2018</p><h2 id="summary">2. Summary</h2><p>本文提出了 ShuffleNetV2，一种新的轻量级卷积神经网络架构，旨在提高计算效率，特别适用于计算能力受限的移动设备。研究指出，直接度量（如速度）不仅取决于FLOPs（乘加数），还受到存储访问量和硬件平台特性等因素的影响。因此，本文基于直接度量评估目标平台上的性能，并提出了几个实用指南来指导高效的网络设计。</p><h2 id="background">3. Background</h2><p>卷积神经网络（CNN）在计算机视觉任务中取得了显著的成功，但其庞大的模型体积和高计算需求，使其在移动设备和嵌入式系统等资源受限的设备上应用时面临挑战。之前的轻量级CNN 模型，如 MobileNet 和ShuffleNet，尝试通过减少参数和计算量来解决这一问题。然而，这些模型在速度和资源使用上的效率仍然存在不足，尤其是在移动和嵌入式设备应用场景中。</p><h2 id="research-objective">4. Research Objective</h2><p>本研究的目标是提供一套设计高效 CNN架构的实用指南，尤其是在移动设备和嵌入式系统应用中。作者旨在通过分析模型的准确度、计算复杂度和资源约束之间的关系，提出ShuffleNet V2 作为一种更高效的轻量级替代方案。</p><h2 id="method">5. Method</h2><p>在设计轻量化网络时不应简单的只考虑间接指标（FLOPs），需要满足以下的设计原则：</p><ol type="1"><li><strong>相同通道宽度能够最小化内存访问成本（MAC）</strong>：当卷积层的输入和输出通道数相同时，所需的内存访问成本（MAC）最小。这是因为在这种情况下，卷积操作可以更高效地利用缓存，减少内存访问次数。</li><li><strong>过度的组卷积会增加MAC</strong>：使用大的分组数进行卷积运算会增加内存访问成本，因为不同组之间的数据无法共享，导致更多的内存访问。</li><li><strong>网络碎片化会降低并行程度</strong>：网络中过多的分支和基本单元会导致计算资源无法充分利用，降低并行计算的效率。</li><li><strong>Element-wise 操作的影响不可忽略</strong>：Element-wise操作（如 ReLU激活函数）虽然计算量不大，但在网络中广泛存在，对整体性能有不可忽视的影响。</li></ol><p>根据上述设计原则，ShuffleNet V2 对 ShuffleNet V1进行了若干优化，提升了模型的计算效率和内存访问效率。以下是对 V2网络架构的改进：</p><p align="center"><img src="1.png" style="zoom:67%;" /></p><p>在 ShuffleNet V1 中，网络结构基于通道混洗和深度可分卷积（depthwiseseparable convolution）两大核心技术进行设计。然而，ShuffleNet V1中的某些设计存在计算效率上的瓶颈，特别是在内存访问和网络碎片化方面。</p><h5 id="shufflenet-v2-的改进"><strong>ShuffleNet V2 的改进</strong></h5><ol type="1"><li><strong>通道划分与跳跃链接</strong> 在 ShuffleNet V2中，首先对特征图进行通道划分，一部分通道通过跳跃链接直接传递到下一层，另一部分通道经过卷积处理。在卷积操作中，V2去除了 1x1的分组卷积操作，因为在通道划分阶段，已经对通道进行了分组，从而不再需要额外的分组卷积。这一优化不仅降低了计算量，还减小了内存访问成本。</li><li><strong>替换 Add 操作为 Concat 操作</strong> ShuffleNet V2中将原有的 Add 操作替换成了 Concat操作。在V1中，Add操作通常会导致不同分支的特征图相加，可能带来一些计算上的冗余。而通过Concat操作，将特征图在通道维度上拼接，不仅能够保留更多的信息，还能更好地进行后续的通道混洗操作。</li><li><strong>通道混洗的优化</strong> 在 V2中，进行了优化的通道混洗操作进一步提升了特征图之间的信息交换效率。通道混洗确保了不同通道之间的信息能够流畅传递，而不至于造成信息瓶颈。通过减少计算冗余和提升信息流动效率，通道混洗有效提升了模型的表达能力，同时保持了较低的计算成本。</li><li><strong>下采样操作优化</strong> 对于需要进行下采样的部分，ShuffleNetV2 去除了原始 ShuffleNet V1中的通道分离步骤，而是通过加倍输出通道数来进行下采样。这一变化使得网络结构更加简洁，同时减少了冗余的操作步骤。通过优化下采样策略，V2能够在不牺牲计算效率的情况下，更加平滑地进行空间尺寸的压缩。</li></ol><h2 id="conclusion">6. Conclusion</h2><p>ShuffleNetV2提出了一种实用且高效的CNN架构，通过优化原ShuffleNet的操作，针对移动设备和嵌入式系统的应用需求，提供了一种更轻量、高效的方案。本文提出的设计原则为未来轻量级CNN的研究提供了重要参考，能够在减少计算量的同时保持较高的准确度。与现有的轻量级模型如MobileNetV2相比，ShuffleNetV2在多个方面表现出色，具有较大的应用潜力。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>算法优化</tag>
      
      <tag>轻量化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - MobileNets: Efficient Convolutional Neural Networks for Mobile </title>
    <link href="/2024/11/27/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile/"/>
    <url>/2024/11/27/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: MobileNets: Efficient Convolutional NeuralNetworks for Mobile Vision Applications<br /><strong>Link</strong>: <ahref="https://arxiv.org/pdf/1704.04861">MobileNets: Efficient CNNs forMobile Applications</a><br /><strong>Source</strong>: IEEE Conference on Computer Vision and PatternRecognition (CVPR)<br /><strong>Date</strong>: 2017</p><h2 id="summary">2. Summary</h2><p>MobileNets提出了一种轻量化、高效的卷积神经网络架构，专为移动设备和嵌入式视觉应用设计。论文的核心贡献在于引入<strong>深度可分离卷积（DepthwiseSeparableConvolutions）</strong>，大幅减少模型的计算量和参数量，同时保证精度的相对稳定。此外，作者还设计了两个超参数（宽度乘子和分辨率乘子），以实现模型精度与效率之间的灵活权衡，满足不同设备的资源限制需求。</p><h2 id="background">3. Background</h2><p>传统的卷积神经网络（如 AlexNet 或VGG）计算量庞大，对内存和处理能力要求较高，不适用于移动设备。而随着移动设备对实时图像处理和智能应用需求的增加，设计高效且准确的深度学习模型成为一个关键研究方向。MobileNets的研究背景正是针对这一需求，提出了一种在资源受限环境下优化卷积层的方法。</p><h2 id="research-objective">4. Research Objective</h2><p>本研究的目标包括：</p><ol type="1"><li>设计一种轻量化、高效的卷积神经网络架构，适用于移动和嵌入式视觉任务。</li><li>提供可调超参数以平衡模型的精度和计算效率。</li><li>验证 MobileNets在图像分类、目标检测和语义分割等任务中的性能表现。</li></ol><h2 id="method">5. Method</h2><p>核心方法为<strong>深度可分离卷积</strong>，将标准卷积分解为以下两个步骤：</p><ol type="1"><li><strong>深度卷积（DepthwiseConvolution）</strong>：对每个输入通道单独应用一个卷积核。</li><li><strong>逐点卷积（Pointwise Convolution）</strong>：通过 1x1卷积将深度卷积的输出通道进行线性组合。 此分解方法将计算复杂度降低约 8-9倍。</li></ol><p>此外，作者引入了两个超参数以进一步优化网络：</p><ul><li><strong>宽度乘子（α）</strong>：缩小每层卷积核的数量。</li><li><strong>分辨率乘子（ρ）</strong>：缩小输入图像的分辨率。</li></ul><p>通过调节 α 和 ρ，MobileNets可以灵活适应不同的硬件资源和任务需求。</p><h2 id="conclusion">6. Conclusion</h2><p>MobileNets提供了一种适用于移动和嵌入式设备的高效深度学习模型解决方案。通过引入深度可分离卷积和可调超参数，架构显著减少了计算量和内存占用，同时保持了良好的精度表现。这项工作为资源受限环境下的实时视觉任务提供了广泛的应用可能性。</p><h2 id="notes">7. Notes</h2><ol type="1"><li>宽度乘子是如何实现对模型的修改的？</li></ol><blockquote><p><strong>宽度乘子（WidthMultiplier）作用于模型的所有卷积层</strong>，它通过全局性地缩减网络中每一层的通道数，达到减少模型参数量和计算量的目的。具体来说，宽度乘子会调整每一层的输入通道数和输出通道数，使得整个网络的计算复杂度和内存占用显著降低。</p></blockquote><ol start="2" type="1"><li>为什么需要全局作用于所有层？</li></ol><blockquote><ul><li><strong>保持网络结构的一致性：</strong>如果宽度乘子仅作用于部分层，网络其他层的输入/输出通道数无法对应，可能会导致维度不匹配的错误。</li><li><strong>降低整体计算成本：</strong>卷积操作的计算复杂度与通道数呈线性关系。全局性缩减通道数，可以显著减少FLOPs（浮点运算次数）和参数量。</li><li><strong>平衡各层贡献：</strong>如果只对部分层缩减通道数，可能导致某些层的权重显著减少，而其他层的计算量仍然很高，全局性缩减可以避免这种不平衡。</li></ul></blockquote>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>算法优化</tag>
      
      <tag>轻量化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</title>
    <link href="/2024/11/26/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-ShuffleNet-An-Extremely-Efficient-Convolutional-Neural-Network-for-Mobile-Devices/"/>
    <url>/2024/11/26/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-ShuffleNet-An-Extremely-Efficient-Convolutional-Neural-Network-for-Mobile-Devices/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: ShuffleNet: An Extremely EfficientConvolutional Neural Network for Mobile Devices<br /><strong>Link</strong>: <ahref="https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf">ShuffleNetPaper</a><br /><strong>Source</strong>: IEEE Conference on Computer Vision and PatternRecognition (CVPR)<br /><strong>Date</strong>: 2018</p><h2 id="summary">2. Summary</h2><p>本文提出了ShuffleNet，一种针对移动设备的高效卷积神经网络。核心创新是<strong>通道混洗（channelshuffle）</strong>操作，它在不牺牲准确性的情况下，减少了计算量和模型大小。文章提出了两项关键技术：</p><ol type="1"><li><strong>逐点组卷积（pointwise group convolution）</strong>：通过将1x1 卷积操作分组，减少了参数和计算量。</li><li><strong>通道混洗（channelshuffle）</strong>：这一操作在组卷积后重排特征通道，提升了网络的表示能力。</li></ol><h2 id="background">3. Background</h2><p>由于移动设备面临资源受限的挑战，如何设计高效的神经网络模型变得尤为重要。传统的CNN 架构计算开销大，限制了它们在移动平台上的应用。</p><h2 id="research-objective">4. Research Objective</h2><p>本研究的主要目标是设计一种计算高效且能够保持高准确率的CNN架构，特别是针对移动设备的部署。具体来说，研究目标是：</p><ol type="1"><li>在保证高准确度的同时，减少模型的计算开销（FLOPs）。</li><li>确保模型在内存使用和模型大小方面高效，以便在实际移动设备中使用。</li></ol><h2 id="method">5. Method</h2><h4 id="channel-shuffle-for-group-convolutions">Channel Shuffle forGroup Convolutions</h4><p>现有的模型在使用组卷积时，往往忽视了 1×1卷积（逐点卷积），导致该操作占据了大量计算开销。本文提出了在 1×1卷积中使用分组卷积，以减少计算量。</p><p>然而，分组卷积会使得每个通道的输出仅来自部分输入通道。为了解决这一问题，作者提出了通道混洗技术，如下图（b）和（c）所示。假设某个卷积层被划分为g 组，每组的输出特征维度为 n，首先将其堆叠成形状为（g,n）的张量，再对其进行转置操作，最后将其拉平，完成通道之间的信息融合。</p><p align="center"><img src="1.png" style="zoom:67%;" /></p><h4 id="shufflenet-unit">ShuffleNet Unit</h4><p>基于通道混洗操作，本文提出了 ShuffleNet 单元，如下图所示。</p><p align="center"><img src="2.png" style="zoom:67%;" /></p><p>其中，(a) 为 ResNet 中的基本块，本文将其中的 1×1卷积替换为分组卷积，并在第一个分组卷积之后加入了通道混洗操作。</p><p>在需要缩减图像尺寸的情况下，如图(c)所示，在直接连接路径上使用 3×3的平均池化操作，并将原本在 ResNet中的相加操作替换为通道串联操作，从而增加通道维度。</p><h2 id="evaluation">6. Evaluation</h2><p>作者通过以下方式评估了ShuffleNet的性能：</p><ul><li>在 ImageNet 数据集上的<strong>Top-1和Top-5准确率</strong>。</li><li><strong>FLOPs</strong>（浮点运算）用于衡量计算效率。</li><li><strong>模型大小</strong>和<strong>内存使用</strong>，评估其在移动设备上的可行性。</li></ul><p>ShuffleNet 在准确率和效率上超越了其他高效 CNN 架构（如MobileNet），实现了性能与计算开销的良好平衡。</p><h2 id="conclusion">7. Conclusion</h2><p>ShuffleNet通过创新地结合组卷积和通道混洗操作，设计出了极高效的卷积神经网络，特别适合移动设备使用。实验结果表明，ShuffleNet在计算开销大幅降低的同时，依然能够保持良好的准确率。该方法在实时移动应用中具有很大的潜力，尤其是在速度和内存效率至关重要的场景下。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>算法优化</tag>
      
      <tag>轻量化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Densely Connected Convolutional Networks</title>
    <link href="/2024/11/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Densely-Connected-Convolutional-Networks/"/>
    <url>/2024/11/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Densely-Connected-Convolutional-Networks/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: Densely Connected ConvolutionalNetworks<br /><strong>Link</strong>: <ahref="https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf">DenseNetPaper</a><br /><strong>Source</strong>: IEEE Conference on Computer Vision and PatternRecognition (CVPR)<br /><strong>Date</strong>: 2017</p><h2 id="summary">2. Summary</h2><p>本文提出了DenseNet，一种新的深度学习架构，在该架构中，每一层都接收来自所有前面层的输入，形成密集的连接。与传统的卷积神经网络只传递前一层的信息不同，DenseNet为每一层创建了来自所有前一层的直接连接。该设计显著改善了梯度流动并促进了特征重用，从而实现了更高效的网络，并且参数更少。</p><h2 id="background">3. Background</h2><p>DenseNet 基于传统 CNN架构，解决了梯度消失和参数冗余等关键问题。密集连接的概念受到残差网络（ResNet）成功的启发，但DenseNet通过完全连接每一层的方式进行了更为激进的改进。这项研究在深度学习领域具有重要意义，特别是在提高图像分类、物体检测等任务的网络效率和性能方面。</p><h2 id="research-objective">4. Research Objective</h2><p>本研究的主要目标是提出并验证 DenseNet作为一种更高效的深度学习架构，旨在缓解传统 CNN中梯度消失和特征冗余的问题。研究旨在展示 DenseNet在准确性和参数效率方面相较于传统架构的优势。</p><h2 id="method">5. Method</h2><p>DenseNet架构由多个层块组成，层块内的每一层都与之前的所有层进行连接，如下图所示：</p><p align="center"><img src="1.png" style="zoom:80%;" /></p><p>在每个层块内，特征图的大小保持不变，而层块之间通过过渡层实现特征图的下采样。</p><p>DenseNet的显著特点之一是每一层的输出特征图维度非常窄，输出通道数被称为增长率<span class="math inline">\(k\)</span>。虽然每层的输出维度固定为 <spanclass="math inline">\(k\)</span>，但输入特征图的维度随着网络深度的增加而逐步增多。为提升计算效率，DenseNet在每个 3×3 卷积之前引入 1×1 卷积作为瓶颈层，以减少输入特征图的数量。</p><p>此外，通过在过渡层引入压缩因子 <spanclass="math inline">\(\theta\)</span>，进一步提升了模型的紧凑性，使DenseNet 更高效地利用计算资源。</p><h2 id="evaluation">6. Evaluation</h2><p>作者使用标准数据集（如 CIFAR-10、CIFAR-100 和 ImageNet）对 DenseNet进行了评估，结果表明 DenseNet 在准确性和参数数量方面都显著优于传统CNN，显示出更强的泛化能力。通过广泛的消融研究，分析了密集连接对性能的影响。</p><h2 id="conclusion">7. Conclusion</h2><p>DenseNet通过引入密集层连接，提供了一种有效的解决方案，提升了深度神经网络的效率。这一创新使得网络训练更快、准确性更高，同时参数更少。研究表明，DenseNet可以成为处理大数据集和深度学习任务中的一个有价值的架构。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>训练加速</tag>
      
      <tag>算法优化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Aggregated Residual Transformations for Deep Neural Networks</title>
    <link href="/2024/11/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Aggregated-Residual-Transformations-for-Deep-Neural-Networks/"/>
    <url>/2024/11/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Aggregated-Residual-Transformations-for-Deep-Neural-Networks/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: Aggregated Residual Transformations for DeepNeural Networks<br /><strong>Link</strong>: <ahref="https://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf">ResNeXtPaper</a><br /><strong>Source</strong>: IEEE Conference on Computer Vision and PatternRecognition (CVPR)<br /><strong>Date</strong>: 2017</p><h2 id="summary">2. Summary</h2><p>本文提出了<strong>ResNeXt</strong>，一种改进的残差网络架构，通过引入<strong>聚合变换（AggregatedTransformations）</strong>增强模型表达能力。核心创新是引入了<strong>基数（Cardinality）</strong>的概念，即网络中路径的数量。ResNeXt在显著提升性能的同时，保持了较低的计算复杂度，并在 ImageNet、CIFAR-10 和CIFAR-100 等基准数据集上达到了最先进的性能。</p><h2 id="background">3. Background</h2><p>深度神经网络（如ResNet）通过堆叠多个层及跳跃连接在视觉任务中取得了显著成功。然而，仅增加网络的深度或宽度带来的性能提升逐渐减小，同时计算成本迅速增加。受Inception 等多分支架构的启发，ResNeXt提出了一个更简单且更高效的聚合策略，通过<strong>并行路径的聚合变换</strong>在性能与效率之间取得了平衡。</p><h2 id="research-objective">4. Research Objective</h2><p>本文的研究目标是通过引入<strong>基数（Cardinality）</strong>提升深度网络的表达能力，同时保持计算效率。具体目标包括：</p><ol type="1"><li>提供一种更简洁的多分支架构设计方法；</li><li>验证 ResNeXt 的可扩展性及其在不同任务中的通用性；</li><li>在不显著增加计算成本的情况下实现最先进的性能。</li></ol><h2 id="method">5. Method</h2><ul><li><p><strong>关键创新</strong>：</p><p>引入了基数的概念，下图左边为原始的 ResNet架构；右边为增加了基数的改进版本，每个残差模块包含多条路径，路径数由基数控制，所有路径的输出聚合后再与输入进行残差连接。</p><p align="center"><p><img src="1.png" style="zoom:60%;" /></p></p><p>引入<strong>分组卷积（GroupedConvolution）</strong>以实现多路径聚合变换，下图 (a) 和 (b) 表示相同，图(c) 使用了分组卷积，降低了计算复杂度。</p><ul><li><p>分组卷积将输入通道划分为多个组，每组独立进行卷积操作，降低了计算复杂度。</p></li><li><p>各组的输出通过聚合操作合并，实现模块化和可扩展性。</p></li></ul><p align="center"><p><img src="2.png" style="zoom:60%;" /></p></p></li><li><p><strong>架构设计</strong>：</p><ul><li>ResNeXt 的残差模块在 ResNet的基础上，将瓶颈层的单一卷积替换为分组卷积。</li><li>基数（Cardinality）表示分组的数量，是控制并行路径数量的超参数。</li></ul></li><li><p><strong>与其他架构的比较</strong>：</p><ul><li>ResNeXt在深度（ResNet）和宽度（VGG）之外，提出了新的扩展维度——基数，通过增加基数实现性能提升，同时保持计算效率。</li></ul></li></ul><h2 id="evaluation">6. Evaluation</h2><ul><li><p><strong>数据集</strong>：ImageNet、CIFAR-10 和CIFAR-100。</p></li><li><p><strong>评估指标</strong>：分类任务的 Top-1 和 Top-5准确率。</p></li><li><p><strong>实验结果</strong>：</p><ul><li><p>在相同的深度和宽度下，ResNeXt 比 ResNet表现更优，准确率更高。</p></li><li><p>实验证明，增加基数相比单纯增加深度或宽度更能显著提升网络性能。</p></li></ul></li><li><p><strong>消融实验</strong>：验证了分组卷积的有效性，以及基数对模型性能的影响。</p></li></ul><h2 id="conclusion">7. Conclusion</h2><p>ResNeXt提供了一种通过聚合变换改进神经网络性能的简单方法。通过增加基数，ResNeXt实现了更高的准确率，同时保持了较低的计算成本。研究表明，基数是扩展深度网络性能的重要维度，提供了增加深度或宽度之外的一种灵活替代方案。</p><h2 id="notes">8. Notes</h2><ol type="1"><li><strong>什么是分组卷积</strong>？</li></ol><blockquote><p>分组卷积的核心思想是将卷积操作的输入通道和输出通道分组，然后在每组上独立执行卷积操作，最后将各组的输出拼接在一起。</p><ul><li><strong>传统卷积</strong>：<ul><li>输入特征图和卷积核的所有通道之间会进行完全连接的卷积操作。</li><li>假设输入的通道数为 <span class="math inline">\(C_{\text {in}}\)</span>，输出通道数为 <span class="math inline">\(C_{\text {out}}\)</span>，卷积核大小为 <span class="math inline">\(k \timesk\)</span>，传统卷积需要的参数量为：</li></ul></li></ul><p><span class="math display">\[C_{\text {in }} \times C_{\text {out }} \times k \times k\]</span></p><ul><li><strong>分组卷积</strong>：<ul><li>将输入通道划分为 <span class="math inline">\(g\)</span> 组（每组有<span class="math inline">\(C_{\text {in }}\)</span> / <spanclass="math inline">\(g\)</span> 个通道），输出通道也划分为 <spanclass="math inline">\(g\)</span> 组（每组有 <spanclass="math inline">\(C_{\text {out }}\)</span> / <spanclass="math inline">\(g\)</span> 个通道）。</li><li>每组卷积仅计算输入通道的一部分，从而减少了计算量。</li><li>参数量为：</li></ul></li></ul><p><span class="math display">\[\left(C_{\text {in }} / g\right) \times\left(C_{\text {out }} / g\right)\times k \times k \times g=\frac{C_{\text {in }} \times C_{\text {out }}\times k \times k}{g}\]</span></p></blockquote><ol start="2" type="1"><li><strong>ResNeXt 与 Inception-ResNet 的对比</strong></li></ol><blockquote><table><thead><tr class="header"><th><strong>特性</strong></th><th><strong>ResNeXt</strong></th><th><strong>Inception-ResNet</strong></th></tr></thead><tbody><tr class="odd"><td><strong>路径结构</strong></td><td>等价路径（分组卷积）</td><td>非等价路径（不同卷积核和池化操作）</td></tr><tr class="even"><td><strong>模块复杂度</strong></td><td>简单、模块化</td><td>复杂、需手动调优</td></tr><tr class="odd"><td><strong>计算复杂度</strong></td><td>更低，参数更少</td><td>较高，参数较多</td></tr><tr class="even"><td><strong>特征表达能力</strong></td><td>高效，通过增加基数捕获更多特征</td><td>优秀，能捕获多尺度特征</td></tr><tr class="odd"><td><strong>扩展性和通用性</strong></td><td>高，易于在深层网络中扩展</td><td>中等，适合特定任务（如多尺度特征处理）</td></tr><tr class="even"><td><strong>适用场景</strong></td><td>大规模训练、深层分类网络</td><td>需要多尺度特征融合的任务（如检测、分割）</td></tr></tbody></table></blockquote>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>训练加速</tag>
      
      <tag>算法优化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Rethinking the Inception Architecture for Computer Vision</title>
    <link href="/2024/11/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Rethinking-the-Inception-Architecture-for-Computer-Vision/"/>
    <url>/2024/11/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Rethinking-the-Inception-Architecture-for-Computer-Vision/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: Rethinking the Inception Architecture forComputer Vision<br /><strong>Link</strong>: <ahref="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf">InceptionV3 Paper</a><br /><strong>Source</strong>: IEEE Conference on Computer Vision and PatternRecognition (CVPR)<br /><strong>Date</strong>: 2016</p><h2 id="summary">2. Summary</h2><p>本文重新审视了 Inception V1架构并提出了一系列改进，提升了其效率和性能，从而推出了 Inception V3模型。这些改进包括优化计算成本、减少参数数量，以及在保持高效表示的同时增加网络的深度和宽度。在ImageNet 数据集上，这些模型在减少计算预算的同时实现了最先进的性能。</p><h2 id="background">3. Background</h2><p>Inception网络是一类在每层内通过多尺度特征提取实现计算效率与性能平衡的架构。最初的GoogLeNet（Inception V1）引入了模块化设计，但也带来了参数数量增多和计算开销增加的问题。本研究在Inception V1的成功基础上，提出了解决这些局限性的方法，并进一步提升了视觉任务中的准确率和效率。</p><h2 id="research-objective">4. Research Objective</h2><ol type="1"><li>减少网络计算成本和参数数量。</li><li>平衡网络深度与宽度以实现最优性能。</li><li>探索新的技术，例如卷积因式分解和辅助损失层。</li></ol><h2 id="method">5. Method</h2><h3 id="卷积因式分解"><strong>卷积因式分解</strong></h3><p>用多个较小的卷积（如 3×3 或 1×1 ）替代较大的卷积（如 5×5），以降低计算成本。</p><p align="center"><img src="1.png" style="zoom:80%;" /></p><p>将较大的卷积核（5×5）分解为两个小卷积核（如 1×5 和 5×1），降低参数量和计算复杂度。</p><p align="center"><img src="2.png" style="zoom:80%;" /></p><h3 id="高效网格尺寸缩减"><strong>高效网格尺寸缩减</strong></h3><p>如图 9所示，左侧方法虽然降低了特征图尺寸，但容易引发表征瓶颈；右侧方法虽能缓解表征瓶颈，却带来了较高的计算开销。而图10 中的方法在避免表征瓶颈的基础上，成功实现了计算开销的显著降低。</p><p align="center"><img src="3.png" style="zoom:80%;" /></p><h3 id="辅助分类器"><strong>辅助分类器</strong></h3><p>早期的 GoogLeNet 使用辅助分类器主要是为了缓解梯度消失问题。但Inception V3中研究了它的<strong>正则化效果</strong>，发现即使在没有梯度消失问题的情况下，辅助分类器仍然能提高网络的泛化能力。</p><h3 id="label-smoothing技术"><strong>Label Smoothing技术</strong></h3><p>在传统的分类任务中，目标函数通常使用交叉熵损失（Cross-EntropyLoss），其中真实类别的标签被表示为 <strong>one-hot编码</strong>（即目标类别为 1，其他类别为0）。这种方式在训练时可能导致模型对目标类别的预测概率非常接近1，而对其他类别接近 0，从而导致过拟合问题。Label Smoothing是一种通过调整目标分布的方法，避免模型过度自信。它将原始的 one-hot分布替换为一个“平滑”的分布，即目标类别的标签值被减小，而非目标类别的标签值被稍微增加：<span class="math display">\[q^{\prime}(k)= \begin{cases}1-\epsilon+\frac{\epsilon}{K}, &amp; \text {if } k=y \\ \frac{\epsilon}{K}, &amp; \text { if } k \neq y\end{cases}\]</span></p><ul><li><spanclass="math inline">\(q^{\prime}(k)\)</span>：经过平滑后的标签分布。</li><li><spanclass="math inline">\(\epsilon\)</span>：平滑因子，通常是一个较小的正值，例如0.1。</li><li><span class="math inline">\(K\)</span>：类别总数。</li><li><span class="math inline">\(y\)</span>：真实类别索引。</li></ul><p>这样，目标分布会从严格的 one-hot 变成一个软分布。</p><h2 id="evaluation">6. Evaluation</h2><p>模型在<strong>ImageNet数据集</strong>上进行评估，这是大规模图像分类的基准。</p><ul><li><strong>评估指标</strong>：Top-1 和 Top-5 分类准确率。</li><li><strong>结果</strong>：Inception V3在与前代架构相比时显著提升了准确率，同时降低了计算成本。</li><li><strong>实验验证</strong>：提出的技术（如卷积因式分解和批量归一化）对性能提升起到了关键作用。</li></ul><h2 id="conclusion">7. Conclusion</h2><p>本文成功提出了一种改进的 Inception架构，在计算效率和性能之间实现了平衡。因式分解卷积和改进的训练策略等创新，为设计高效的深度学习架构设立了新的标准。这些发现对构建可扩展、高效的深度学习模型具有广泛意义。</p><h2 id="notes">8. Notes</h2><ol type="1"><li>虽然 Inception V2 和 Inception V3 都是在同一篇论文 "Rethinking theInception Architecture for Computer Vision" 中提及，但出于对 BatchNormalization 的强调，很多人习惯性地将 <strong>BN-Inception</strong>称为 Inception V2。</li></ol>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>算法优化</tag>
      
      <tag>CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Deep Residual Learning for Image Recognition</title>
    <link href="/2024/11/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deep-Residual-Learning-for-Image-Recognition/"/>
    <url>/2024/11/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deep-Residual-Learning-for-Image-Recognition/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: Deep Residual Learning for ImageRecognition<br /><strong>Link</strong>: <ahref="https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">ResNetPaper</a><br /><strong>Source</strong>: IEEE Conference on Computer Vision and PatternRecognition (CVPR)<br /><strong>Date</strong>: 2016</p><h2 id="summary">2. Summary</h2><p>本文提出了一种新型的深度学习架构——残差网络（ResNet），它通过引入<strong>残差连接</strong>来解决传统深度神经网络在训练过程中遇到的梯度消失和退化问题。作者通过实验证明，残差网络在多个图像识别任务中，特别是在ImageNet图像分类任务上，超越了现有的深度网络架构，达到了更好的效果。该方法可以加速深层网络的训练，并显著提高模型的性能。</p><h2 id="background">3. Background</h2><p>随着深度学习技术的发展，深度神经网络的应用逐渐取得了许多成功。然而，当网络层数增多时，模型的训练难度也随之增加，通常会遇到梯度消失、过拟合、训练退化等问题。传统的做法是通过增加层数来提升模型的表现，但实际效果往往没有预期那么好，这也成为深度学习研究中的一个瓶颈。因此，如何构建更深且更易训练的网络结构成为研究的热点。</p><h2 id="research-objective">4. Research Objective</h2><p>本研究的主要目标是通过引入<strong>残差学习</strong>来改进非常深的网络训练，使其能够更好地进行图像识别任务。</p><h2 id="method">5. Method</h2><ul><li><strong>残差学习</strong>：ResNet不是直接学习输入和输出之间的映射，而是学习输入与输出之间的残差，即 <spanclass="math inline">\(\mathcal{F}(x)=H(x)-x\)</span>，其中 <spanclass="math inline">\(H(x)\)</span> 是期望的映射。</li></ul><p align="center"><img src="1.png" style="zoom:67%;" /></p><ul><li><p><strong>网络模块</strong>：该架构由<strong>残差块（ResidualBlock）</strong>组成，每个残差块中包含一个快捷连接，该连接绕过一个或多个层。</p></li><li><p><strong>深层网络</strong>：通过残差学习，ResNet能够构建任意深度的网络（如152 层），同时避免性能退化。</p></li><li><p><strong>优化</strong>：快捷连接确保了在反向传播中梯度能够顺利流动，从而更容易训练非常深的网络。</p></li></ul><h2 id="evaluation">6. Evaluation</h2><ul><li>作者在<strong>ImageNet 2012 分类挑战</strong>中评估了 ResNet架构，并取得了最先进的结果。ResNet-152 模型的 top-5 错误率为3.0%，远超之前的模型。</li><li>作者还在 COCO 检测和分割任务中验证了该方法，表现也十分优秀。</li><li>他们将 ResNet 与传统的深度 CNN模型以及其他网络进行了比较，证明了更深的 ResNet在准确率上始终优于浅层网络。</li></ul><h2 id="conclusion">7. Conclusion</h2><ul><li><strong>残差网络（ResNets）</strong>使得构建非常深的网络成为可能，并且能够避免训练中的性能退化问题，解决了深层神经网络的梯度消失和优化问题。</li><li><strong>残差学习框架</strong>既简单又有效，可以广泛应用于各种任务，除了图像分类，还包括目标检测和图像分割等。</li><li>残差网络的成功表明，更深的网络结构并非一定会导致性能下降，只要能够有效地传递梯度。</li></ul>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>训练加速</tag>
      
      <tag>算法优化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Batch Normalization: Accelerating Deep Network Training by  Reducing Internal Covariate Shift</title>
    <link href="/2024/11/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/"/>
    <url>/2024/11/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: Batch Normalization Accelerating Deep NetworkTraining by Reducing Internal Covariate Shift<br /><strong>Link</strong>: <ahref="https://asvk.cs.msu.ru/~sveta/%D1%80%D0%B5%D1%84%D0%B5%D1%80%D0%B0%D1%82/batch_normalization.pdf">BatchNormalization Paper</a><br /><strong>Source</strong>: Neural Information Processing Systems(NeurIPS)<br /><strong>Date</strong>: 2015</p><h2 id="summary">2. Summary</h2><p>本文提出了批量归一化（BatchNormalization，BN）技术，通过减少深度神经网络中的内部协变量偏移（internalcovariateshift），加速网络训练。内部协变量偏移是指在训练过程中，层输入的分布发生变化。BN的核心创新是对每一层的输入进行归一化，使其均值为 0，方差为1，然后进行一个学习的线性变换。BN加速了收敛速度，允许使用更高的学习率，并且缓解了梯度消失和梯度爆炸的问题。BN还具有一定的正则化效果，减少了对 Dropout 的需求。</p><h2 id="background">3. Background</h2><p>深度神经网络在训练时常常面临内部协变量偏移问题，即随着参数的更新，层输入的分布发生变化。这种不稳定性会导致优化过程效率低下，需要精心设计的初始化和学习率调优。之前的解决方法，如预训练和权重初始化，间接解决了这一问题，而BN 通过直接归一化层输入来解决根本问题。</p><h2 id="research-objective">4. Research Objective</h2><ul><li>提出批量归一化方法来减少内部协变量偏移。</li><li>证明BN能够加速训练并提升模型性能。</li><li>评估BN与常用优化方法（如 SGD）兼容性。</li><li>探讨BN的正则化效果及对超参数调节的影响。</li></ul><h2 id="method">5. Method</h2><ul><li><p><strong>归一化</strong>： 对每个 mini-batch 中的激活值 <spanclass="math inline">\(x\)</span>，进行归一化： <spanclass="math display">\[\hat{x}=\frac{x-\mu_B}{\sqrt{\sigma_B^2+\epsilon}}\]</span> 其中，<span class="math inline">\(\mu_B\)</span> 和 <spanclass="math inline">\(\sigma_B^2\)</span> 分别是 mini-batch的均值和方差，<span class="math inline">\(\epsilon\)</span>用于防止除零错误。</p></li><li><p><strong>仿射变换</strong>： 归一化之后，使用可学习的参数 <spanclass="math inline">\(\gamma\)</span> （缩放）和 <spanclass="math inline">\(\beta\)</span>（平移）对激活值进行缩放和偏移：</p></li></ul><p><span class="math display">\[y=\gamma \hat{x}+\beta\]</span></p><ul><li><p><strong>训练阶段</strong>： 在训练过程中，使用 mini-batch的统计量进行归一化。同时，利用移动平均对均值 <spanclass="math inline">\(\mu_B\)</span> 和方差 <spanclass="math inline">\(\sigma_B^2\)</span>进行估算，以便在推理时使用。</p></li><li><p><strong>推理阶段</strong>：在推理阶段，使用固定的均值和方差（训练时的全局统计量）进行归一化，确保输出是确定的。</p></li></ul><h2 id="evaluation">6. Evaluation</h2><ul><li><p><strong>数据集</strong>：在 CIFAR-10、ImageNet等基准数据集上进行测试。</p></li><li><p><strong>结果</strong>：</p><ul><li><p>BN显著加速了收敛（例如在 ImageNet 上，训练时间减少了多达 14倍）。</p></li><li><p>与没有BN的模型相比，BN 提高了模型的准确率。</p></li></ul></li><li><p><strong>消融实验</strong>：</p><ul><li><p>显示了 <span class="math inline">\(\gamma\)</span> 和 <spanclass="math inline">\(\beta\)</span> 的重要性，以及 mini-batch归一化的有效性。</p></li><li><p>BN 具有一定的正则化效果，减少了对 Dropout 的需求。</p></li></ul></li></ul><h2 id="conclusion">7. Conclusion</h2><p>批量归一化提出了一种简单有效的方法来稳定并加速深度神经网络的训练。它通过减少内部协变量偏移，能够加快收敛速度并提高泛化能力。BN的普适性和有效性使其成为现代深度学习架构中的标准组件。研究强调了解决网络内部分布变化问题对提升训练效率的重要性。</p><h2 id="notes">8. Notes</h2><ol type="1"><li>为什么要进行归一化？</li></ol><blockquote><ul><li><p><strong>避免梯度消失或梯度爆炸：</strong></p><ul><li><strong>梯度消失：</strong> 对于偏大的通道值，激活函数（如 Sigmoid或 Tanh）的输出可能趋近其饱和区间（例如，Sigmoid 趋近于 0 或1）。在饱和区域，导数接近于 0，导致梯度几乎消失，权重无法有效更新。</li><li><strong>梯度爆炸：</strong>对于偏小的通道值，激活函数的导数可能非常大，导致梯度在反向传播过程中不断累积并放大，最终引起梯度爆炸。</li></ul><p>这些现象会使优化过程变得极其不稳定，甚至使模型无法收敛。</p></li><li><p><strong>平衡通道值范围：</strong></p><ul><li>如果不同通道的值范围差异显著：<ul><li><strong>梯度更新受大值主导：</strong>较大的值会主导梯度更新方向，网络可能优先调整这些通道的权重。</li><li><strong>忽略小值信息：</strong>较小值的通道可能被忽略，导致网络无法充分利用所有特征信息。</li></ul></li></ul><p>这种不平衡会降低模型的学习效率，延长训练时间，并难以达到最佳性能。</p></li><li><p><strong>简化损失函数的优化过程：</strong></p><ul><li>通道间值差异较大时，损失函数的形状可能会变得复杂（例如，陡峭的谷底或平坦的高原）。</li><li>优化器可能需要更小的学习率逐渐调整权重，从而减慢模型的收敛速度。</li></ul></li></ul></blockquote>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>训练加速</tag>
      
      <tag>算法优化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记 - Going Deeper with Convolutions</title>
    <link href="/2024/11/19/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Going-Deeper-with-Convolutions/"/>
    <url>/2024/11/19/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Going-Deeper-with-Convolutions/</url>
    
    <content type="html"><![CDATA[<h2 id="information">1. Information</h2><p><strong>Title</strong>: Going Deeper with Convolutions<br /><strong>Link</strong>: <ahref="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf">InceptionV1 Paper</a><br /><strong>Source</strong>: IEEE Conference on Computer Vision and PatternRecognition (CVPR)<br /><strong>Date</strong>: 2015</p><h2 id="summary">2. Summary</h2><p>本文提出了一种名为 <strong>Inception</strong>的深度卷积神经网络架构，在提高模型深度和宽度的同时，保持计算开销较低。基于此架构设计的<strong>GoogLeNet</strong>在图像分类和目标检测任务中取得了显著的性能提升。其核心思想是通过多个并行计算路径近似局部稀疏结构，兼顾了计算效率和模型精度。</p><h2 id="background">3. Background</h2><ul><li><p>深度学习的发展依赖于更强大的硬件、更大的数据集以及更高效的网络架构。然而，在移动设备或嵌入式环境中，功耗和内存限制要求算法需更高效。</p></li><li><p>增大网络规模虽能提升性能，但带来了两个问题：</p><ol type="1"><li><p>容易过拟合，需要昂贵的高质量标注数据。</p></li><li><p>参数利用率低，造成计算资源浪费。</p></li></ol></li><li><p>稀疏网络可减少计算量，但现代硬件在稀疏计算上效率不高。</p></li></ul><h2 id="research-objective">4. Research Objective</h2><p>设计一种高效的网络架构，在降低计算复杂度和参数量的同时，保留深度模型的表达能力。通过使用密集的并行模块近似稀疏性，解决传统稀疏结构难以高效并行的问题。</p><h2 id="method">5. Method</h2><ul><li><p><strong>核心思想</strong>：</p><ol type="1"><li>使用 <strong>1×1、3×3 和 5×5 卷积</strong>提取多尺度特征，同时结合池化操作以捕获全局信息。</li><li>在大卷积核之前加入 <strong>1×1卷积</strong>，用于降维和提升非线性表达能力。</li><li>通过模块化设计，平衡计算成本和特征提取能力。</li></ol></li><li><p><strong>网络结构</strong>：</p><ul><li>初版 Inception模块中并行使用不同卷积核和池化操作，会导致通道数增加过快。</li></ul></li></ul><p align="center"><img src="1.png" style="zoom:50%;" /></p><ul><li>改进版通过在每条路径前增加 <strong>1×1卷积降维</strong>，有效控制通道数，降低参数量。</li></ul><p align="center"><img src="2.png" style="zoom:50%;" /></p><ul><li>GoogLeNet 总体架构：<ul><li>采用多层 Inception 模块堆叠，深度增加但计算效率较高。</li><li>引入辅助分类器（仅训练时使用）缓解梯度消失问题。</li></ul></li></ul><p align="center"><img src="3.png" style="zoom:50%;" /></p><h2 id="evaluation">6. Evaluation</h2><h3 id="图像分类任务">① 图像分类任务</h3><ul><li>数据集：ImageNet</li><li>GoogLeNet 在分类任务中取得了 6.67% 的 top-5 错误率，相比 AlexNet 和VGG 显著提升。</li></ul><p align="center"><img src="4.png" style="zoom:50%;" /></p><h3 id="目标检测任务">② 目标检测任务</h3><ul><li>数据集：PASCAL VOC 和 COCO</li><li>在目标检测任务中，结合 Inception 的 R-CNN模型在精度和效率上表现出色。</li></ul><p align="center"><img src="5.png" style="zoom:50%;" /></p><h2 id="conclusion">7. Conclusion</h2><ul><li><strong>稀疏性近似</strong>：通过并行使用多尺度卷积和池化操作，Inception模块模拟局部稀疏结构，既降低了计算复杂度，又避免了稀疏计算的硬件瓶颈。</li><li><strong>模块化设计</strong>：使用 1×1卷积降维，控制通道数增长，有效减少参数量和内存占用。</li><li><strong>高效性能</strong>：GoogLeNet在分类和检测任务上均实现了卓越的性能，是一种计算资源友好的深度学习模型。</li></ul><h2 id="notes">8. Notes</h2><ol type="1"><li><strong>1×1 卷积的作用</strong>：</li></ol><blockquote><ul><li>降维与升维</li><li>降低参数量</li><li>跨通道信息融合</li><li>提高非线性表达能力</li></ul></blockquote><ol start="2" type="1"><li><strong>辅助分类器的设计注意事项</strong>：</li></ol><blockquote><ul><li><p>如果设计不当，可能干扰主分类器优化。</p></li><li><p>解决方法包括降低辅助分类器损失权重或简化其结构。</p></li></ul></blockquote>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>算法优化</tag>
      
      <tag>CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
